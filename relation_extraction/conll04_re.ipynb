{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-letter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import conll04_parser\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 8 # Number of relation classes\n",
    "NUM_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-oliver",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(group):\n",
    "    data = conll04_parser.extract_data(group)\n",
    "    for doc in data:\n",
    "        # If this sentence has at least two entities for a possible relation\n",
    "        if len(doc[\"entity_position\"]) >= 2:\n",
    "            new_entity_position = {}\n",
    "            for entity in doc[\"entity_position\"]:\n",
    "                new_entity_position[entity] = (\n",
    "                    doc[\"entity_position\"][entity][0] + 1, # +1: space for CLS token\n",
    "                    doc[\"entity_position\"][entity][1] + 1  # +1: space for CLS token\n",
    "                )\n",
    "            # Add CLS and SEP to the sentence\n",
    "            input_ids = [conll04_parser.CLS_TOKEN] + doc[\"data_frame\"][\"token_ids\"].tolist() + [conll04_parser.SEP_TOKEN]\n",
    "            e1_mask, e2_mask, labels = model.generate_entity_mask(len(input_ids), new_entity_position, doc[\"relations\"])\n",
    "            assert e1_mask.shape[0] == e2_mask.shape[0] == labels.shape[0]\n",
    "            assert len(input_ids) == e1_mask.shape[1] == e2_mask.shape[1]\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor([input_ids]).long().to(device), \n",
    "                \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                \"e1_mask\": e1_mask.to(device),\n",
    "                \"e2_mask\": e2_mask.to(device),\n",
    "                \"labels\": labels.to(device)\n",
    "            }\n",
    "            del e1_mask\n",
    "            del e2_mask\n",
    "            del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data_generator()\n",
    "# generator = data_generator(\"train\")\n",
    "# # Test on the first document (\"1024\")\n",
    "# test_inputs = next(generator)\n",
    "# assert test_inputs[\"input_ids\"][0, 0] == conll04_parser.CLS_TOKEN\n",
    "# assert test_inputs[\"input_ids\"][0, 1] == 2200\n",
    "# assert test_inputs[\"input_ids\"][0, -2] == 1012\n",
    "# assert test_inputs[\"input_ids\"][0, -1] == conll04_parser.SEP_TOKEN\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][0, 22:24], torch.tensor([1, 1]))\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][2, 25:28], torch.tensor([1, 1, 1]))\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][4, 29:31], torch.tensor([1, 1]))\n",
    "# assert torch.equal(test_inputs[\"labels\"], torch.tensor([0, 2, 0, 2, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-brand",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "mre_model = model.BertForMre(NUM_CLASSES)\n",
    "mre_model.load_state_dict(torch.load(\"../model/re/conll04_50.model\"))\n",
    "mre_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for the last classifier layer on top\n",
    "for param in mre_model.parameters():\n",
    "    param.requires_grad = False\n",
    "mre_model.classifier.weight.requires_grad = True\n",
    "mre_model.classifier.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-collect",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in mre_model.parameters():\n",
    "    print(\"size:\", param.shape)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-inside",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(mre_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-short",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "    val_generator = data_generator(\"dev\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in val_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "        \n",
    "    print(\"[validation]\")\n",
    "    result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for i, inputs in enumerate(data_generator(\"train\"), 0):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mre_model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            true_labels += inputs[\"labels\"].tolist()\n",
    "            pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "            predicted_labels += pred_labels.tolist()\n",
    "            assert len(predicted_labels) == len(true_labels)\n",
    "            if i % 800 == 799:    # print every 800 mini-batches\n",
    "                print(\"[%d, %5d]\" % (epoch + 1, i + 1))\n",
    "                result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "                result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "                result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "                print(result)\n",
    "                true_labels = []\n",
    "                predicted_labels = []\n",
    "\n",
    "            del inputs\n",
    "            \n",
    "        validate_model()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-yemen",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    test_generator = data_generator(\"test\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in test_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "    \n",
    "    label_map = {v: k for k, v in conll04_parser.relation_encode.items()}\n",
    "    classes = list(label_map.keys())\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=classes)\n",
    "    result = pd.DataFrame(index=[label_map[c] for c in classes])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result.to_csv(\"conll04_100_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solar-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(mre_model.state_dict(), \"../model/re/conll04_100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mre_model.load_state_dict(torch.load(\"../model/re/conll04.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-helena",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
