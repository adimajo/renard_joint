{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import conll04_parser\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 8 # Number of relation classes\n",
    "NUM_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-print",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(group):\n",
    "    data = conll04_parser.extract_data(group)\n",
    "    for doc in data:\n",
    "        # If this sentence has at least two entities for a possible relation\n",
    "        if len(doc[\"entity_position\"]) >= 2:\n",
    "            new_entity_position = {}\n",
    "            for entity in doc[\"entity_position\"]:\n",
    "                new_entity_position[entity] = (\n",
    "                    doc[\"entity_position\"][entity][0] + 1, # +1: space for CLS token\n",
    "                    doc[\"entity_position\"][entity][1] + 1  # +1: space for CLS token\n",
    "                )\n",
    "            # Add CLS and SEP to the sentence\n",
    "            input_ids = [conll04_parser.CLS_TOKEN] + doc[\"data_frame\"][\"token_ids\"].tolist() + [conll04_parser.SEP_TOKEN]\n",
    "            e1_mask, e2_mask, labels = model.generate_entity_mask(len(input_ids), new_entity_position, doc[\"relations\"])\n",
    "            assert e1_mask.shape[0] == e2_mask.shape[0] == labels.shape[0]\n",
    "            assert len(input_ids) == e1_mask.shape[1] == e2_mask.shape[1]\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor([input_ids]).long(), \n",
    "                \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long),\n",
    "                \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long),\n",
    "                \"e1_mask\": e1_mask,\n",
    "                \"e2_mask\": e2_mask,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "            del e1_mask\n",
    "            del e2_mask\n",
    "            del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-military",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data_generator()\n",
    "generator = data_generator(\"train\")\n",
    "# Test on the first document (\"1024\")\n",
    "test_inputs = next(generator)\n",
    "assert test_inputs[\"input_ids\"][0, 0] == conll04_parser.CLS_TOKEN\n",
    "assert test_inputs[\"input_ids\"][0, 1] == 2200\n",
    "assert test_inputs[\"input_ids\"][0, -2] == 1012\n",
    "assert test_inputs[\"input_ids\"][0, -1] == conll04_parser.SEP_TOKEN\n",
    "assert torch.equal(test_inputs[\"e1_mask\"][0, 22:24], torch.tensor([1, 1]))\n",
    "assert torch.equal(test_inputs[\"e1_mask\"][2, 25:28], torch.tensor([1, 1, 1]))\n",
    "assert torch.equal(test_inputs[\"e1_mask\"][4, 29:31], torch.tensor([1, 1]))\n",
    "assert torch.equal(test_inputs[\"labels\"], torch.tensor([0, 2, 0, 2, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_model = model.BertForMre(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for the last classifier layer on top\n",
    "for param in mre_model.parameters():\n",
    "    param.requires_grad = False\n",
    "mre_model.classifier.weight.requires_grad = True\n",
    "mre_model.classifier.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-idaho",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in mre_model.parameters():\n",
    "    print(\"size:\", param.shape)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-steel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(mre_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "    val_generator = data_generator(\"val\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in val_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "        \n",
    "    print(\"[validation]\")\n",
    "    result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for i, inputs in enumerate(data_generator(\"train\"), 0):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mre_model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            true_labels += inputs[\"labels\"].tolist()\n",
    "            pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "            predicted_labels += pred_labels.tolist()\n",
    "            assert len(predicted_labels) == len(true_labels)\n",
    "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
    "                print(\"[%d, %5d]\" % (epoch + 1, i + 1))\n",
    "                result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "                result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "                result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "                print(result)\n",
    "                true_labels = []\n",
    "                predicted_labels = []\n",
    "\n",
    "            del inputs\n",
    "            \n",
    "        validate_model()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    test_generator = data_generator(\"test\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in test_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "    \n",
    "    label_map = {v: k for k, v in conll04_parser.relation_encode.items()}\n",
    "    classes = list(label_map.keys())\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=classes)\n",
    "    result = pd.DataFrame(index=[label_map[c] for c in classes])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
