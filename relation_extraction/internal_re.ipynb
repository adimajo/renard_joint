{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-ordering",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import internal_parser\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metallic-nitrogen",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-candle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 8 # Number of relation classes\n",
    "NUM_EPOCH = 3\n",
    "MAX_TOKEN = 128 # For limited RAM capacity\n",
    "VALIDATION_SIZE = 100 # Number of observations evalutated in validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(group):\n",
    "    docs = internal_parser.get_docs(group)\n",
    "    data = internal_parser.extract_data(docs)\n",
    "    for doc in data:\n",
    "        sentence_id = 0\n",
    "        starting_index = 0\n",
    "        input_ids = []\n",
    "        # ddd a final row with dummy sentence embedding\n",
    "        doc[\"data_frame\"].loc[doc[\"data_frame\"].index.max() + 1, \"sentence_embedding\"] \\\n",
    "            = doc[\"data_frame\"][\"sentence_embedding\"].max() + 1\n",
    "        for index, row in doc[\"data_frame\"].iterrows():\n",
    "            if row[\"sentence_embedding\"] != sentence_id or index - starting_index >= MAX_TOKEN - 2:\n",
    "                new_entity_position = {}\n",
    "                for entity in doc[\"entity_position\"]:\n",
    "                    if starting_index <= doc[\"entity_position\"][entity][0] < doc[\"entity_position\"][entity][1] <= index:\n",
    "                        new_entity_position[entity] = (\n",
    "                            doc[\"entity_position\"][entity][0] - starting_index + 1, # +1: space for CLS token\n",
    "                            doc[\"entity_position\"][entity][1] - starting_index + 1  # +1: space for CLS token\n",
    "                        )\n",
    "                        \n",
    "                # If this sentence has at least two entities for a possible relation\n",
    "                if len(new_entity_position) >= 2:\n",
    "                    # Add CLS and SEP to the sentence\n",
    "                    input_ids = [internal_parser.CLS_TOKEN] + input_ids + [internal_parser.SEP_TOKEN]\n",
    "                    e1_mask, e2_mask, labels = model.generate_entity_mask(len(input_ids), new_entity_position, doc[\"relations\"])\n",
    "                    assert e1_mask.shape[0] == e2_mask.shape[0] == labels.shape[0]\n",
    "                    assert len(input_ids) == e1_mask.shape[1] == e2_mask.shape[1]\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor([input_ids]).long(), \n",
    "                        \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long),\n",
    "                        \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long),\n",
    "                        \"e1_mask\": e1_mask,\n",
    "                        \"e2_mask\": e2_mask,\n",
    "                        \"labels\": labels\n",
    "                    }\n",
    "                    del e1_mask\n",
    "                    del e2_mask\n",
    "                    del labels\n",
    "                    \n",
    "                sentence_id = row[\"sentence_embedding\"]\n",
    "                input_ids = []\n",
    "                starting_index = index\n",
    "            \n",
    "            input_ids.append(row[\"token_ids\"])   \n",
    "        \n",
    "        del input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data_generator()\n",
    "# generator = data_generator(\"All\")\n",
    "# # Test on the first document (\"143f9e00-34c4-11eb-a28a-8b07c9b15060-0\")\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 1015\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2057\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2119\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2012\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 5214\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 1016\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2057\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 8115\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 4550\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 1999\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 1016\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2009\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2057\n",
    "# assert next(generator)[\"input_ids\"][0, 1] == 2156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-salad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mre_model = model.BertForMre(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-hotel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for the last classifier layer on top\n",
    "for param in mre_model.parameters():\n",
    "    param.requires_grad = False\n",
    "mre_model.classifier.weight.requires_grad = True\n",
    "mre_model.classifier.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-banks",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param in mre_model.parameters():\n",
    "    print(\"size:\", param.shape)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(mre_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(count):\n",
    "    val_generator = data_generator(\"Test\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for i in range(count):\n",
    "        inputs = next(val_generator)\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "        \n",
    "    print(\"[validation %d]\" % (count))\n",
    "    result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for i, inputs in enumerate(data_generator(\"Training\"), 0):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mre_model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            true_labels += inputs[\"labels\"].tolist()\n",
    "            pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "            predicted_labels += pred_labels.tolist()\n",
    "            assert len(predicted_labels) == len(true_labels)\n",
    "            if i % 1000 == 999:    # print every 2000 mini-batches\n",
    "                print(\"[%d, %5d]\" % (epoch + 1, i + 1))\n",
    "                result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "                result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "                result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "                print(result)\n",
    "                validate_model(VALIDATION_SIZE)\n",
    "                true_labels = []\n",
    "                predicted_labels = []\n",
    "\n",
    "            del inputs\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    test_generator = data_generator(\"Test\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in test_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "    \n",
    "    label_map = {v: k for k, v in internal_parser.relation_encode.items()}\n",
    "    classes = list(label_map.keys())\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=classes)\n",
    "    result = pd.DataFrame(index=[label_map[c] for c in classes])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-battle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
