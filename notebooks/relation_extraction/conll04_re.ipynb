{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minus-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spectacular-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../parser\")\n",
    "import conll04_parser\n",
    "import model\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mature-match",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll04_parser.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "smoking-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "neutral-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NUM_CLASSES = 6 # Number of relation classes\n",
    "NUM_EPOCH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "meaning-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alternate-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(group):\n",
    "    data = conll04_parser.extract_data(group)\n",
    "    for doc in data:\n",
    "        # If this sentence has at least two entities for a possible relation\n",
    "        if len(doc[\"entity_position\"]) >= 2:\n",
    "            new_entity_position = {}\n",
    "            for entity in doc[\"entity_position\"]:\n",
    "                new_entity_position[entity] = (\n",
    "                    doc[\"entity_position\"][entity][0] + 1, # +1: space for CLS token\n",
    "                    doc[\"entity_position\"][entity][1] + 1  # +1: space for CLS token\n",
    "                )\n",
    "            # Add CLS and SEP to the sentence\n",
    "            input_ids = [conll04_parser.CLS_TOKEN] + doc[\"data_frame\"][\"token_ids\"].tolist() + [conll04_parser.SEP_TOKEN]\n",
    "            e1_mask, e2_mask, labels = model.generate_entity_mask(len(input_ids), new_entity_position, doc[\"relations\"])\n",
    "            assert e1_mask.shape[0] == e2_mask.shape[0] == labels.shape[0]\n",
    "            assert len(input_ids) == e1_mask.shape[1] == e2_mask.shape[1]\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor([input_ids]).long().to(device), \n",
    "                \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                \"e1_mask\": e1_mask.to(device),\n",
    "                \"e2_mask\": e2_mask.to(device),\n",
    "                \"labels\": labels.to(device)\n",
    "            }\n",
    "            del e1_mask\n",
    "            del e2_mask\n",
    "            del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "normal-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test data_generator()\n",
    "# generator = data_generator(\"train\")\n",
    "# # Test on the first document (\"1024\")\n",
    "# test_inputs = next(generator)\n",
    "# assert test_inputs[\"input_ids\"][0, 0] == conll04_parser.CLS_TOKEN\n",
    "# assert test_inputs[\"input_ids\"][0, 1] == 2200\n",
    "# assert test_inputs[\"input_ids\"][0, -2] == 1012\n",
    "# assert test_inputs[\"input_ids\"][0, -1] == conll04_parser.SEP_TOKEN\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][0, 22:24], torch.tensor([1, 1]))\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][2, 25:28], torch.tensor([1, 1, 1]))\n",
    "# assert torch.equal(test_inputs[\"e1_mask\"][4, 29:31], torch.tensor([1, 1]))\n",
    "# assert torch.equal(test_inputs[\"labels\"], torch.tensor([0, 2, 0, 2, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "steady-fireplace",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMre(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1536, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mre_model = model.BertForMre(NUM_CLASSES)\n",
    "mre_model.load_state_dict(torch.load(\"../../model/re/conll04_50.model\"))\n",
    "mre_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "facial-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers except for the last classifier layer on top\n",
    "for param in mre_model.parameters():\n",
    "    param.requires_grad = False\n",
    "mre_model.classifier.weight.requires_grad = True\n",
    "mre_model.classifier.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "confused-mechanics",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size: torch.Size([30522, 768])\n",
      "False\n",
      "size: torch.Size([512, 768])\n",
      "False\n",
      "size: torch.Size([2, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([3072, 768])\n",
      "False\n",
      "size: torch.Size([3072])\n",
      "False\n",
      "size: torch.Size([768, 3072])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([768, 768])\n",
      "False\n",
      "size: torch.Size([768])\n",
      "False\n",
      "size: torch.Size([6, 1536])\n",
      "True\n",
      "size: torch.Size([6])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for param in mre_model.parameters():\n",
    "    print(\"size:\", param.shape)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "automatic-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(mre_model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consistent-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model():\n",
    "    val_generator = data_generator(\"dev\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in val_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "        \n",
    "    print(\"[validation]\")\n",
    "    result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "designing-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "\n",
    "        for i, inputs in enumerate(data_generator(\"train\"), 0):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = mre_model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            true_labels += inputs[\"labels\"].tolist()\n",
    "            pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "            predicted_labels += pred_labels.tolist()\n",
    "            assert len(predicted_labels) == len(true_labels)\n",
    "            if i % 800 == 799:    # print every 800 mini-batches\n",
    "                print(\"[%d, %5d]\" % (epoch + 1, i + 1))\n",
    "                result = pd.DataFrame(columns=[\"precision\", \"recall\", \"fbeta_score\", \"support\"])\n",
    "                result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "                result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "                print(result)\n",
    "                true_labels = []\n",
    "                predicted_labels = []\n",
    "\n",
    "            del inputs\n",
    "            \n",
    "        validate_model()\n",
    "\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bibliographic-foster",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.754528  0.606628     0.661935      NaN\n",
      "micro   0.932754  0.932754     0.932754      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.746110  0.598716     0.653817      NaN\n",
      "micro   0.905797  0.905797     0.905797      NaN\n",
      "[2,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.750365  0.612338     0.667248      NaN\n",
      "micro   0.932933  0.932933     0.932933      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.718114  0.565275     0.618478      NaN\n",
      "micro   0.899931  0.899931     0.899931      NaN\n",
      "[3,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.756164  0.625380     0.675470      NaN\n",
      "micro   0.934724  0.934724     0.934724      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.729296  0.574840     0.632862      NaN\n",
      "micro   0.902692  0.902692     0.902692      NaN\n",
      "[4,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.762444  0.627995     0.681125      NaN\n",
      "micro   0.935709  0.935709     0.935709      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.735242  0.589775     0.641426      NaN\n",
      "micro   0.903727  0.903727     0.903727      NaN\n",
      "[5,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.767280  0.626867     0.681476      NaN\n",
      "micro   0.936605  0.936605     0.936605      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.749182  0.594158     0.652605      NaN\n",
      "micro   0.905452  0.905452     0.905452      NaN\n",
      "[6,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.758514  0.622484     0.676743      NaN\n",
      "micro   0.935172  0.935172     0.935172      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.755991  0.587486     0.646150      NaN\n",
      "micro   0.907177  0.907177     0.907177      NaN\n",
      "[7,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.764704  0.623297     0.679248      NaN\n",
      "micro   0.935799  0.935799     0.935799      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.740212  0.590374     0.642661      NaN\n",
      "micro   0.903382  0.903382     0.903382      NaN\n",
      "[8,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.766516  0.632528     0.686096      NaN\n",
      "micro   0.936336  0.936336     0.936336      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.743407  0.590901     0.645381      NaN\n",
      "micro   0.905107  0.905107     0.905107      NaN\n",
      "[9,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.746949  0.631785     0.678609      NaN\n",
      "micro   0.934187  0.934187     0.934187      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.739565  0.596122     0.649795      NaN\n",
      "micro   0.903037  0.903037     0.903037      NaN\n",
      "[10,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.755111  0.627716     0.678081      NaN\n",
      "micro   0.934635  0.934635     0.934635      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.726871  0.597963     0.645580      NaN\n",
      "micro   0.902346  0.902346     0.902346      NaN\n",
      "[11,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.761537  0.643476     0.691046      NaN\n",
      "micro   0.936426  0.936426     0.936426      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.743143  0.605611     0.657198      NaN\n",
      "micro   0.904762  0.904762     0.904762      NaN\n",
      "[12,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.745116  0.636404     0.676854      NaN\n",
      "micro   0.933739  0.933739     0.933739      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.715399  0.617044     0.652596      NaN\n",
      "micro   0.903727  0.903727     0.903727      NaN\n",
      "[13,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.743831  0.635470     0.679009      NaN\n",
      "micro   0.933829  0.933829     0.933829      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.734686  0.608272     0.654481      NaN\n",
      "micro   0.903037  0.903037     0.903037      NaN\n",
      "[14,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.752749  0.643128     0.687465      NaN\n",
      "micro   0.936246  0.936246     0.936246      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.744523  0.617793     0.664103      NaN\n",
      "micro   0.906832  0.906832     0.906832      NaN\n",
      "[15,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.752671  0.637869     0.684544      NaN\n",
      "micro   0.935799  0.935799     0.935799      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.756669  0.637972     0.681784      NaN\n",
      "micro   0.909593  0.909593     0.909593      NaN\n",
      "[16,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.763317  0.643345     0.690315      NaN\n",
      "micro   0.936336  0.936336     0.936336      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.760427  0.638535     0.685874      NaN\n",
      "micro   0.910283  0.910283     0.910283      NaN\n",
      "[17,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.755336  0.634625     0.682409      NaN\n",
      "micro   0.935082  0.935082     0.935082      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.746153  0.627263     0.668343      NaN\n",
      "micro   0.906832  0.906832     0.906832      NaN\n",
      "[18,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.757133  0.645000     0.689586      NaN\n",
      "micro   0.936336  0.936336     0.936336      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.728714  0.627622     0.665143      NaN\n",
      "micro   0.904762  0.904762     0.904762      NaN\n",
      "[19,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.761810  0.641556     0.689991      NaN\n",
      "micro   0.936605  0.936605     0.936605      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.725610  0.630199     0.665623      NaN\n",
      "micro   0.903382  0.903382     0.903382      NaN\n",
      "[20,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.746435  0.648306     0.688892      NaN\n",
      "micro   0.935172  0.935172     0.935172      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.744797  0.618842     0.665908      NaN\n",
      "micro   0.906832  0.906832     0.906832      NaN\n",
      "[21,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.766018  0.650052     0.696215      NaN\n",
      "micro   0.937142  0.937142     0.937142      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.726469  0.615468     0.657910      NaN\n",
      "micro   0.904417  0.904417     0.904417      NaN\n",
      "[22,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.771653  0.668056     0.709652      NaN\n",
      "micro   0.939739  0.939739     0.939739      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.741401  0.620955     0.666699      NaN\n",
      "micro   0.905107  0.905107     0.905107      NaN\n",
      "[23,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.769387  0.664210     0.706751      NaN\n",
      "micro   0.939291  0.939291     0.939291      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.737264  0.624331     0.664737      NaN\n",
      "micro   0.905107  0.905107     0.905107      NaN\n",
      "[24,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.758229  0.648181     0.690972      NaN\n",
      "micro   0.936694  0.936694     0.936694      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.723840  0.622085     0.659656      NaN\n",
      "micro   0.904762  0.904762     0.904762      NaN\n",
      "[25,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.763818  0.656311     0.700538      NaN\n",
      "micro   0.938216  0.938216     0.938216      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.736698  0.629056     0.668446      NaN\n",
      "micro   0.904072  0.904072     0.904072      NaN\n",
      "[26,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.791300  0.665290     0.713889      NaN\n",
      "micro   0.940992  0.940992     0.940992      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.738336  0.631420     0.667760      NaN\n",
      "micro   0.905452  0.905452     0.905452      NaN\n",
      "[27,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.775642  0.669190     0.713338      NaN\n",
      "micro   0.940723  0.940723     0.940723      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.735533  0.627023     0.668248      NaN\n",
      "micro   0.906142  0.906142     0.906142      NaN\n",
      "[28,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.762374  0.666919     0.707018      NaN\n",
      "micro   0.938664  0.938664     0.938664      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.734557  0.625193     0.666693      NaN\n",
      "micro   0.905797  0.905797     0.905797      NaN\n",
      "[29,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.764334  0.665707     0.706714      NaN\n",
      "micro   0.938754  0.938754     0.938754      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.736955  0.630851     0.669492      NaN\n",
      "micro   0.907522  0.907522     0.907522      NaN\n",
      "[30,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.775848  0.656286     0.703530      NaN\n",
      "micro   0.939559  0.939559     0.939559      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.750180  0.648281     0.687827      NaN\n",
      "micro   0.910973  0.910973     0.910973      NaN\n",
      "[31,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.758557  0.659586     0.700622      NaN\n",
      "micro   0.937231  0.937231     0.937231      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.735159  0.625019     0.665896      NaN\n",
      "micro   0.905107  0.905107     0.905107      NaN\n",
      "[32,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.756188  0.663096     0.701526      NaN\n",
      "micro   0.937948  0.937948     0.937948      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.730781  0.630595     0.667543      NaN\n",
      "micro   0.905107  0.905107     0.905107      NaN\n",
      "[33,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.776003  0.678571     0.719443      NaN\n",
      "micro   0.940903  0.940903     0.940903      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.731124  0.641719     0.675405      NaN\n",
      "micro   0.907867  0.907867     0.907867      NaN\n",
      "[34,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.771953  0.663162     0.708757      NaN\n",
      "micro   0.939470  0.939470     0.939470      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.765796  0.635449     0.685193      NaN\n",
      "micro   0.911663  0.911663     0.911663      NaN\n",
      "[35,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.763635  0.678636     0.713466      NaN\n",
      "micro   0.939112  0.939112     0.939112      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.740991  0.616555     0.663426      NaN\n",
      "micro   0.906142  0.906142     0.906142      NaN\n",
      "[36,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.760208  0.680324     0.712116      NaN\n",
      "micro   0.939470  0.939470     0.939470      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.740345  0.648817     0.683521      NaN\n",
      "micro   0.908558  0.908558     0.908558      NaN\n",
      "[37,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.768318  0.677219     0.713361      NaN\n",
      "micro   0.939559  0.939559     0.939559      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.745100  0.654072     0.687743      NaN\n",
      "micro   0.909248  0.909248     0.909248      NaN\n",
      "[38,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro    0.77453  0.683226     0.719689      NaN\n",
      "micro    0.94144  0.941440     0.941440      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.744496  0.642674     0.681762      NaN\n",
      "micro   0.908558  0.908558     0.908558      NaN\n",
      "[39,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.788127  0.683101     0.726462      NaN\n",
      "micro   0.942962  0.942962     0.942962      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.707463  0.638672     0.666035      NaN\n",
      "micro   0.906487  0.906487     0.906487      NaN\n",
      "[40,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.773835  0.678449     0.717741      NaN\n",
      "micro   0.940903  0.940903     0.940903      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.724084  0.633516     0.667739      NaN\n",
      "micro   0.908213  0.908213     0.908213      NaN\n",
      "[41,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.790113  0.688282     0.730290      NaN\n",
      "micro   0.943231  0.943231     0.943231      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.716935  0.650264     0.672695      NaN\n",
      "micro   0.902692  0.902692     0.902692      NaN\n",
      "[42,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.784835  0.678220     0.720827      NaN\n",
      "micro   0.941708  0.941708     0.941708      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.749308  0.651498     0.687999      NaN\n",
      "micro   0.908903  0.908903     0.908903      NaN\n",
      "[43,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.763623  0.679493     0.714083      NaN\n",
      "micro   0.939649  0.939649     0.939649      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.735837  0.644924     0.679451      NaN\n",
      "micro   0.906832  0.906832     0.906832      NaN\n",
      "[44,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.765877  0.675639     0.713582      NaN\n",
      "micro   0.939559  0.939559     0.939559      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.740527  0.630756     0.670519      NaN\n",
      "micro   0.907177  0.907177     0.907177      NaN\n",
      "[45,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.784609  0.686853     0.727685      NaN\n",
      "micro   0.942425  0.942425     0.942425      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.722500  0.648683     0.677787      NaN\n",
      "micro   0.908213  0.908213     0.908213      NaN\n",
      "[46,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.772853  0.677364     0.716798      NaN\n",
      "micro   0.941171  0.941171     0.941171      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.726326  0.636808     0.670571      NaN\n",
      "micro   0.905452  0.905452     0.905452      NaN\n",
      "[47,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.793236  0.688096     0.733056      NaN\n",
      "micro   0.944753  0.944753     0.944753      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.735304  0.642916     0.677253      NaN\n",
      "micro   0.906832  0.906832     0.906832      NaN\n",
      "[48,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.781432  0.690444     0.727659      NaN\n",
      "micro   0.942962  0.942962     0.942962      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.716492  0.630620     0.664274      NaN\n",
      "micro   0.902346  0.902346     0.902346      NaN\n",
      "[49,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.775641  0.690414     0.726071      NaN\n",
      "micro   0.942335  0.942335     0.942335      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.728566  0.655535     0.684683      NaN\n",
      "micro   0.907867  0.907867     0.907867      NaN\n",
      "[50,   800]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.768159  0.684149     0.719608      NaN\n",
      "micro   0.940186  0.940186     0.940186      NaN\n",
      "[validation]\n",
      "       precision    recall  fbeta_score  support\n",
      "macro   0.740286  0.649657     0.686264      NaN\n",
      "micro   0.908903  0.908903     0.908903      NaN\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "attractive-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model():\n",
    "    test_generator = data_generator(\"test\")\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    for inputs in test_generator:\n",
    "        # forward\n",
    "        outputs = mre_model(**inputs)\n",
    "        true_labels += inputs[\"labels\"].tolist()\n",
    "        pred_labels = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        predicted_labels += pred_labels.tolist()\n",
    "        assert len(predicted_labels) == len(true_labels)\n",
    "        del inputs\n",
    "    \n",
    "    label_map = {v: k for k, v in conll04_parser.relation_encode.items()}\n",
    "    classes = list(label_map.keys())\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(true_labels, predicted_labels, average=None, labels=classes)\n",
    "    result = pd.DataFrame(index=[label_map[c] for c in classes])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    result.loc[\"macro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"macro\"))\n",
    "    result.loc[\"micro\"] = list(precision_recall_fscore_support(true_labels, predicted_labels, average=\"micro\"))\n",
    "    \n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faced-tunnel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  fbeta_score  support\n",
      "N             0.949753  0.961765     0.955721   3400.0\n",
      "Kill          0.770833  0.787234     0.778947     47.0\n",
      "Located_In    0.695652  0.510638     0.588957     94.0\n",
      "OrgBased_In   0.644737  0.466667     0.541436    105.0\n",
      "Live_In       0.514019  0.550000     0.531401    100.0\n",
      "Work_For      0.696203  0.723684     0.709677     76.0\n",
      "macro         0.711866  0.666665     0.684357      NaN\n",
      "micro         0.919414  0.919414     0.919414      NaN\n"
     ]
    }
   ],
   "source": [
    "result = test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cleared-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(\"conll04_100_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "directed-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mre_model.state_dict(), \"../../model/re/conll04_100.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-trial",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
