{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chinese-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers import BertModel\n",
    "from transformers import BertPreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "invisible-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, BCEWithLogitsLoss\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "marked-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpERT(BertPreTrainedModel):\n",
    "    \"\"\" Span-based model to jointly extract entities and relations \"\"\"\n",
    "\n",
    "    def __init__(self, config: BertConfig, relation_types: int, entity_types: int, \n",
    "                 width_embedding_size: int, prop_drop: float, freeze_transformer: bool, max_pairs: int, \n",
    "                 is_overlapping: bool, relation_filter_threshold: float):\n",
    "        super(SpERT, self).__init__(config)\n",
    "\n",
    "        # BERT model\n",
    "        self.bert = BertModel(config)\n",
    "\n",
    "        # layers\n",
    "        self.relation_classifier = nn.Linear(config.hidden_size * 3 + width_embedding_size * 2, relation_types)\n",
    "        self.entity_classifier = nn.Linear(config.hidden_size * 2 + width_embedding_size, entity_types)\n",
    "        self.width_embedding = nn.Embedding(100, width_embedding_size)\n",
    "        self.dropout = nn.Dropout(prop_drop)\n",
    "\n",
    "        self._hidden_size = config.hidden_size\n",
    "        self._relation_types = relation_types\n",
    "        self._entity_types = entity_types\n",
    "        self._relation_filter_threshold = relation_filter_threshold\n",
    "        self._max_pairs = max_pairs\n",
    "        self._is_overlapping = is_overlapping # whether overlapping entities are allowed\n",
    "\n",
    "        # weight initialization\n",
    "        self.init_weights()\n",
    "\n",
    "        if freeze_transformer:\n",
    "            # freeze all transformer weights\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "                        \n",
    "    def _classify_entity(self, token_embedding, width_embedding, cls_embedding, entity_mask, entity_label):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        token_embedding.shape = (sentence_length, hidden_size)\n",
    "        width_embedding.shape = (entity_count, width_embedding_size)\n",
    "        cls_embedding.shape = (1, hidden_size)\n",
    "        entity_mask.shape = (entity_count, sentence_length)\n",
    "        entity_label.shape = (entity_count,)\n",
    "        \n",
    "        RETURN:\n",
    "        entity_logit.shape = (entity_count, self._entity_types)\n",
    "        entity_loss -> scala\n",
    "        entity_pred.shape = (entity_count,)\n",
    "        \"\"\"\n",
    "        sentence_length = token_embedding.shape[0]\n",
    "        hidden_size = token_embedding.shape[1]\n",
    "        entity_count = entity_mask.shape[0]\n",
    "        \n",
    "        entity_embedding = torch.mul(token_embedding.view(1, sentence_length, hidden_size), \n",
    "                                     entity_mask.view(entity_count, sentence_length, 1))\n",
    "        \n",
    "        entity_embedding = entity_embedding.max(dim=-2)[0] # maxpool\n",
    "        \n",
    "        entity_embedding = torch.cat([entity_embedding, \n",
    "                                      width_embedding, \n",
    "                                      cls_embedding.repeat(entity_count, 1)], dim=1)\n",
    "        \n",
    "        entity_logit = self.entity_classifier(entity_embedding)\n",
    "        entity_loss = None\n",
    "        if entity_label != None:\n",
    "            # If entity labels are provided, calculate cross entropy loss and take the average over all samples\n",
    "            # Refer to the paper\n",
    "            loss_fct = CrossEntropyLoss(reduction='mean')\n",
    "            entity_loss = loss_fct(entity_logit, entity_label)\n",
    "        entity_pred = F.softmax(entity_logit, dim=-1).argmax(dim=-1).long()\n",
    "        \n",
    "        return entity_logit, entity_loss, entity_pred \n",
    "    \n",
    "    \n",
    "    def _filter_span(self, entity_mask: torch.tensor, entity_pred: torch.tensor):\n",
    "        entity_count = entity_mask.shape[0]\n",
    "        sentence_length = entity_mask.shape[1]\n",
    "        entity_span = []\n",
    "        entity_embedding = torch.zeros((sentence_length,)) if not self._is_overlapping else None\n",
    "        \n",
    "        for i in range(entity_count):\n",
    "            if entity_pred[i] != 0:\n",
    "                begin = torch.argmax(entity_mask[i]).item()\n",
    "                end = sentence_length - torch.argmax(entity_mask[i].flip(0)).item()\n",
    "                \n",
    "                assert end > begin\n",
    "                assert entity_mask[i, begin:end].sum() == end - begin\n",
    "                \n",
    "                if self._is_overlapping:\n",
    "                    entity_span.append((begin, end, entity_pred[i].item()))\n",
    "                elif not self._is_overlapping and entity_embedding[begin:end].sum() == 0:\n",
    "                    entity_span.append((begin, end, entity_pred[i].item()))\n",
    "                    entity_embedding[begin:end] = entity_pred[i]\n",
    "        \n",
    "        return entity_span, entity_embedding\n",
    "    \n",
    "    \n",
    "    def _generate_relation_mask(self, entity_span, sentence_length):\n",
    "        relation_mask = []\n",
    "        for e1 in entity_span:\n",
    "            for e2 in entity_span:\n",
    "                c = (min(e1[1], e2[1]), max(e1[0], e2[0]))\n",
    "                if c[1] > c[0]:\n",
    "                    template = [0] * sentence_length\n",
    "                    template[e1[0]: e1[1]] = [1] * (e1[1] - e1[0])\n",
    "                    template[e2[0]: e2[1]] = [2] * (e2[1] - e2[0])\n",
    "                    template[c[0]: c[1]] = [3] * (c[1] - c[0])\n",
    "                    relation_mask.append(template)        \n",
    "        return torch.tensor(relation_mask, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    def _classify_relation(self, token_embedding, e1_width_embedding, e2_width_embedding, \n",
    "                           relation_mask, relation_label):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "        token_embedding.shape = (sentence_length, hidden_size)\n",
    "        e1_width_embedding.shape = (relation_count, width_embedding_size)\n",
    "        e2_width_embedding.shape = (relation_count, width_embedding_size)\n",
    "        relation_mask.shape = (relation_count, sentence_length)\n",
    "        relation_label.shape = (relation_count,)\n",
    "        \n",
    "        RETURN:\n",
    "        relation_logit.shape = (relation_count, self._relation_types)\n",
    "        relation_loss -> scala\n",
    "        relation_pred.shape = (relation_count,)\n",
    "        \"\"\"\n",
    "        sentence_length = token_embedding.shape[0]\n",
    "        hidden_size = token_embedding.shape[1]\n",
    "        relation_count = relation_mask.shape[0]\n",
    "        \n",
    "        e1_embedding = torch.mul(token_embedding.view(1, sentence_length, hidden_size), \n",
    "                                 (relation_mask == 1).view(relation_count, sentence_length, 1))\n",
    "        e1_embedding = e1_embedding.max(dim=-2)[0] # maxpool\n",
    "        \n",
    "        e2_embedding = torch.mul(token_embedding.view(1, sentence_length, hidden_size), \n",
    "                                 (relation_mask == 2).view(relation_count, sentence_length, 1))\n",
    "        e2_embedding = e2_embedding.max(dim=-2)[0] # maxpool\n",
    "        \n",
    "        c_embedding = torch.mul(token_embedding.view(1, sentence_length, hidden_size), \n",
    "                                 (relation_mask == 3).view(relation_count, sentence_length, 1))\n",
    "        c_embedding = c_embedding.max(dim=-2)[0] # maxpool\n",
    "        \n",
    "        relation_embedding = torch.cat([e1_embedding, e1_width_embedding,\n",
    "                                        c_embedding,\n",
    "                                        e2_embedding, e2_width_embedding], dim=1)\n",
    "        \n",
    "        relation_logit = self.relation_classifier(relation_embedding)\n",
    "        relation_loss = None\n",
    "        if relation_label != None:\n",
    "            # If relation labels are provided, calculate the binary cross entropy loss \n",
    "            # and take the sum over all samples\n",
    "            loss_fct = BCEWithLogitsLoss(reduction='sum')\n",
    "            onehot_relation_label = F.one_hot(relation_label, num_classes=self._relation_types).float()\n",
    "            relation_loss = loss_fct(relation_logit, onehot_relation_label)\n",
    "            \n",
    "        relation_softmax = F.softmax(relation_logit, dim=-1)\n",
    "        # Filter out low confident relations\n",
    "        relation_softmax[relation_softmax < self._relation_filter_threshold] = 0\n",
    "        relation_pred = relation_softmax.argmax(dim=-1).long()\n",
    "        \n",
    "        return relation_logit, relation_loss, relation_pred \n",
    "    \n",
    "    \n",
    "    def _filter_relation(self, relation_mask: torch.tensor, relation_pred: torch.tensor):\n",
    "        relation_count = relation_mask.shape[0]\n",
    "        sentence_length = relation_mask.shape[1]\n",
    "        relation_span = []\n",
    "        \n",
    "        for i in range(relation_count):\n",
    "            if relation_pred[i] != 0:\n",
    "                e1_begin = torch.argmax((relation_mask[i] == 1).long()).item()\n",
    "                e1_end = sentence_length - torch.argmax((relation_mask[i].flip(0) == 1).long()).item()\n",
    "                \n",
    "                assert e1_end > e1_begin\n",
    "                assert relation_mask[i, e1_begin:e1_end].sum() == (e1_end - e1_begin) * 1\n",
    "                \n",
    "                e2_begin = torch.argmax((relation_mask[i] == 2).long()).item()\n",
    "                e2_end = sentence_length - torch.argmax((relation_mask[i].flip(0) == 2).long()).item()\n",
    "                \n",
    "                assert e2_end > e2_begin\n",
    "                assert relation_mask[i, e2_begin:e2_end].sum() == (e2_end - e2_begin) * 2\n",
    "                \n",
    "                relation_span.append((e1_begin, e1_end, e2_begin, e2_end, relation_pred[i].item()))\n",
    "        \n",
    "        return relation_span\n",
    "    \n",
    "                \n",
    "    def forward(self, input_ids: torch.tensor, attention_mask: torch.tensor, token_type_ids: torch.tensor, \n",
    "                entity_mask: torch.tensor = None, entity_label: torch.tensor = None, \n",
    "                relation_mask: torch.tensor = None, relation_label: torch.tensor = None,\n",
    "                is_training: bool = True):\n",
    "            \n",
    "        # get the last hidden layer from BERT\n",
    "        bert_embedding = self.bert(input_ids=input_ids, \n",
    "                                   attention_mask=attention_mask, \n",
    "                                   token_type_ids=token_type_ids)['last_hidden_state']\n",
    "        \n",
    "        # get the CLS and other tokens embedding\n",
    "        bert_embedding = torch.reshape(bert_embedding, (-1, self._hidden_size))\n",
    "        cls_embedding = bert_embedding[:1] # CLS is the first element\n",
    "        token_embedding = bert_embedding[1:-1] # everything except CLS and SEP at both ends\n",
    "        \n",
    "        # get the width embedding for each entity length\n",
    "        width_embedding = self.width_embedding(torch.sum(entity_mask, dim=-1))\n",
    "        entity_logit, entity_loss, entity_pred \\\n",
    "            = self._classify_entity(token_embedding, width_embedding, cls_embedding, entity_mask, entity_label)\n",
    "        \n",
    "        entity_span, entity_embedding = self._filter_span(entity_mask, entity_pred)\n",
    "        \n",
    "        # if not relation_mask then generate them from pairs of entities\n",
    "        # only for prediction and evaluation\n",
    "        if not is_training or relation_mask == None:\n",
    "            relation_mask = self._generate_relation_mask(entity_span, token_embedding.shape[0])\n",
    "            relation_label = None\n",
    "        \n",
    "        # return immediately if there is no relations to predict (e.g. there are less than 2 entities)\n",
    "        output = {\n",
    "            \"loss\": entity_loss,\n",
    "            \"entity\": {\n",
    "                \"logit\": entity_logit,\n",
    "                \"pred\": entity_pred,\n",
    "                \"span\": entity_span,\n",
    "                \"embedding\": entity_embedding\n",
    "            },\n",
    "            \"relation\": None\n",
    "        }\n",
    "        if relation_mask == None or torch.equal(relation_mask, torch.tensor([], dtype=torch.long)):\n",
    "            return output\n",
    "        \n",
    "        relation_count = relation_mask.shape[0]\n",
    "        relation_logit = torch.zeros((relation_count, self._relation_types))\n",
    "        relation_loss = []\n",
    "        relation_pred = torch.zeros((relation_count,), dtype=torch.long)\n",
    "        e1_width_embedding = self.width_embedding(torch.sum(relation_mask == 1, dim=-1))\n",
    "        e2_width_embedding = self.width_embedding(torch.sum(relation_mask == 2, dim=-1))\n",
    "        \n",
    "        # break down relation_mask (list of possible relations) to smaller chunks\n",
    "        for i in range(0, relation_count, self._max_pairs):\n",
    "            j = min(relation_count, i + self._max_pairs)\n",
    "            logit, loss, pred = self._classify_relation(token_embedding, \n",
    "                                                        e1_width_embedding[i: j], \n",
    "                                                        e2_width_embedding[i: j], \n",
    "                                                        relation_mask[i: j], \n",
    "                                                        relation_label[i: j] if relation_label != None else None)\n",
    "            relation_logit[i: j] = logit\n",
    "            if loss != None:\n",
    "                relation_loss.append(loss)\n",
    "            relation_pred[i: j] = pred\n",
    "        # relation loss is the average of binary cross entropy loss of each sample\n",
    "        # refer to the paper\n",
    "        relation_loss = None if len(relation_loss) == 0 else (sum(relation_loss) / float(relation_count))\n",
    "        relation_span = self._filter_relation(relation_mask, relation_pred)\n",
    "        # Final loss is the sum of entity_loss and relation_loss\n",
    "        if relation_loss != None: \n",
    "            if output[\"loss\"] == None: \n",
    "                output[\"loss\"] = relation_loss\n",
    "            else:\n",
    "                output[\"loss\"] += relation_loss\n",
    "        output[\"relation\"] = {\n",
    "            \"logit\": relation_logit,\n",
    "            \"pred\": relation_pred,\n",
    "            \"span\": relation_span\n",
    "        }\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conll04_constants as constants\n",
    "import conll04_input_generator as input_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "israeli-oliver",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SpERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SpERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['relation_classifier.weight', 'relation_classifier.bias', 'entity_classifier.weight', 'entity_classifier.bias', 'width_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(constants.model_path)\n",
    "model = SpERT.from_pretrained(constants.model_path,\n",
    "                              config=config,\n",
    "                              # SpERT model parameters\n",
    "                              relation_types=6, \n",
    "                              entity_types=5, \n",
    "                              width_embedding_size=constants.width_embedding_size, \n",
    "                              prop_drop=constants.prop_drop, \n",
    "                              freeze_transformer=True, \n",
    "                              max_pairs=constants.max_pairs, \n",
    "                              is_overlapping=False, \n",
    "                              relation_filter_threshold=constants.relation_filter_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "latin-norway",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (relation_classifier): Linear(in_features=2354, out_features=6, bias=True)\n",
       "  (entity_classifier): Linear(in_features=1561, out_features=5, bias=True)\n",
       "  (width_embedding): Embedding(100, 25)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-evidence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator = input_generator.data_generator(\"train\", device)\n",
    "inputs = next(generator)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-camping",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(6.0548, grad_fn=<AddBackward0>),\n",
       " 'entity': {'logit': tensor([[ 2.8177e-01,  1.2669e-02, -2.2483e-02,  7.4377e-01,  6.4114e-01],\n",
       "          [ 3.6579e-01,  7.8646e-04, -2.0291e-01,  7.8411e-01,  4.7328e-01],\n",
       "          [-3.8452e-02, -1.8831e-01, -2.1909e-01,  8.1786e-01,  8.2508e-01],\n",
       "          [ 4.6173e-01, -1.6989e-01, -3.5750e-01,  1.1920e+00,  6.0526e-01],\n",
       "          [ 9.0851e-02,  2.4033e-01, -3.4296e-01,  5.8736e-01,  6.9949e-01],\n",
       "          [ 2.0702e-01,  8.6041e-02, -3.4433e-01,  9.3151e-01,  8.5519e-01],\n",
       "          [ 1.5012e-01, -3.1589e-02, -1.3847e-01,  7.2958e-01,  8.9096e-01],\n",
       "          [ 1.3835e-01, -1.9620e-01, -4.9140e-01,  8.0954e-01,  2.5814e-01],\n",
       "          [ 1.7310e-01, -1.4149e-02, -3.5881e-01,  9.3199e-01,  8.7219e-01],\n",
       "          [ 1.6613e-01, -1.6769e-01, -6.4479e-01,  7.1199e-01,  5.1751e-01],\n",
       "          [ 4.6173e-01,  9.6146e-02, -6.3711e-03,  8.2203e-01,  8.5123e-01],\n",
       "          [ 1.7755e-01, -2.7787e-01, -4.2383e-01,  7.4441e-01,  5.2356e-01],\n",
       "          [ 9.9975e-02, -7.5132e-02, -2.6795e-01,  6.7249e-01,  8.5580e-01],\n",
       "          [ 6.6750e-01,  1.8657e-01,  7.2402e-02,  1.0451e+00,  8.1962e-01],\n",
       "          [ 5.6482e-01,  3.8926e-03, -1.0826e-01,  1.0118e+00,  7.9538e-01],\n",
       "          [ 5.4538e-01, -5.2672e-02,  3.4258e-02,  1.0948e+00,  6.7946e-01],\n",
       "          [ 3.7276e-03,  1.0472e-01, -2.4651e-01,  8.3703e-01,  5.8390e-01],\n",
       "          [ 1.5692e-01, -1.7722e-01, -2.7597e-01,  8.4934e-01,  5.8085e-01],\n",
       "          [ 4.5169e-01,  1.2375e-01, -3.5125e-01,  1.0346e+00,  6.2421e-01],\n",
       "          [ 5.1560e-01,  1.4317e-01, -1.4986e-01,  8.2671e-01,  8.0259e-01],\n",
       "          [ 2.2257e-01,  1.9108e-01, -2.6576e-01,  6.7484e-01,  6.9046e-01],\n",
       "          [ 3.7660e-01,  3.9021e-02, -3.3509e-01,  7.9089e-01,  8.8859e-01],\n",
       "          [ 2.5526e-01, -2.2583e-01, -2.6333e-01,  8.4747e-01,  5.5845e-01],\n",
       "          [ 4.8884e-02, -4.7706e-02, -3.6357e-01,  8.5422e-01,  5.1388e-01],\n",
       "          [ 2.5546e-01,  7.7642e-02, -1.2582e-01,  8.8456e-01,  8.8481e-01],\n",
       "          [-7.8629e-02,  2.0829e-01, -1.8541e-01,  5.7639e-01,  4.7462e-01],\n",
       "          [ 4.1079e-01,  7.8661e-02, -1.0797e-01,  8.8112e-01,  8.3920e-01],\n",
       "          [ 4.0445e-01,  1.5527e-01, -3.3703e-01,  1.0374e+00,  5.9436e-01],\n",
       "          [ 4.7842e-01, -1.6175e-01, -3.0961e-01,  1.0536e+00,  5.5535e-01],\n",
       "          [ 1.6568e-01, -2.9524e-01, -1.7787e-01,  8.9311e-01,  4.9858e-01],\n",
       "          [ 4.9542e-01,  1.3211e-01, -2.1382e-01,  8.1904e-01,  8.3501e-01],\n",
       "          [ 6.5539e-01,  3.0594e-02, -2.0686e-01,  1.0184e+00,  8.7878e-01],\n",
       "          [ 4.1869e-01,  1.2844e-01, -2.0660e-01,  8.5990e-01,  8.4041e-01],\n",
       "          [ 1.7165e-02, -2.5712e-01, -2.4247e-01,  7.1881e-01,  7.2404e-01],\n",
       "          [ 3.2928e-01,  2.1778e-01, -1.0908e-01,  8.2004e-01,  7.6380e-01],\n",
       "          [ 2.7295e-01, -4.5689e-01, -6.5167e-01,  8.9739e-01,  5.7058e-01],\n",
       "          [ 2.9991e-01,  1.2331e-02, -2.1683e-01,  9.1805e-01,  7.3733e-01],\n",
       "          [ 6.6867e-01,  8.7483e-02,  7.6067e-03,  1.0637e+00,  8.1090e-01],\n",
       "          [ 3.8939e-01,  1.5274e-01, -3.4589e-01,  1.0677e+00,  6.1068e-01],\n",
       "          [ 3.8778e-01, -2.6063e-02, -2.4703e-01,  6.7907e-01,  5.8616e-01],\n",
       "          [ 9.4098e-02,  9.9491e-02, -9.5968e-02,  8.5394e-01,  5.1605e-01],\n",
       "          [ 3.0989e-01,  1.5368e-01, -3.5479e-01,  1.0722e+00,  6.5626e-01],\n",
       "          [ 4.7833e-01,  5.4693e-03, -1.3749e-01,  1.0861e+00,  6.4626e-01],\n",
       "          [ 3.6982e-01, -2.8005e-02, -3.1082e-01,  1.0148e+00,  6.5918e-01],\n",
       "          [ 4.1416e-01,  3.0884e-03, -2.6780e-01,  9.3192e-01,  7.6276e-01],\n",
       "          [ 2.7709e-01,  1.4343e-01, -8.0531e-02,  7.5560e-01,  9.6730e-01],\n",
       "          [ 2.4399e-01,  4.7383e-02, -3.2784e-01,  9.3774e-01,  8.3749e-01],\n",
       "          [ 1.3804e-01,  2.7239e-02, -3.4356e-01,  9.3317e-01,  8.2738e-01],\n",
       "          [ 2.8271e-01, -2.6541e-01, -1.0061e-01,  9.8938e-01,  6.3729e-01],\n",
       "          [ 3.9405e-01,  1.6547e-01, -4.0929e-01,  1.0154e+00,  5.7825e-01],\n",
       "          [ 6.6227e-01, -4.1064e-02, -1.3333e-01,  1.0356e+00,  8.8439e-01],\n",
       "          [ 2.7694e-01, -1.0382e-02, -3.1491e-01,  8.0507e-01,  4.9698e-01],\n",
       "          [ 1.0381e-01, -2.1600e-02, -3.1872e-01,  9.0170e-01,  5.7334e-01],\n",
       "          [ 3.7891e-01,  9.5582e-02, -1.7673e-01,  8.2151e-01,  7.3801e-01],\n",
       "          [ 2.9076e-01, -2.9708e-01, -2.0912e-01,  9.5057e-01,  6.4084e-01],\n",
       "          [ 2.6619e-01,  6.9394e-02, -2.6218e-01,  1.0128e+00,  5.6012e-01],\n",
       "          [ 3.7307e-01, -4.6590e-02, -2.6814e-01,  1.0656e+00,  4.2119e-01],\n",
       "          [ 2.2731e-01,  1.1559e-02, -2.7026e-01,  9.6932e-01,  9.0281e-01],\n",
       "          [ 6.2810e-01,  1.6609e-02, -7.3520e-02,  1.0732e+00,  7.3391e-01],\n",
       "          [ 2.9011e-01,  2.4957e-02, -3.2176e-01,  8.9890e-01,  7.0089e-01],\n",
       "          [ 3.8448e-01,  2.1653e-01, -1.1030e-01,  8.5885e-01,  7.5268e-01],\n",
       "          [ 2.0532e-02,  1.3029e-01, -3.3042e-01,  5.9492e-01,  4.9950e-01],\n",
       "          [ 5.5895e-01, -4.6567e-02, -2.5915e-02,  1.0987e+00,  7.7761e-01],\n",
       "          [ 3.5382e-01,  1.6227e-01, -3.6493e-01,  1.0703e+00,  6.9072e-01],\n",
       "          [ 1.3184e-01, -3.0833e-01, -3.1054e-01,  8.2184e-01,  5.7394e-01],\n",
       "          [ 4.1920e-01, -2.1902e-01, -4.2857e-01,  9.8244e-01,  5.8933e-01],\n",
       "          [ 3.6505e-01,  8.2202e-02, -1.1284e-01,  7.5856e-01,  7.2716e-01],\n",
       "          [ 4.4859e-01,  9.4811e-02, -2.6821e-01,  1.0125e+00,  6.5283e-01],\n",
       "          [ 4.3368e-01, -8.0039e-02, -3.1827e-01,  1.0260e+00,  6.7531e-01],\n",
       "          [ 4.3457e-01,  8.3466e-02, -2.6591e-01,  9.7906e-01,  6.7404e-01],\n",
       "          [ 6.1474e-01, -4.4482e-02, -1.2041e-01,  1.1346e+00,  7.3496e-01],\n",
       "          [-2.1381e-02, -9.3746e-02, -3.4035e-01,  8.4539e-01,  4.8051e-01],\n",
       "          [-1.1324e-01,  8.3830e-02, -3.7675e-01,  6.6181e-01,  3.2620e-01],\n",
       "          [ 3.0187e-01, -1.9314e-02, -3.4561e-01,  8.6151e-01,  3.5761e-01],\n",
       "          [ 8.9046e-02, -7.5981e-02, -3.9614e-01,  9.1628e-01,  8.6617e-01],\n",
       "          [-1.8818e-02, -2.2347e-01, -2.7547e-01,  8.6697e-01,  8.5609e-01],\n",
       "          [ 5.4320e-01,  9.6948e-02, -1.3228e-01,  8.6307e-01,  8.2067e-01],\n",
       "          [ 3.4652e-01,  4.6014e-02, -2.4881e-01,  8.0220e-01,  5.6368e-01],\n",
       "          [ 2.9514e-01,  6.2091e-02, -3.2324e-01,  9.4384e-01,  7.4534e-01],\n",
       "          [ 6.6510e-02, -1.9342e-01, -2.8904e-01,  8.1227e-01,  6.0717e-01],\n",
       "          [ 1.5207e-01, -1.0072e-01, -3.5106e-01,  6.5003e-01,  3.7600e-01],\n",
       "          [ 3.6804e-01,  1.5937e-01, -3.5558e-01,  9.8437e-01,  6.0608e-01],\n",
       "          [ 3.8812e-01,  6.8897e-02, -2.1144e-01,  7.8248e-01,  6.0236e-01],\n",
       "          [ 2.5408e-01, -1.8922e-01, -3.3556e-01,  8.3626e-01,  4.2509e-01],\n",
       "          [ 1.6229e-01, -2.0448e-02, -3.8937e-01,  9.1704e-01,  8.8306e-01],\n",
       "          [-3.1558e-02,  3.5562e-02, -2.6906e-01,  6.4373e-01,  5.3163e-01],\n",
       "          [ 1.2589e-01, -1.7025e-01, -4.7652e-01,  9.3601e-01,  6.9140e-01],\n",
       "          [ 1.4346e-01, -5.1403e-02, -1.2149e-01,  8.8826e-01,  8.5450e-01],\n",
       "          [ 6.5800e-01,  8.7993e-03, -1.1933e-01,  1.0795e+00,  7.1149e-01],\n",
       "          [ 4.7728e-01,  2.2978e-01, -1.6748e-01,  7.8861e-01,  7.9343e-01],\n",
       "          [ 3.0854e-01,  1.4350e-01, -1.1124e-01,  7.6631e-01,  9.8818e-01],\n",
       "          [ 2.9717e-01,  1.1720e-01, -2.2381e-01,  9.0829e-01,  9.2345e-01],\n",
       "          [ 5.4377e-01,  2.9443e-01, -1.9549e-01,  8.9600e-01,  8.0987e-01],\n",
       "          [ 3.4098e-01, -3.6919e-01, -4.8815e-01,  9.5107e-01,  5.8210e-01],\n",
       "          [ 1.9609e-01,  7.5495e-02, -3.5313e-01,  9.6850e-01,  8.5670e-01],\n",
       "          [ 4.0740e-01, -3.1544e-01, -5.6682e-01,  8.5552e-01,  4.6666e-01],\n",
       "          [ 2.6548e-01, -2.4336e-02, -3.8207e-01,  9.1466e-01,  6.9989e-01],\n",
       "          [ 7.2837e-02, -1.8188e-01, -4.2536e-01,  8.6363e-01,  4.3446e-01],\n",
       "          [-6.1225e-02,  1.3810e-01, -2.0089e-01,  6.8198e-01,  1.4104e-01],\n",
       "          [ 3.0830e-01,  1.9305e-02, -3.3929e-01,  9.9236e-01,  5.3996e-01],\n",
       "          [ 2.1240e-02,  4.8640e-02, -2.8131e-01,  6.7124e-01,  6.3004e-01],\n",
       "          [ 6.5026e-01, -1.1663e-03, -1.2619e-01,  1.0467e+00,  8.9509e-01],\n",
       "          [ 2.4002e-01,  2.7087e-02, -3.3856e-02,  8.2122e-01,  8.6044e-01]],\n",
       "         grad_fn=<AddmmBackward>),\n",
       "  'pred': tensor([3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "          4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 4]),\n",
       "  'span': [(21, 23, 3),\n",
       "   (24, 27, 3),\n",
       "   (28, 30, 4),\n",
       "   (40, 43, 3),\n",
       "   (5, 12, 4),\n",
       "   (43, 44, 3),\n",
       "   (16, 18, 3),\n",
       "   (12, 15, 3),\n",
       "   (0, 2, 3),\n",
       "   (19, 21, 3),\n",
       "   (34, 40, 3),\n",
       "   (2, 3, 3),\n",
       "   (15, 16, 3),\n",
       "   (30, 32, 3)],\n",
       "  'embedding': tensor([3., 3., 3., 0., 0., 4., 4., 4., 4., 4., 4., 4., 3., 3., 3., 3., 3., 3.,\n",
       "          0., 3., 3., 3., 3., 0., 3., 3., 3., 0., 4., 4., 3., 3., 0., 0., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 3., 3.])},\n",
       " 'relation': {'logit': tensor([[-0.0988, -0.1076,  0.5334, -0.1317,  0.0497,  0.1911],\n",
       "          [ 0.2751, -0.0478, -0.0318,  0.0573,  0.1461, -0.1379],\n",
       "          [ 0.1365,  0.4908,  0.2196,  0.2389,  0.2098, -0.3061],\n",
       "          [ 0.2662,  0.0607,  0.3816,  0.1557, -0.1884,  0.0193],\n",
       "          [-0.2373,  0.2175,  0.5483, -0.1614,  0.3876, -0.0501],\n",
       "          [ 0.2663, -0.1530,  0.1452, -0.0556,  0.0858, -0.0536]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  'pred': tensor([0, 0, 0, 0, 0, 0]),\n",
       "  'span': []}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, is_training=True)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consistent-trout",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.7337, grad_fn=<NllLossBackward>),\n",
       " 'entity': {'logit': tensor([[ 2.8177e-01,  1.2669e-02, -2.2483e-02,  7.4377e-01,  6.4114e-01],\n",
       "          [ 3.6579e-01,  7.8646e-04, -2.0291e-01,  7.8411e-01,  4.7328e-01],\n",
       "          [-3.8452e-02, -1.8831e-01, -2.1909e-01,  8.1786e-01,  8.2508e-01],\n",
       "          [ 4.6173e-01, -1.6989e-01, -3.5750e-01,  1.1920e+00,  6.0526e-01],\n",
       "          [ 9.0851e-02,  2.4033e-01, -3.4296e-01,  5.8736e-01,  6.9949e-01],\n",
       "          [ 2.0702e-01,  8.6041e-02, -3.4433e-01,  9.3151e-01,  8.5519e-01],\n",
       "          [ 1.5012e-01, -3.1589e-02, -1.3847e-01,  7.2958e-01,  8.9096e-01],\n",
       "          [ 1.3835e-01, -1.9620e-01, -4.9140e-01,  8.0954e-01,  2.5814e-01],\n",
       "          [ 1.7310e-01, -1.4149e-02, -3.5881e-01,  9.3199e-01,  8.7219e-01],\n",
       "          [ 1.6613e-01, -1.6769e-01, -6.4479e-01,  7.1199e-01,  5.1751e-01],\n",
       "          [ 4.6173e-01,  9.6146e-02, -6.3711e-03,  8.2203e-01,  8.5123e-01],\n",
       "          [ 1.7755e-01, -2.7787e-01, -4.2383e-01,  7.4441e-01,  5.2356e-01],\n",
       "          [ 9.9975e-02, -7.5132e-02, -2.6795e-01,  6.7249e-01,  8.5580e-01],\n",
       "          [ 6.6750e-01,  1.8657e-01,  7.2402e-02,  1.0451e+00,  8.1962e-01],\n",
       "          [ 5.6482e-01,  3.8926e-03, -1.0826e-01,  1.0118e+00,  7.9538e-01],\n",
       "          [ 5.4538e-01, -5.2672e-02,  3.4258e-02,  1.0948e+00,  6.7946e-01],\n",
       "          [ 3.7276e-03,  1.0472e-01, -2.4651e-01,  8.3703e-01,  5.8390e-01],\n",
       "          [ 1.5692e-01, -1.7722e-01, -2.7597e-01,  8.4934e-01,  5.8085e-01],\n",
       "          [ 4.5169e-01,  1.2375e-01, -3.5125e-01,  1.0346e+00,  6.2421e-01],\n",
       "          [ 5.1560e-01,  1.4317e-01, -1.4986e-01,  8.2671e-01,  8.0259e-01],\n",
       "          [ 2.2257e-01,  1.9108e-01, -2.6576e-01,  6.7484e-01,  6.9046e-01],\n",
       "          [ 3.7660e-01,  3.9021e-02, -3.3509e-01,  7.9089e-01,  8.8859e-01],\n",
       "          [ 2.5526e-01, -2.2583e-01, -2.6333e-01,  8.4747e-01,  5.5845e-01],\n",
       "          [ 4.8884e-02, -4.7706e-02, -3.6357e-01,  8.5422e-01,  5.1388e-01],\n",
       "          [ 2.5546e-01,  7.7642e-02, -1.2582e-01,  8.8456e-01,  8.8481e-01],\n",
       "          [-7.8629e-02,  2.0829e-01, -1.8541e-01,  5.7639e-01,  4.7462e-01],\n",
       "          [ 4.1079e-01,  7.8661e-02, -1.0797e-01,  8.8112e-01,  8.3920e-01],\n",
       "          [ 4.0445e-01,  1.5527e-01, -3.3703e-01,  1.0374e+00,  5.9436e-01],\n",
       "          [ 4.7842e-01, -1.6175e-01, -3.0961e-01,  1.0536e+00,  5.5535e-01],\n",
       "          [ 1.6568e-01, -2.9524e-01, -1.7787e-01,  8.9311e-01,  4.9858e-01],\n",
       "          [ 4.9542e-01,  1.3211e-01, -2.1382e-01,  8.1904e-01,  8.3501e-01],\n",
       "          [ 6.5539e-01,  3.0594e-02, -2.0686e-01,  1.0184e+00,  8.7878e-01],\n",
       "          [ 4.1869e-01,  1.2844e-01, -2.0660e-01,  8.5990e-01,  8.4041e-01],\n",
       "          [ 1.7165e-02, -2.5712e-01, -2.4247e-01,  7.1881e-01,  7.2404e-01],\n",
       "          [ 3.2928e-01,  2.1778e-01, -1.0908e-01,  8.2004e-01,  7.6380e-01],\n",
       "          [ 2.7295e-01, -4.5689e-01, -6.5167e-01,  8.9739e-01,  5.7058e-01],\n",
       "          [ 2.9991e-01,  1.2331e-02, -2.1683e-01,  9.1805e-01,  7.3733e-01],\n",
       "          [ 6.6867e-01,  8.7483e-02,  7.6067e-03,  1.0637e+00,  8.1090e-01],\n",
       "          [ 3.8939e-01,  1.5274e-01, -3.4589e-01,  1.0677e+00,  6.1068e-01],\n",
       "          [ 3.8778e-01, -2.6063e-02, -2.4703e-01,  6.7907e-01,  5.8616e-01],\n",
       "          [ 9.4098e-02,  9.9491e-02, -9.5968e-02,  8.5394e-01,  5.1605e-01],\n",
       "          [ 3.0989e-01,  1.5368e-01, -3.5479e-01,  1.0722e+00,  6.5626e-01],\n",
       "          [ 4.7833e-01,  5.4693e-03, -1.3749e-01,  1.0861e+00,  6.4626e-01],\n",
       "          [ 3.6982e-01, -2.8005e-02, -3.1082e-01,  1.0148e+00,  6.5918e-01],\n",
       "          [ 4.1416e-01,  3.0884e-03, -2.6780e-01,  9.3192e-01,  7.6276e-01],\n",
       "          [ 2.7709e-01,  1.4343e-01, -8.0531e-02,  7.5560e-01,  9.6730e-01],\n",
       "          [ 2.4399e-01,  4.7383e-02, -3.2784e-01,  9.3774e-01,  8.3749e-01],\n",
       "          [ 1.3804e-01,  2.7239e-02, -3.4356e-01,  9.3317e-01,  8.2738e-01],\n",
       "          [ 2.8271e-01, -2.6541e-01, -1.0061e-01,  9.8938e-01,  6.3729e-01],\n",
       "          [ 3.9405e-01,  1.6547e-01, -4.0929e-01,  1.0154e+00,  5.7825e-01],\n",
       "          [ 6.6227e-01, -4.1064e-02, -1.3333e-01,  1.0356e+00,  8.8439e-01],\n",
       "          [ 2.7694e-01, -1.0382e-02, -3.1491e-01,  8.0507e-01,  4.9698e-01],\n",
       "          [ 1.0381e-01, -2.1600e-02, -3.1872e-01,  9.0170e-01,  5.7334e-01],\n",
       "          [ 3.7891e-01,  9.5582e-02, -1.7673e-01,  8.2151e-01,  7.3801e-01],\n",
       "          [ 2.9076e-01, -2.9708e-01, -2.0912e-01,  9.5057e-01,  6.4084e-01],\n",
       "          [ 2.6619e-01,  6.9394e-02, -2.6218e-01,  1.0128e+00,  5.6012e-01],\n",
       "          [ 3.7307e-01, -4.6590e-02, -2.6814e-01,  1.0656e+00,  4.2119e-01],\n",
       "          [ 2.2731e-01,  1.1559e-02, -2.7026e-01,  9.6932e-01,  9.0281e-01],\n",
       "          [ 6.2810e-01,  1.6609e-02, -7.3520e-02,  1.0732e+00,  7.3391e-01],\n",
       "          [ 2.9011e-01,  2.4957e-02, -3.2176e-01,  8.9890e-01,  7.0089e-01],\n",
       "          [ 3.8448e-01,  2.1653e-01, -1.1030e-01,  8.5885e-01,  7.5268e-01],\n",
       "          [ 2.0532e-02,  1.3029e-01, -3.3042e-01,  5.9492e-01,  4.9950e-01],\n",
       "          [ 5.5895e-01, -4.6567e-02, -2.5915e-02,  1.0987e+00,  7.7761e-01],\n",
       "          [ 3.5382e-01,  1.6227e-01, -3.6493e-01,  1.0703e+00,  6.9072e-01],\n",
       "          [ 1.3184e-01, -3.0833e-01, -3.1054e-01,  8.2184e-01,  5.7394e-01],\n",
       "          [ 4.1920e-01, -2.1902e-01, -4.2857e-01,  9.8244e-01,  5.8933e-01],\n",
       "          [ 3.6505e-01,  8.2202e-02, -1.1284e-01,  7.5856e-01,  7.2716e-01],\n",
       "          [ 4.4859e-01,  9.4811e-02, -2.6821e-01,  1.0125e+00,  6.5283e-01],\n",
       "          [ 4.3368e-01, -8.0039e-02, -3.1827e-01,  1.0260e+00,  6.7531e-01],\n",
       "          [ 4.3457e-01,  8.3466e-02, -2.6591e-01,  9.7906e-01,  6.7404e-01],\n",
       "          [ 6.1474e-01, -4.4482e-02, -1.2041e-01,  1.1346e+00,  7.3496e-01],\n",
       "          [-2.1381e-02, -9.3746e-02, -3.4035e-01,  8.4539e-01,  4.8051e-01],\n",
       "          [-1.1324e-01,  8.3830e-02, -3.7675e-01,  6.6181e-01,  3.2620e-01],\n",
       "          [ 3.0187e-01, -1.9314e-02, -3.4561e-01,  8.6151e-01,  3.5761e-01],\n",
       "          [ 8.9046e-02, -7.5981e-02, -3.9614e-01,  9.1628e-01,  8.6617e-01],\n",
       "          [-1.8818e-02, -2.2347e-01, -2.7547e-01,  8.6697e-01,  8.5609e-01],\n",
       "          [ 5.4320e-01,  9.6948e-02, -1.3228e-01,  8.6307e-01,  8.2067e-01],\n",
       "          [ 3.4652e-01,  4.6014e-02, -2.4881e-01,  8.0220e-01,  5.6368e-01],\n",
       "          [ 2.9514e-01,  6.2091e-02, -3.2324e-01,  9.4384e-01,  7.4534e-01],\n",
       "          [ 6.6510e-02, -1.9342e-01, -2.8904e-01,  8.1227e-01,  6.0717e-01],\n",
       "          [ 1.5207e-01, -1.0072e-01, -3.5106e-01,  6.5003e-01,  3.7600e-01],\n",
       "          [ 3.6804e-01,  1.5937e-01, -3.5558e-01,  9.8437e-01,  6.0608e-01],\n",
       "          [ 3.8812e-01,  6.8897e-02, -2.1144e-01,  7.8248e-01,  6.0236e-01],\n",
       "          [ 2.5408e-01, -1.8922e-01, -3.3556e-01,  8.3626e-01,  4.2509e-01],\n",
       "          [ 1.6229e-01, -2.0448e-02, -3.8937e-01,  9.1704e-01,  8.8306e-01],\n",
       "          [-3.1558e-02,  3.5562e-02, -2.6906e-01,  6.4373e-01,  5.3163e-01],\n",
       "          [ 1.2589e-01, -1.7025e-01, -4.7652e-01,  9.3601e-01,  6.9140e-01],\n",
       "          [ 1.4346e-01, -5.1403e-02, -1.2149e-01,  8.8826e-01,  8.5450e-01],\n",
       "          [ 6.5800e-01,  8.7993e-03, -1.1933e-01,  1.0795e+00,  7.1149e-01],\n",
       "          [ 4.7728e-01,  2.2978e-01, -1.6748e-01,  7.8861e-01,  7.9343e-01],\n",
       "          [ 3.0854e-01,  1.4350e-01, -1.1124e-01,  7.6631e-01,  9.8818e-01],\n",
       "          [ 2.9717e-01,  1.1720e-01, -2.2381e-01,  9.0829e-01,  9.2345e-01],\n",
       "          [ 5.4377e-01,  2.9443e-01, -1.9549e-01,  8.9600e-01,  8.0987e-01],\n",
       "          [ 3.4098e-01, -3.6919e-01, -4.8815e-01,  9.5107e-01,  5.8210e-01],\n",
       "          [ 1.9609e-01,  7.5495e-02, -3.5313e-01,  9.6850e-01,  8.5670e-01],\n",
       "          [ 4.0740e-01, -3.1544e-01, -5.6682e-01,  8.5552e-01,  4.6666e-01],\n",
       "          [ 2.6548e-01, -2.4336e-02, -3.8207e-01,  9.1466e-01,  6.9989e-01],\n",
       "          [ 7.2837e-02, -1.8188e-01, -4.2536e-01,  8.6363e-01,  4.3446e-01],\n",
       "          [-6.1225e-02,  1.3810e-01, -2.0089e-01,  6.8198e-01,  1.4104e-01],\n",
       "          [ 3.0830e-01,  1.9305e-02, -3.3929e-01,  9.9236e-01,  5.3996e-01],\n",
       "          [ 2.1240e-02,  4.8640e-02, -2.8131e-01,  6.7124e-01,  6.3004e-01],\n",
       "          [ 6.5026e-01, -1.1663e-03, -1.2619e-01,  1.0467e+00,  8.9509e-01],\n",
       "          [ 2.4002e-01,  2.7087e-02, -3.3856e-02,  8.2122e-01,  8.6044e-01]],\n",
       "         grad_fn=<AddmmBackward>),\n",
       "  'pred': tensor([3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "          4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 4]),\n",
       "  'span': [(21, 23, 3),\n",
       "   (24, 27, 3),\n",
       "   (28, 30, 4),\n",
       "   (40, 43, 3),\n",
       "   (5, 12, 4),\n",
       "   (43, 44, 3),\n",
       "   (16, 18, 3),\n",
       "   (12, 15, 3),\n",
       "   (0, 2, 3),\n",
       "   (19, 21, 3),\n",
       "   (34, 40, 3),\n",
       "   (2, 3, 3),\n",
       "   (15, 16, 3),\n",
       "   (30, 32, 3)],\n",
       "  'embedding': tensor([3., 3., 3., 0., 0., 4., 4., 4., 4., 4., 4., 4., 3., 3., 3., 3., 3., 3.,\n",
       "          0., 3., 3., 3., 3., 0., 3., 3., 3., 0., 4., 4., 3., 3., 0., 0., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 3., 3.])},\n",
       " 'relation': {'logit': tensor([[ 2.6622e-01,  6.0671e-02,  3.8161e-01,  1.5568e-01, -1.8836e-01,\n",
       "            1.9343e-02],\n",
       "          [-9.8756e-02, -1.0759e-01,  5.3339e-01, -1.3167e-01,  4.9718e-02,\n",
       "            1.9108e-01],\n",
       "          [-2.8315e-01,  1.7643e-01,  8.2004e-01, -1.6041e-01,  6.5978e-01,\n",
       "            6.8824e-01],\n",
       "          [ 3.2675e-01,  2.4856e-01,  9.8601e-01,  1.6534e-01,  4.0929e-01,\n",
       "            3.9949e-01],\n",
       "          [-3.9677e-01, -1.4827e-01,  7.6153e-01, -5.8644e-02,  7.9752e-01,\n",
       "            1.4386e-01],\n",
       "          [ 1.8044e-01,  1.2414e-01,  2.0894e-01,  2.7586e-01,  5.2179e-01,\n",
       "            3.5623e-01],\n",
       "          [ 2.9375e-01,  5.3578e-01,  7.3172e-01,  5.6485e-01,  1.2648e-01,\n",
       "            5.3062e-01],\n",
       "          [-1.2718e-01,  1.4994e-01,  4.0391e-01, -5.9308e-02,  6.2562e-01,\n",
       "            3.2249e-01],\n",
       "          [-2.0606e-01, -1.5780e-01,  9.5580e-01,  9.1567e-03,  5.3721e-01,\n",
       "            1.8436e-01],\n",
       "          [-2.2468e-01,  1.1792e-01,  5.4526e-01,  1.0211e-01,  6.5229e-01,\n",
       "            5.1482e-01],\n",
       "          [ 4.3218e-01,  2.3736e-01,  4.6133e-01,  1.0047e-01,  6.0327e-01,\n",
       "            3.6220e-01],\n",
       "          [-1.4772e-01, -2.3396e-01,  4.5405e-01, -2.2541e-01,  4.4287e-01,\n",
       "            2.8142e-01],\n",
       "          [ 2.6631e-01, -1.5296e-01,  1.4516e-01, -5.5578e-02,  8.5812e-02,\n",
       "           -5.3584e-02],\n",
       "          [ 2.7513e-01, -4.7849e-02, -3.1751e-02,  5.7339e-02,  1.4609e-01,\n",
       "           -1.3785e-01],\n",
       "          [-6.2526e-02,  1.7599e-01,  7.4962e-01, -2.6689e-01,  8.0289e-01,\n",
       "            6.7533e-01],\n",
       "          [ 4.1527e-01, -1.2611e-03,  9.9102e-01, -1.3547e-01,  7.4591e-01,\n",
       "            3.3099e-01],\n",
       "          [-1.6895e-01, -1.7574e-01,  7.2271e-01, -1.9116e-01,  9.7045e-01,\n",
       "            1.6202e-01],\n",
       "          [ 2.2476e-01, -2.3063e-01,  2.9253e-01, -2.1250e-02,  1.0129e+00,\n",
       "            2.9263e-01],\n",
       "          [ 3.6206e-01,  2.4777e-01,  7.5546e-01,  3.1235e-01,  6.1721e-01,\n",
       "            3.9975e-01],\n",
       "          [ 5.5615e-02, -3.9552e-02,  4.1899e-01, -2.9276e-01,  9.2928e-01,\n",
       "            3.3357e-01],\n",
       "          [ 4.2798e-01, -4.2591e-01,  1.8172e-01,  4.2324e-03,  8.7947e-01,\n",
       "            3.4072e-01],\n",
       "          [ 2.6366e-02, -1.0804e-01,  8.2916e-01, -6.3261e-02,  7.2730e-01,\n",
       "            8.1201e-02],\n",
       "          [-5.1692e-02, -1.0456e-01,  5.5965e-01, -1.4799e-01,  9.6420e-01,\n",
       "            5.0576e-01],\n",
       "          [ 4.9514e-01, -7.8732e-02,  5.1155e-01, -1.7498e-01,  1.0697e+00,\n",
       "            2.4040e-01],\n",
       "          [ 3.3904e-02, -2.2095e-01,  1.8803e-01, -2.0800e-01,  5.6027e-01,\n",
       "            2.3513e-01],\n",
       "          [-2.3732e-01,  2.1747e-01,  5.4831e-01, -1.6137e-01,  3.8761e-01,\n",
       "           -5.0118e-02],\n",
       "          [ 1.3648e-01,  4.9083e-01,  2.1961e-01,  2.3889e-01,  2.0981e-01,\n",
       "           -3.0613e-01],\n",
       "          [-1.9845e-01,  6.2087e-01,  1.1142e+00,  1.8686e-02,  7.9587e-01,\n",
       "            8.5148e-01],\n",
       "          [-1.1369e-02,  5.3450e-01,  1.2303e+00, -1.0179e-01,  7.5494e-01,\n",
       "            2.9712e-01],\n",
       "          [-3.3831e-01,  3.0414e-01,  1.0363e+00,  5.4328e-02,  9.7777e-01,\n",
       "            3.0575e-01],\n",
       "          [-2.0322e-01,  3.8172e-01,  4.8245e-01, -1.0532e-01,  9.4326e-01,\n",
       "            3.9321e-01],\n",
       "          [-1.0170e-01,  7.9285e-01,  9.7785e-01,  2.9183e-01,  6.5627e-01,\n",
       "            4.7304e-01],\n",
       "          [-2.5663e-01,  4.5909e-01,  6.2698e-01, -2.6996e-01,  9.7363e-01,\n",
       "            2.5338e-01],\n",
       "          [-3.3819e-02,  1.2339e-01,  4.5216e-01, -3.5898e-02,  8.7263e-01,\n",
       "            4.3357e-01],\n",
       "          [-4.1403e-02,  3.1424e-01,  1.2253e+00,  1.3726e-01,  6.0207e-01,\n",
       "            2.2569e-01],\n",
       "          [-3.6982e-01,  3.8515e-01,  7.5359e-01, -1.3122e-01,  1.0089e+00,\n",
       "            4.4134e-01],\n",
       "          [-4.2096e-03,  4.8378e-01,  7.0702e-01, -2.3111e-01,  1.0391e+00,\n",
       "            3.2032e-01],\n",
       "          [-7.5120e-01, -3.4001e-01,  9.4773e-01,  5.8318e-02,  3.9110e-01,\n",
       "           -1.0541e-01],\n",
       "          [-5.3067e-01, -1.2683e-01,  1.1138e+00,  1.6309e-01,  2.6003e-01,\n",
       "           -4.5396e-02],\n",
       "          [-5.2794e-01, -2.2063e-01,  1.2270e+00,  2.6712e-01,  1.8929e-01,\n",
       "            2.9903e-01],\n",
       "          [-3.6816e-01, -1.7308e-01,  1.3243e+00,  1.0401e-01,  6.5430e-01,\n",
       "            2.1223e-01],\n",
       "          [-5.5244e-01, -3.6783e-01,  6.5283e-01,  6.4504e-02,  8.6613e-01,\n",
       "            4.3045e-01],\n",
       "          [-5.0401e-01,  5.4940e-02,  1.1346e+00,  4.7113e-01,  5.5636e-01,\n",
       "            4.3633e-01],\n",
       "          [-5.7649e-01, -2.4632e-01,  7.2604e-01, -3.7622e-02,  8.5644e-01,\n",
       "            2.0608e-01],\n",
       "          [-4.8071e-01, -4.8135e-01,  8.5660e-01,  2.0448e-01,  8.2891e-01,\n",
       "            3.7236e-01],\n",
       "          [-6.8511e-01, -3.1548e-01,  8.3804e-01,  8.4745e-02,  8.9768e-01,\n",
       "            4.0761e-01],\n",
       "          [-4.0333e-01, -2.3992e-01,  8.2727e-01, -4.8990e-02,  9.2860e-01,\n",
       "            3.0378e-01],\n",
       "          [-5.1906e-01, -3.1918e-01,  9.7289e-01,  2.4984e-01,  6.4304e-01,\n",
       "            3.6872e-01],\n",
       "          [-8.0152e-01,  4.4577e-01,  6.6515e-01, -4.7944e-02,  3.4723e-01,\n",
       "           -1.1081e-01],\n",
       "          [-7.1310e-01,  4.0958e-01,  9.0661e-01, -1.3750e-01,  4.0967e-01,\n",
       "           -1.0639e-01],\n",
       "          [-1.0011e+00,  4.0666e-01,  8.9454e-01, -2.8537e-01,  3.5499e-01,\n",
       "            2.8010e-02],\n",
       "          [-1.0284e+00,  5.4058e-01,  8.7575e-01, -3.2800e-01,  8.6092e-01,\n",
       "            4.9558e-01],\n",
       "          [-1.1576e+00,  2.0867e-01,  7.4623e-01, -2.6195e-01,  9.8430e-01,\n",
       "           -5.2745e-02],\n",
       "          [-6.0947e-01,  4.2262e-01,  2.0159e-01, -2.1878e-01,  5.6524e-01,\n",
       "            5.8821e-01],\n",
       "          [-6.2794e-01,  4.9381e-01,  3.4406e-01, -1.8399e-01,  4.0782e-01,\n",
       "            1.8867e-01],\n",
       "          [-4.6401e-01,  3.1044e-01,  5.6483e-01,  3.7266e-02,  6.0739e-01,\n",
       "            4.5922e-01],\n",
       "          [-1.0212e+00,  3.1327e-01,  1.0967e+00, -1.4752e-01,  7.2705e-01,\n",
       "            5.7604e-02],\n",
       "          [-6.3243e-01,  5.7513e-01,  5.0074e-01,  1.4963e-02,  4.9232e-01,\n",
       "            2.9036e-01],\n",
       "          [-4.8795e-01,  3.9507e-01,  2.5820e-01, -3.5804e-01,  5.6391e-01,\n",
       "            3.5317e-01],\n",
       "          [-1.0021e+00,  2.9444e-01,  7.7561e-01, -3.1883e-01,  7.2211e-01,\n",
       "            1.6608e-01],\n",
       "          [-4.1141e-01,  1.9214e-01,  9.9866e-01, -1.8760e-01,  3.4866e-01,\n",
       "           -1.9698e-01],\n",
       "          [-1.8369e-01,  3.7830e-01,  1.1963e+00, -1.0886e-01,  2.4743e-01,\n",
       "           -1.0590e-01],\n",
       "          [-2.1439e-01,  3.1949e-01,  1.2585e+00, -4.4923e-02,  1.9103e-01,\n",
       "            2.0610e-01],\n",
       "          [-4.3947e-02,  3.5186e-01,  1.3042e+00, -1.7762e-01,  5.9751e-01,\n",
       "            1.1671e-01],\n",
       "          [-2.0653e-01,  1.5666e-01,  6.9528e-01, -2.4280e-01,  8.3540e-01,\n",
       "            3.6181e-01],\n",
       "          [-1.8603e-01,  5.4949e-01,  1.1415e+00,  1.7762e-01,  4.9168e-01,\n",
       "            3.5403e-01],\n",
       "          [-2.5323e-01,  2.6539e-01,  7.1059e-01, -3.1930e-01,  8.1117e-01,\n",
       "            7.9734e-02],\n",
       "          [-1.4700e-01,  3.4068e-02,  8.9766e-01, -4.8287e-02,  7.7645e-01,\n",
       "            2.8106e-01],\n",
       "          [-1.7069e-01,  4.0401e-01,  1.0838e+00,  1.6654e-01,  2.6617e-01,\n",
       "            2.0304e-01],\n",
       "          [-3.4842e-01,  2.1481e-01,  8.2193e-01, -2.0345e-01,  8.4657e-01,\n",
       "            2.8434e-01],\n",
       "          [-8.3636e-02,  2.5655e-01,  8.2224e-01, -3.4820e-01,  8.5657e-01,\n",
       "            2.2937e-01],\n",
       "          [-1.8569e-01,  2.1394e-01,  1.0165e+00, -8.7852e-02,  6.3859e-01,\n",
       "            2.9377e-01],\n",
       "          [-7.7912e-01,  4.5512e-01,  1.8946e-01,  5.8711e-02,  2.7128e-02,\n",
       "           -5.4744e-01],\n",
       "          [-7.3489e-01,  3.1397e-01,  5.0950e-01, -2.7146e-02,  2.4402e-01,\n",
       "           -5.3812e-01],\n",
       "          [-1.0242e+00,  3.8764e-01,  4.4805e-01, -2.9276e-01,  1.1070e-01,\n",
       "           -2.6926e-01],\n",
       "          [-1.0440e+00,  4.7959e-01,  5.0566e-01, -3.7137e-01,  6.4015e-01,\n",
       "            3.2043e-01],\n",
       "          [-4.4075e-01,  5.5639e-01,  5.0296e-01, -2.2265e-01,  1.3264e-01,\n",
       "            1.9484e-01],\n",
       "          [-1.1514e+00,  1.4723e-01,  4.3866e-01, -3.3100e-01,  7.8959e-01,\n",
       "           -2.0102e-01],\n",
       "          [-3.6662e-01,  8.2183e-01,  4.5657e-02,  1.6071e-01, -2.2356e-01,\n",
       "            4.8269e-01],\n",
       "          [-8.9783e-01,  3.6754e-01,  8.9185e-02, -3.4770e-01,  4.6316e-01,\n",
       "            4.2465e-02],\n",
       "          [-3.0010e-01,  3.7905e-01, -1.0694e-01,  1.5536e-01,  1.6140e-01,\n",
       "            1.7291e-01],\n",
       "          [-1.0292e+00,  2.2748e-01,  7.3126e-01, -1.7210e-01,  5.1172e-01,\n",
       "           -1.9200e-01],\n",
       "          [-9.8584e-01,  3.5047e-01,  2.3985e-01, -1.9265e-01,  4.7677e-01,\n",
       "            2.3482e-01],\n",
       "          [-1.0352e+00,  2.4805e-01,  3.8866e-01, -4.0207e-01,  4.6589e-01,\n",
       "           -1.4028e-01],\n",
       "          [-6.9383e-01,  6.1070e-01,  8.2630e-01, -2.3260e-01, -1.9138e-01,\n",
       "            1.4389e-01],\n",
       "          [-6.2561e-01,  5.3632e-01,  1.0865e+00, -2.7383e-01,  2.5183e-02,\n",
       "            8.5955e-02],\n",
       "          [-9.5072e-01,  5.4271e-01,  1.0575e+00, -4.7591e-01,  5.2324e-04,\n",
       "            3.2752e-01],\n",
       "          [-1.0235e+00,  6.4631e-01,  1.1015e+00, -5.4503e-01,  5.0719e-01,\n",
       "            8.4326e-01],\n",
       "          [-1.1590e+00,  2.8400e-01,  9.9896e-01, -4.9086e-01,  6.2268e-01,\n",
       "            3.0815e-01],\n",
       "          [-3.9464e-01,  5.6577e-01,  1.5973e-01, -4.1959e-01, -4.6749e-02,\n",
       "            9.9964e-01],\n",
       "          [-8.0857e-01,  4.6591e-01,  6.5330e-01, -5.7198e-01,  3.2742e-01,\n",
       "            3.0130e-01],\n",
       "          [-2.1289e-01,  5.2486e-01,  7.6088e-01, -1.9521e-01,  2.3970e-02,\n",
       "            7.5629e-01],\n",
       "          [-9.7965e-01,  4.0763e-01,  1.3067e+00, -3.5918e-01,  3.7768e-01,\n",
       "            3.7076e-01],\n",
       "          [-8.8628e-01,  4.6055e-01,  8.3520e-01, -4.1285e-01,  3.1082e-01,\n",
       "            4.5531e-01],\n",
       "          [-9.6394e-01,  4.0860e-01,  9.9226e-01, -5.5669e-01,  3.4003e-01,\n",
       "            4.6406e-01],\n",
       "          [-8.5616e-01,  5.4399e-01,  6.8976e-01, -3.3415e-01,  1.6785e-01,\n",
       "           -1.5899e-01],\n",
       "          [-6.7346e-01,  5.6813e-01,  9.4129e-01, -3.5634e-01,  1.9733e-01,\n",
       "           -7.4987e-02],\n",
       "          [-8.4705e-01,  5.2809e-01,  8.9791e-01, -5.1510e-01,  1.7796e-01,\n",
       "            1.3102e-02],\n",
       "          [-8.3743e-01,  6.6419e-01,  8.8420e-01, -5.3119e-01,  6.6735e-01,\n",
       "            5.1825e-01],\n",
       "          [-2.2864e-01,  6.9066e-01,  9.5077e-01, -2.4555e-01,  1.2108e-02,\n",
       "            2.1750e-01],\n",
       "          [-9.6757e-01,  3.1904e-01,  7.5930e-01, -4.6519e-01,  8.0225e-01,\n",
       "           -6.0901e-02],\n",
       "          [-6.6725e-01,  4.3062e-01,  3.9452e-01, -4.0539e-01,  5.0005e-01,\n",
       "            4.6465e-01],\n",
       "          [-5.4997e-01,  7.8505e-01,  8.4457e-01, -4.9379e-02,  1.8750e-01,\n",
       "            2.0654e-01],\n",
       "          [-5.6885e-01,  4.0509e-01,  6.0493e-01, -2.1819e-01,  4.9438e-01,\n",
       "            3.6402e-01],\n",
       "          [-8.5072e-01,  4.4557e-01,  1.0830e+00, -3.5981e-01,  5.3944e-01,\n",
       "            6.2440e-02],\n",
       "          [-5.6323e-01,  4.6183e-01,  5.1527e-01, -5.6757e-01,  5.5994e-01,\n",
       "            2.5088e-01],\n",
       "          [-8.3598e-01,  4.0714e-01,  7.6351e-01, -5.3302e-01,  5.6335e-01,\n",
       "            1.5692e-01],\n",
       "          [-5.9466e-01, -2.6923e-02,  4.9476e-01, -4.2324e-01,  2.5723e-01,\n",
       "           -4.0538e-01],\n",
       "          [-9.1780e-01, -1.6308e-02,  5.1383e-01, -6.4492e-01,  1.8666e-01,\n",
       "           -1.4426e-01],\n",
       "          [-1.0352e+00,  2.2046e-01,  8.0550e-01, -6.5297e-01,  7.4952e-01,\n",
       "            3.4698e-01],\n",
       "          [-3.5828e-01,  2.9859e-01,  9.6228e-01, -3.8818e-01,  3.2138e-01,\n",
       "            1.5050e-01],\n",
       "          [-1.1549e+00, -1.2098e-01,  7.3712e-01, -5.5806e-01,  8.7722e-01,\n",
       "           -1.9713e-01],\n",
       "          [-3.6308e-01,  2.3344e-01, -1.0865e-02, -2.6621e-01,  3.0799e-01,\n",
       "            2.5755e-01],\n",
       "          [-2.4785e-01,  6.3531e-01,  7.4288e-01, -3.6488e-02, -6.2531e-03,\n",
       "            3.2398e-01],\n",
       "          [-8.6241e-01,  1.9640e-01,  3.9567e-01, -5.8207e-01,  6.0408e-01,\n",
       "            2.6473e-02],\n",
       "          [-9.8191e-01, -1.0465e-01,  9.4340e-01, -5.0388e-01,  6.4215e-01,\n",
       "           -1.4828e-01],\n",
       "          [-9.6076e-01,  1.6196e-01,  5.3783e-01, -4.2441e-01,  6.2603e-01,\n",
       "            2.2504e-01],\n",
       "          [-1.0434e-01,  3.5013e-01,  5.0083e-01, -5.1714e-01,  4.2983e-01,\n",
       "            1.4515e-01],\n",
       "          [-9.3926e-01, -1.5956e-01,  4.4275e-01, -7.4305e-01,  5.4282e-01,\n",
       "           -3.9445e-02],\n",
       "          [-7.9146e-01,  1.6362e-02,  4.6452e-01, -3.4651e-01,  1.9946e-01,\n",
       "           -1.2997e-01],\n",
       "          [-5.5913e-01,  2.7975e-01,  5.7433e-01, -2.0767e-01,  1.1537e-01,\n",
       "           -1.6020e-01],\n",
       "          [-4.8825e-01,  1.6335e-01,  7.1906e-01, -1.8870e-01, -7.3582e-02,\n",
       "            1.5256e-01],\n",
       "          [-4.7830e-01,  2.9023e-01,  9.2628e-01, -2.8990e-01,  4.5135e-01,\n",
       "            2.5358e-01],\n",
       "          [-7.4145e-01,  2.3776e-01,  3.5537e-01, -6.0164e-02,  3.7726e-01,\n",
       "            2.2955e-01],\n",
       "          [-6.5503e-01,  7.0672e-02,  2.5946e-01, -3.1061e-01,  6.6862e-01,\n",
       "            3.9735e-01],\n",
       "          [-5.7748e-01,  5.0687e-01,  7.2086e-01,  8.2606e-02,  3.5777e-01,\n",
       "            4.4316e-01],\n",
       "          [-7.0715e-01,  2.2568e-01,  3.0591e-01, -4.4063e-01,  6.5945e-01,\n",
       "            2.2959e-01],\n",
       "          [-5.4477e-01, -1.1584e-01,  3.7552e-01, -2.2082e-01,  6.5246e-01,\n",
       "            3.5642e-01],\n",
       "          [-8.1572e-01,  1.5667e-01,  4.2331e-01, -3.1757e-01,  6.9663e-01,\n",
       "            4.3209e-01],\n",
       "          [-4.8139e-01,  2.0561e-01,  4.2145e-01, -4.3578e-01,  7.3387e-01,\n",
       "            3.0207e-01],\n",
       "          [-4.5078e-01,  1.4170e-01,  3.3538e-01,  1.8479e-02,  2.6664e-01,\n",
       "            2.1420e-01],\n",
       "          [-6.6318e-01,  4.0565e-01,  6.0526e-01, -1.3973e-01,  1.3662e-01,\n",
       "           -1.8337e-01],\n",
       "          [-4.9029e-01,  3.9679e-01,  8.5610e-01, -1.7858e-01,  1.7436e-01,\n",
       "           -1.1950e-01],\n",
       "          [-6.6976e-01,  3.4782e-01,  7.9868e-01, -3.4336e-01,  1.5532e-01,\n",
       "           -1.5650e-02],\n",
       "          [-6.5556e-01,  4.8870e-01,  7.7036e-01, -3.7583e-01,  6.5069e-01,\n",
       "            5.0307e-01],\n",
       "          [ 5.7347e-02,  6.6565e-01,  8.8160e-01, -1.3602e-02,  3.8712e-02,\n",
       "            1.0247e-01],\n",
       "          [-7.7228e-01,  1.6213e-01,  6.4480e-01, -3.1635e-01,  7.7976e-01,\n",
       "           -7.3002e-02],\n",
       "          [-4.6478e-01,  3.0722e-01,  3.1934e-01, -2.1735e-01,  4.5576e-01,\n",
       "            4.4030e-01],\n",
       "          [-3.3720e-01,  6.7335e-01,  8.0061e-01,  1.4274e-01,  1.1300e-01,\n",
       "            1.4384e-01],\n",
       "          [-3.7672e-01,  2.6432e-01,  5.2123e-01, -2.7536e-02,  4.5843e-01,\n",
       "            3.4588e-01],\n",
       "          [-6.6882e-01,  2.7023e-01,  9.7459e-01, -2.0375e-01,  5.1872e-01,\n",
       "            4.8230e-02],\n",
       "          [-3.5689e-01,  3.4180e-01,  4.4954e-01, -3.9113e-01,  4.9424e-01,\n",
       "            2.0384e-01],\n",
       "          [-6.4951e-01,  2.3430e-01,  6.5556e-01, -3.7488e-01,  5.3463e-01,\n",
       "            1.3718e-01],\n",
       "          [-4.1843e-01,  6.3742e-01,  6.8527e-01,  1.8178e-03,  2.3901e-02,\n",
       "           -1.7448e-01],\n",
       "          [-3.5556e-01,  5.3495e-01,  9.7193e-01, -6.2379e-02,  2.1618e-01,\n",
       "           -2.2336e-01],\n",
       "          [-7.1626e-01,  5.5878e-01,  9.1604e-01, -3.0006e-01,  1.2185e-01,\n",
       "            2.4838e-02],\n",
       "          [-7.8589e-01,  6.7659e-01,  9.2352e-01, -3.6637e-01,  6.1791e-01,\n",
       "            5.6074e-01],\n",
       "          [-2.1029e-01,  5.9792e-01,  8.0299e-01, -2.4341e-01,  4.6603e-02,\n",
       "            3.2679e-01],\n",
       "          [-9.1960e-01,  3.1619e-01,  8.0905e-01, -3.1790e-01,  7.2605e-01,\n",
       "            3.3529e-02],\n",
       "          [-6.8486e-01,  4.6783e-01,  4.5335e-01, -3.9138e-01,  4.3834e-01,\n",
       "            1.9568e-01],\n",
       "          [ 6.7595e-02,  5.6482e-01,  6.4818e-01,  2.2931e-02,  1.9853e-01,\n",
       "            4.2749e-01],\n",
       "          [-7.4659e-01,  4.3150e-01,  1.1367e+00, -1.7877e-01,  4.9226e-01,\n",
       "            7.9712e-02],\n",
       "          [-7.6900e-01,  4.5413e-01,  6.1347e-01, -2.4794e-01,  4.3054e-01,\n",
       "            3.6535e-01],\n",
       "          [-7.3794e-01,  4.2585e-01,  8.2972e-01, -3.8410e-01,  4.6886e-01,\n",
       "            1.6036e-01],\n",
       "          [-7.1245e-01,  1.6936e-01,  3.8279e-01, -4.5014e-01, -3.9184e-02,\n",
       "            5.3912e-02],\n",
       "          [-5.3092e-01,  3.9600e-01,  3.5322e-01, -2.2147e-01, -1.9596e-01,\n",
       "            8.0547e-02],\n",
       "          [-6.1574e-01,  6.0059e-01,  7.7394e-01, -1.9361e-01,  4.2967e-01,\n",
       "            9.3487e-01],\n",
       "          [-4.3851e-01,  5.0055e-01,  1.0252e+00, -3.3028e-01,  3.0212e-01,\n",
       "            4.4887e-01],\n",
       "          [-7.3577e-01,  2.7686e-01,  7.0814e-01, -1.8362e-01,  6.0538e-01,\n",
       "            4.0711e-01],\n",
       "          [-6.4033e-01,  3.2040e-01,  3.3689e-01, -4.0964e-01,  4.7850e-01,\n",
       "            5.3588e-01],\n",
       "          [-5.4109e-01,  7.3701e-01,  8.2642e-01,  1.6024e-02,  1.7582e-01,\n",
       "            6.2327e-01],\n",
       "          [-6.7173e-01,  4.1641e-01,  4.0640e-01, -4.8291e-01,  5.3906e-01,\n",
       "            4.1089e-01],\n",
       "          [-4.8144e-01,  5.8394e-02,  2.9490e-01, -3.2905e-01,  4.0884e-01,\n",
       "            5.5208e-01],\n",
       "          [-4.3010e-01,  3.7085e-01,  7.5541e-01,  1.4941e-01,  1.2235e-01,\n",
       "            3.0102e-01],\n",
       "          [-7.7574e-01,  3.4990e-01,  5.2430e-01, -3.5775e-01,  5.6825e-01,\n",
       "            6.0786e-01],\n",
       "          [-4.5206e-01,  4.2912e-01,  5.3452e-01, -5.1017e-01,  5.6618e-01,\n",
       "            4.6954e-01]], grad_fn=<CopyBackwards>),\n",
       "  'pred': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'span': [(12, 15, 34, 40, 2)]}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**inputs, is_training=False)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decent-battle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(1.7337, grad_fn=<NllLossBackward>),\n",
       " 'entity': {'logit': tensor([[ 2.8177e-01,  1.2669e-02, -2.2483e-02,  7.4377e-01,  6.4114e-01],\n",
       "          [ 3.6579e-01,  7.8646e-04, -2.0291e-01,  7.8411e-01,  4.7328e-01],\n",
       "          [-3.8452e-02, -1.8831e-01, -2.1909e-01,  8.1786e-01,  8.2508e-01],\n",
       "          [ 4.6173e-01, -1.6989e-01, -3.5750e-01,  1.1920e+00,  6.0526e-01],\n",
       "          [ 9.0851e-02,  2.4033e-01, -3.4296e-01,  5.8736e-01,  6.9949e-01],\n",
       "          [ 2.0702e-01,  8.6041e-02, -3.4433e-01,  9.3151e-01,  8.5519e-01],\n",
       "          [ 1.5012e-01, -3.1589e-02, -1.3847e-01,  7.2958e-01,  8.9096e-01],\n",
       "          [ 1.3835e-01, -1.9620e-01, -4.9140e-01,  8.0954e-01,  2.5814e-01],\n",
       "          [ 1.7310e-01, -1.4149e-02, -3.5881e-01,  9.3199e-01,  8.7219e-01],\n",
       "          [ 1.6613e-01, -1.6769e-01, -6.4479e-01,  7.1199e-01,  5.1751e-01],\n",
       "          [ 4.6173e-01,  9.6146e-02, -6.3711e-03,  8.2203e-01,  8.5123e-01],\n",
       "          [ 1.7755e-01, -2.7787e-01, -4.2383e-01,  7.4441e-01,  5.2356e-01],\n",
       "          [ 9.9975e-02, -7.5132e-02, -2.6795e-01,  6.7249e-01,  8.5580e-01],\n",
       "          [ 6.6750e-01,  1.8657e-01,  7.2402e-02,  1.0451e+00,  8.1962e-01],\n",
       "          [ 5.6482e-01,  3.8926e-03, -1.0826e-01,  1.0118e+00,  7.9538e-01],\n",
       "          [ 5.4538e-01, -5.2672e-02,  3.4258e-02,  1.0948e+00,  6.7946e-01],\n",
       "          [ 3.7276e-03,  1.0472e-01, -2.4651e-01,  8.3703e-01,  5.8390e-01],\n",
       "          [ 1.5692e-01, -1.7722e-01, -2.7597e-01,  8.4934e-01,  5.8085e-01],\n",
       "          [ 4.5169e-01,  1.2375e-01, -3.5125e-01,  1.0346e+00,  6.2421e-01],\n",
       "          [ 5.1560e-01,  1.4317e-01, -1.4986e-01,  8.2671e-01,  8.0259e-01],\n",
       "          [ 2.2257e-01,  1.9108e-01, -2.6576e-01,  6.7484e-01,  6.9046e-01],\n",
       "          [ 3.7660e-01,  3.9021e-02, -3.3509e-01,  7.9089e-01,  8.8859e-01],\n",
       "          [ 2.5526e-01, -2.2583e-01, -2.6333e-01,  8.4747e-01,  5.5845e-01],\n",
       "          [ 4.8884e-02, -4.7706e-02, -3.6357e-01,  8.5422e-01,  5.1388e-01],\n",
       "          [ 2.5546e-01,  7.7642e-02, -1.2582e-01,  8.8456e-01,  8.8481e-01],\n",
       "          [-7.8629e-02,  2.0829e-01, -1.8541e-01,  5.7639e-01,  4.7462e-01],\n",
       "          [ 4.1079e-01,  7.8661e-02, -1.0797e-01,  8.8112e-01,  8.3920e-01],\n",
       "          [ 4.0445e-01,  1.5527e-01, -3.3703e-01,  1.0374e+00,  5.9436e-01],\n",
       "          [ 4.7842e-01, -1.6175e-01, -3.0961e-01,  1.0536e+00,  5.5535e-01],\n",
       "          [ 1.6568e-01, -2.9524e-01, -1.7787e-01,  8.9311e-01,  4.9858e-01],\n",
       "          [ 4.9542e-01,  1.3211e-01, -2.1382e-01,  8.1904e-01,  8.3501e-01],\n",
       "          [ 6.5539e-01,  3.0594e-02, -2.0686e-01,  1.0184e+00,  8.7878e-01],\n",
       "          [ 4.1869e-01,  1.2844e-01, -2.0660e-01,  8.5990e-01,  8.4041e-01],\n",
       "          [ 1.7165e-02, -2.5712e-01, -2.4247e-01,  7.1881e-01,  7.2404e-01],\n",
       "          [ 3.2928e-01,  2.1778e-01, -1.0908e-01,  8.2004e-01,  7.6380e-01],\n",
       "          [ 2.7295e-01, -4.5689e-01, -6.5167e-01,  8.9739e-01,  5.7058e-01],\n",
       "          [ 2.9991e-01,  1.2331e-02, -2.1683e-01,  9.1805e-01,  7.3733e-01],\n",
       "          [ 6.6867e-01,  8.7483e-02,  7.6067e-03,  1.0637e+00,  8.1090e-01],\n",
       "          [ 3.8939e-01,  1.5274e-01, -3.4589e-01,  1.0677e+00,  6.1068e-01],\n",
       "          [ 3.8778e-01, -2.6063e-02, -2.4703e-01,  6.7907e-01,  5.8616e-01],\n",
       "          [ 9.4098e-02,  9.9491e-02, -9.5968e-02,  8.5394e-01,  5.1605e-01],\n",
       "          [ 3.0989e-01,  1.5368e-01, -3.5479e-01,  1.0722e+00,  6.5626e-01],\n",
       "          [ 4.7833e-01,  5.4693e-03, -1.3749e-01,  1.0861e+00,  6.4626e-01],\n",
       "          [ 3.6982e-01, -2.8005e-02, -3.1082e-01,  1.0148e+00,  6.5918e-01],\n",
       "          [ 4.1416e-01,  3.0884e-03, -2.6780e-01,  9.3192e-01,  7.6276e-01],\n",
       "          [ 2.7709e-01,  1.4343e-01, -8.0531e-02,  7.5560e-01,  9.6730e-01],\n",
       "          [ 2.4399e-01,  4.7383e-02, -3.2784e-01,  9.3774e-01,  8.3749e-01],\n",
       "          [ 1.3804e-01,  2.7239e-02, -3.4356e-01,  9.3317e-01,  8.2738e-01],\n",
       "          [ 2.8271e-01, -2.6541e-01, -1.0061e-01,  9.8938e-01,  6.3729e-01],\n",
       "          [ 3.9405e-01,  1.6547e-01, -4.0929e-01,  1.0154e+00,  5.7825e-01],\n",
       "          [ 6.6227e-01, -4.1064e-02, -1.3333e-01,  1.0356e+00,  8.8439e-01],\n",
       "          [ 2.7694e-01, -1.0382e-02, -3.1491e-01,  8.0507e-01,  4.9698e-01],\n",
       "          [ 1.0381e-01, -2.1600e-02, -3.1872e-01,  9.0170e-01,  5.7334e-01],\n",
       "          [ 3.7891e-01,  9.5582e-02, -1.7673e-01,  8.2151e-01,  7.3801e-01],\n",
       "          [ 2.9076e-01, -2.9708e-01, -2.0912e-01,  9.5057e-01,  6.4084e-01],\n",
       "          [ 2.6619e-01,  6.9394e-02, -2.6218e-01,  1.0128e+00,  5.6012e-01],\n",
       "          [ 3.7307e-01, -4.6590e-02, -2.6814e-01,  1.0656e+00,  4.2119e-01],\n",
       "          [ 2.2731e-01,  1.1559e-02, -2.7026e-01,  9.6932e-01,  9.0281e-01],\n",
       "          [ 6.2810e-01,  1.6609e-02, -7.3520e-02,  1.0732e+00,  7.3391e-01],\n",
       "          [ 2.9011e-01,  2.4957e-02, -3.2176e-01,  8.9890e-01,  7.0089e-01],\n",
       "          [ 3.8448e-01,  2.1653e-01, -1.1030e-01,  8.5885e-01,  7.5268e-01],\n",
       "          [ 2.0532e-02,  1.3029e-01, -3.3042e-01,  5.9492e-01,  4.9950e-01],\n",
       "          [ 5.5895e-01, -4.6567e-02, -2.5915e-02,  1.0987e+00,  7.7761e-01],\n",
       "          [ 3.5382e-01,  1.6227e-01, -3.6493e-01,  1.0703e+00,  6.9072e-01],\n",
       "          [ 1.3184e-01, -3.0833e-01, -3.1054e-01,  8.2184e-01,  5.7394e-01],\n",
       "          [ 4.1920e-01, -2.1902e-01, -4.2857e-01,  9.8244e-01,  5.8933e-01],\n",
       "          [ 3.6505e-01,  8.2202e-02, -1.1284e-01,  7.5856e-01,  7.2716e-01],\n",
       "          [ 4.4859e-01,  9.4811e-02, -2.6821e-01,  1.0125e+00,  6.5283e-01],\n",
       "          [ 4.3368e-01, -8.0039e-02, -3.1827e-01,  1.0260e+00,  6.7531e-01],\n",
       "          [ 4.3457e-01,  8.3466e-02, -2.6591e-01,  9.7906e-01,  6.7404e-01],\n",
       "          [ 6.1474e-01, -4.4482e-02, -1.2041e-01,  1.1346e+00,  7.3496e-01],\n",
       "          [-2.1381e-02, -9.3746e-02, -3.4035e-01,  8.4539e-01,  4.8051e-01],\n",
       "          [-1.1324e-01,  8.3830e-02, -3.7675e-01,  6.6181e-01,  3.2620e-01],\n",
       "          [ 3.0187e-01, -1.9314e-02, -3.4561e-01,  8.6151e-01,  3.5761e-01],\n",
       "          [ 8.9046e-02, -7.5981e-02, -3.9614e-01,  9.1628e-01,  8.6617e-01],\n",
       "          [-1.8818e-02, -2.2347e-01, -2.7547e-01,  8.6697e-01,  8.5609e-01],\n",
       "          [ 5.4320e-01,  9.6948e-02, -1.3228e-01,  8.6307e-01,  8.2067e-01],\n",
       "          [ 3.4652e-01,  4.6014e-02, -2.4881e-01,  8.0220e-01,  5.6368e-01],\n",
       "          [ 2.9514e-01,  6.2091e-02, -3.2324e-01,  9.4384e-01,  7.4534e-01],\n",
       "          [ 6.6510e-02, -1.9342e-01, -2.8904e-01,  8.1227e-01,  6.0717e-01],\n",
       "          [ 1.5207e-01, -1.0072e-01, -3.5106e-01,  6.5003e-01,  3.7600e-01],\n",
       "          [ 3.6804e-01,  1.5937e-01, -3.5558e-01,  9.8437e-01,  6.0608e-01],\n",
       "          [ 3.8812e-01,  6.8897e-02, -2.1144e-01,  7.8248e-01,  6.0236e-01],\n",
       "          [ 2.5408e-01, -1.8922e-01, -3.3556e-01,  8.3626e-01,  4.2509e-01],\n",
       "          [ 1.6229e-01, -2.0448e-02, -3.8937e-01,  9.1704e-01,  8.8306e-01],\n",
       "          [-3.1558e-02,  3.5562e-02, -2.6906e-01,  6.4373e-01,  5.3163e-01],\n",
       "          [ 1.2589e-01, -1.7025e-01, -4.7652e-01,  9.3601e-01,  6.9140e-01],\n",
       "          [ 1.4346e-01, -5.1403e-02, -1.2149e-01,  8.8826e-01,  8.5450e-01],\n",
       "          [ 6.5800e-01,  8.7993e-03, -1.1933e-01,  1.0795e+00,  7.1149e-01],\n",
       "          [ 4.7728e-01,  2.2978e-01, -1.6748e-01,  7.8861e-01,  7.9343e-01],\n",
       "          [ 3.0854e-01,  1.4350e-01, -1.1124e-01,  7.6631e-01,  9.8818e-01],\n",
       "          [ 2.9717e-01,  1.1720e-01, -2.2381e-01,  9.0829e-01,  9.2345e-01],\n",
       "          [ 5.4377e-01,  2.9443e-01, -1.9549e-01,  8.9600e-01,  8.0987e-01],\n",
       "          [ 3.4098e-01, -3.6919e-01, -4.8815e-01,  9.5107e-01,  5.8210e-01],\n",
       "          [ 1.9609e-01,  7.5495e-02, -3.5313e-01,  9.6850e-01,  8.5670e-01],\n",
       "          [ 4.0740e-01, -3.1544e-01, -5.6682e-01,  8.5552e-01,  4.6666e-01],\n",
       "          [ 2.6548e-01, -2.4336e-02, -3.8207e-01,  9.1466e-01,  6.9989e-01],\n",
       "          [ 7.2837e-02, -1.8188e-01, -4.2536e-01,  8.6363e-01,  4.3446e-01],\n",
       "          [-6.1225e-02,  1.3810e-01, -2.0089e-01,  6.8198e-01,  1.4104e-01],\n",
       "          [ 3.0830e-01,  1.9305e-02, -3.3929e-01,  9.9236e-01,  5.3996e-01],\n",
       "          [ 2.1240e-02,  4.8640e-02, -2.8131e-01,  6.7124e-01,  6.3004e-01],\n",
       "          [ 6.5026e-01, -1.1663e-03, -1.2619e-01,  1.0467e+00,  8.9509e-01],\n",
       "          [ 2.4002e-01,  2.7087e-02, -3.3856e-02,  8.2122e-01,  8.6044e-01]],\n",
       "         grad_fn=<AddmmBackward>),\n",
       "  'pred': tensor([3, 3, 4, 3, 4, 3, 4, 3, 3, 3, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3,\n",
       "          4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3,\n",
       "          3, 3, 3, 3, 3, 3, 4]),\n",
       "  'span': [(21, 23, 3),\n",
       "   (24, 27, 3),\n",
       "   (28, 30, 4),\n",
       "   (40, 43, 3),\n",
       "   (5, 12, 4),\n",
       "   (43, 44, 3),\n",
       "   (16, 18, 3),\n",
       "   (12, 15, 3),\n",
       "   (0, 2, 3),\n",
       "   (19, 21, 3),\n",
       "   (34, 40, 3),\n",
       "   (2, 3, 3),\n",
       "   (15, 16, 3),\n",
       "   (30, 32, 3)],\n",
       "  'embedding': tensor([3., 3., 3., 0., 0., 4., 4., 4., 4., 4., 4., 4., 3., 3., 3., 3., 3., 3.,\n",
       "          0., 3., 3., 3., 3., 0., 3., 3., 3., 0., 4., 4., 3., 3., 0., 0., 3., 3.,\n",
       "          3., 3., 3., 3., 3., 3., 3., 3.])},\n",
       " 'relation': {'logit': tensor([[ 2.6622e-01,  6.0671e-02,  3.8161e-01,  1.5568e-01, -1.8836e-01,\n",
       "            1.9343e-02],\n",
       "          [-9.8756e-02, -1.0759e-01,  5.3339e-01, -1.3167e-01,  4.9718e-02,\n",
       "            1.9108e-01],\n",
       "          [-2.8315e-01,  1.7643e-01,  8.2004e-01, -1.6041e-01,  6.5978e-01,\n",
       "            6.8824e-01],\n",
       "          [ 3.2675e-01,  2.4856e-01,  9.8601e-01,  1.6534e-01,  4.0929e-01,\n",
       "            3.9949e-01],\n",
       "          [-3.9677e-01, -1.4827e-01,  7.6153e-01, -5.8644e-02,  7.9752e-01,\n",
       "            1.4386e-01],\n",
       "          [ 1.8044e-01,  1.2414e-01,  2.0894e-01,  2.7586e-01,  5.2179e-01,\n",
       "            3.5623e-01],\n",
       "          [ 2.9375e-01,  5.3578e-01,  7.3172e-01,  5.6485e-01,  1.2648e-01,\n",
       "            5.3062e-01],\n",
       "          [-1.2718e-01,  1.4994e-01,  4.0391e-01, -5.9308e-02,  6.2562e-01,\n",
       "            3.2249e-01],\n",
       "          [-2.0606e-01, -1.5780e-01,  9.5580e-01,  9.1567e-03,  5.3721e-01,\n",
       "            1.8436e-01],\n",
       "          [-2.2468e-01,  1.1792e-01,  5.4526e-01,  1.0211e-01,  6.5229e-01,\n",
       "            5.1482e-01],\n",
       "          [ 4.3218e-01,  2.3736e-01,  4.6133e-01,  1.0047e-01,  6.0327e-01,\n",
       "            3.6220e-01],\n",
       "          [-1.4772e-01, -2.3396e-01,  4.5405e-01, -2.2541e-01,  4.4287e-01,\n",
       "            2.8142e-01],\n",
       "          [ 2.6631e-01, -1.5296e-01,  1.4516e-01, -5.5578e-02,  8.5812e-02,\n",
       "           -5.3584e-02],\n",
       "          [ 2.7513e-01, -4.7849e-02, -3.1751e-02,  5.7339e-02,  1.4609e-01,\n",
       "           -1.3785e-01],\n",
       "          [-6.2526e-02,  1.7599e-01,  7.4962e-01, -2.6689e-01,  8.0289e-01,\n",
       "            6.7533e-01],\n",
       "          [ 4.1527e-01, -1.2611e-03,  9.9102e-01, -1.3547e-01,  7.4591e-01,\n",
       "            3.3099e-01],\n",
       "          [-1.6895e-01, -1.7574e-01,  7.2271e-01, -1.9116e-01,  9.7045e-01,\n",
       "            1.6202e-01],\n",
       "          [ 2.2476e-01, -2.3063e-01,  2.9253e-01, -2.1250e-02,  1.0129e+00,\n",
       "            2.9263e-01],\n",
       "          [ 3.6206e-01,  2.4777e-01,  7.5546e-01,  3.1235e-01,  6.1721e-01,\n",
       "            3.9975e-01],\n",
       "          [ 5.5615e-02, -3.9552e-02,  4.1899e-01, -2.9276e-01,  9.2928e-01,\n",
       "            3.3357e-01],\n",
       "          [ 4.2798e-01, -4.2591e-01,  1.8172e-01,  4.2324e-03,  8.7947e-01,\n",
       "            3.4072e-01],\n",
       "          [ 2.6366e-02, -1.0804e-01,  8.2916e-01, -6.3261e-02,  7.2730e-01,\n",
       "            8.1201e-02],\n",
       "          [-5.1692e-02, -1.0456e-01,  5.5965e-01, -1.4799e-01,  9.6420e-01,\n",
       "            5.0576e-01],\n",
       "          [ 4.9514e-01, -7.8732e-02,  5.1155e-01, -1.7498e-01,  1.0697e+00,\n",
       "            2.4040e-01],\n",
       "          [ 3.3904e-02, -2.2095e-01,  1.8803e-01, -2.0800e-01,  5.6027e-01,\n",
       "            2.3513e-01],\n",
       "          [-2.3732e-01,  2.1747e-01,  5.4831e-01, -1.6137e-01,  3.8761e-01,\n",
       "           -5.0118e-02],\n",
       "          [ 1.3648e-01,  4.9083e-01,  2.1961e-01,  2.3889e-01,  2.0981e-01,\n",
       "           -3.0613e-01],\n",
       "          [-1.9845e-01,  6.2087e-01,  1.1142e+00,  1.8686e-02,  7.9587e-01,\n",
       "            8.5148e-01],\n",
       "          [-1.1369e-02,  5.3450e-01,  1.2303e+00, -1.0179e-01,  7.5494e-01,\n",
       "            2.9712e-01],\n",
       "          [-3.3831e-01,  3.0414e-01,  1.0363e+00,  5.4328e-02,  9.7777e-01,\n",
       "            3.0575e-01],\n",
       "          [-2.0322e-01,  3.8172e-01,  4.8245e-01, -1.0532e-01,  9.4326e-01,\n",
       "            3.9321e-01],\n",
       "          [-1.0170e-01,  7.9285e-01,  9.7785e-01,  2.9183e-01,  6.5627e-01,\n",
       "            4.7304e-01],\n",
       "          [-2.5663e-01,  4.5909e-01,  6.2698e-01, -2.6996e-01,  9.7363e-01,\n",
       "            2.5338e-01],\n",
       "          [-3.3819e-02,  1.2339e-01,  4.5216e-01, -3.5898e-02,  8.7263e-01,\n",
       "            4.3357e-01],\n",
       "          [-4.1403e-02,  3.1424e-01,  1.2253e+00,  1.3726e-01,  6.0207e-01,\n",
       "            2.2569e-01],\n",
       "          [-3.6982e-01,  3.8515e-01,  7.5359e-01, -1.3122e-01,  1.0089e+00,\n",
       "            4.4134e-01],\n",
       "          [-4.2096e-03,  4.8378e-01,  7.0702e-01, -2.3111e-01,  1.0391e+00,\n",
       "            3.2032e-01],\n",
       "          [-7.5120e-01, -3.4001e-01,  9.4773e-01,  5.8318e-02,  3.9110e-01,\n",
       "           -1.0541e-01],\n",
       "          [-5.3067e-01, -1.2683e-01,  1.1138e+00,  1.6309e-01,  2.6003e-01,\n",
       "           -4.5396e-02],\n",
       "          [-5.2794e-01, -2.2063e-01,  1.2270e+00,  2.6712e-01,  1.8929e-01,\n",
       "            2.9903e-01],\n",
       "          [-3.6816e-01, -1.7308e-01,  1.3243e+00,  1.0401e-01,  6.5430e-01,\n",
       "            2.1223e-01],\n",
       "          [-5.5244e-01, -3.6783e-01,  6.5283e-01,  6.4504e-02,  8.6613e-01,\n",
       "            4.3045e-01],\n",
       "          [-5.0401e-01,  5.4940e-02,  1.1346e+00,  4.7113e-01,  5.5636e-01,\n",
       "            4.3633e-01],\n",
       "          [-5.7649e-01, -2.4632e-01,  7.2604e-01, -3.7622e-02,  8.5644e-01,\n",
       "            2.0608e-01],\n",
       "          [-4.8071e-01, -4.8135e-01,  8.5660e-01,  2.0448e-01,  8.2891e-01,\n",
       "            3.7236e-01],\n",
       "          [-6.8511e-01, -3.1548e-01,  8.3804e-01,  8.4745e-02,  8.9768e-01,\n",
       "            4.0761e-01],\n",
       "          [-4.0333e-01, -2.3992e-01,  8.2727e-01, -4.8990e-02,  9.2860e-01,\n",
       "            3.0378e-01],\n",
       "          [-5.1906e-01, -3.1918e-01,  9.7289e-01,  2.4984e-01,  6.4304e-01,\n",
       "            3.6872e-01],\n",
       "          [-8.0152e-01,  4.4577e-01,  6.6515e-01, -4.7944e-02,  3.4723e-01,\n",
       "           -1.1081e-01],\n",
       "          [-7.1310e-01,  4.0958e-01,  9.0661e-01, -1.3750e-01,  4.0967e-01,\n",
       "           -1.0639e-01],\n",
       "          [-1.0011e+00,  4.0666e-01,  8.9454e-01, -2.8537e-01,  3.5499e-01,\n",
       "            2.8010e-02],\n",
       "          [-1.0284e+00,  5.4058e-01,  8.7575e-01, -3.2800e-01,  8.6092e-01,\n",
       "            4.9558e-01],\n",
       "          [-1.1576e+00,  2.0867e-01,  7.4623e-01, -2.6195e-01,  9.8430e-01,\n",
       "           -5.2745e-02],\n",
       "          [-6.0947e-01,  4.2262e-01,  2.0159e-01, -2.1878e-01,  5.6524e-01,\n",
       "            5.8821e-01],\n",
       "          [-6.2794e-01,  4.9381e-01,  3.4406e-01, -1.8399e-01,  4.0782e-01,\n",
       "            1.8867e-01],\n",
       "          [-4.6401e-01,  3.1044e-01,  5.6483e-01,  3.7266e-02,  6.0739e-01,\n",
       "            4.5922e-01],\n",
       "          [-1.0212e+00,  3.1327e-01,  1.0967e+00, -1.4752e-01,  7.2705e-01,\n",
       "            5.7604e-02],\n",
       "          [-6.3243e-01,  5.7513e-01,  5.0074e-01,  1.4963e-02,  4.9232e-01,\n",
       "            2.9036e-01],\n",
       "          [-4.8795e-01,  3.9507e-01,  2.5820e-01, -3.5804e-01,  5.6391e-01,\n",
       "            3.5317e-01],\n",
       "          [-1.0021e+00,  2.9444e-01,  7.7561e-01, -3.1883e-01,  7.2211e-01,\n",
       "            1.6608e-01],\n",
       "          [-4.1141e-01,  1.9214e-01,  9.9866e-01, -1.8760e-01,  3.4866e-01,\n",
       "           -1.9698e-01],\n",
       "          [-1.8369e-01,  3.7830e-01,  1.1963e+00, -1.0886e-01,  2.4743e-01,\n",
       "           -1.0590e-01],\n",
       "          [-2.1439e-01,  3.1949e-01,  1.2585e+00, -4.4923e-02,  1.9103e-01,\n",
       "            2.0610e-01],\n",
       "          [-4.3947e-02,  3.5186e-01,  1.3042e+00, -1.7762e-01,  5.9751e-01,\n",
       "            1.1671e-01],\n",
       "          [-2.0653e-01,  1.5666e-01,  6.9528e-01, -2.4280e-01,  8.3540e-01,\n",
       "            3.6181e-01],\n",
       "          [-1.8603e-01,  5.4949e-01,  1.1415e+00,  1.7762e-01,  4.9168e-01,\n",
       "            3.5403e-01],\n",
       "          [-2.5323e-01,  2.6539e-01,  7.1059e-01, -3.1930e-01,  8.1117e-01,\n",
       "            7.9734e-02],\n",
       "          [-1.4700e-01,  3.4068e-02,  8.9766e-01, -4.8287e-02,  7.7645e-01,\n",
       "            2.8106e-01],\n",
       "          [-1.7069e-01,  4.0401e-01,  1.0838e+00,  1.6654e-01,  2.6617e-01,\n",
       "            2.0304e-01],\n",
       "          [-3.4842e-01,  2.1481e-01,  8.2193e-01, -2.0345e-01,  8.4657e-01,\n",
       "            2.8434e-01],\n",
       "          [-8.3636e-02,  2.5655e-01,  8.2224e-01, -3.4820e-01,  8.5657e-01,\n",
       "            2.2937e-01],\n",
       "          [-1.8569e-01,  2.1394e-01,  1.0165e+00, -8.7852e-02,  6.3859e-01,\n",
       "            2.9377e-01],\n",
       "          [-7.7912e-01,  4.5512e-01,  1.8946e-01,  5.8711e-02,  2.7128e-02,\n",
       "           -5.4744e-01],\n",
       "          [-7.3489e-01,  3.1397e-01,  5.0950e-01, -2.7146e-02,  2.4402e-01,\n",
       "           -5.3812e-01],\n",
       "          [-1.0242e+00,  3.8764e-01,  4.4805e-01, -2.9276e-01,  1.1070e-01,\n",
       "           -2.6926e-01],\n",
       "          [-1.0440e+00,  4.7959e-01,  5.0566e-01, -3.7137e-01,  6.4015e-01,\n",
       "            3.2043e-01],\n",
       "          [-4.4075e-01,  5.5639e-01,  5.0296e-01, -2.2265e-01,  1.3264e-01,\n",
       "            1.9484e-01],\n",
       "          [-1.1514e+00,  1.4723e-01,  4.3866e-01, -3.3100e-01,  7.8959e-01,\n",
       "           -2.0102e-01],\n",
       "          [-3.6662e-01,  8.2183e-01,  4.5657e-02,  1.6071e-01, -2.2356e-01,\n",
       "            4.8269e-01],\n",
       "          [-8.9783e-01,  3.6754e-01,  8.9185e-02, -3.4770e-01,  4.6316e-01,\n",
       "            4.2465e-02],\n",
       "          [-3.0010e-01,  3.7905e-01, -1.0694e-01,  1.5536e-01,  1.6140e-01,\n",
       "            1.7291e-01],\n",
       "          [-1.0292e+00,  2.2748e-01,  7.3126e-01, -1.7210e-01,  5.1172e-01,\n",
       "           -1.9200e-01],\n",
       "          [-9.8584e-01,  3.5047e-01,  2.3985e-01, -1.9265e-01,  4.7677e-01,\n",
       "            2.3482e-01],\n",
       "          [-1.0352e+00,  2.4805e-01,  3.8866e-01, -4.0207e-01,  4.6589e-01,\n",
       "           -1.4028e-01],\n",
       "          [-6.9383e-01,  6.1070e-01,  8.2630e-01, -2.3260e-01, -1.9138e-01,\n",
       "            1.4389e-01],\n",
       "          [-6.2561e-01,  5.3632e-01,  1.0865e+00, -2.7383e-01,  2.5183e-02,\n",
       "            8.5955e-02],\n",
       "          [-9.5072e-01,  5.4271e-01,  1.0575e+00, -4.7591e-01,  5.2324e-04,\n",
       "            3.2752e-01],\n",
       "          [-1.0235e+00,  6.4631e-01,  1.1015e+00, -5.4503e-01,  5.0719e-01,\n",
       "            8.4326e-01],\n",
       "          [-1.1590e+00,  2.8400e-01,  9.9896e-01, -4.9086e-01,  6.2268e-01,\n",
       "            3.0815e-01],\n",
       "          [-3.9464e-01,  5.6577e-01,  1.5973e-01, -4.1959e-01, -4.6749e-02,\n",
       "            9.9964e-01],\n",
       "          [-8.0857e-01,  4.6591e-01,  6.5330e-01, -5.7198e-01,  3.2742e-01,\n",
       "            3.0130e-01],\n",
       "          [-2.1289e-01,  5.2486e-01,  7.6088e-01, -1.9521e-01,  2.3970e-02,\n",
       "            7.5629e-01],\n",
       "          [-9.7965e-01,  4.0763e-01,  1.3067e+00, -3.5918e-01,  3.7768e-01,\n",
       "            3.7076e-01],\n",
       "          [-8.8628e-01,  4.6055e-01,  8.3520e-01, -4.1285e-01,  3.1082e-01,\n",
       "            4.5531e-01],\n",
       "          [-9.6394e-01,  4.0860e-01,  9.9226e-01, -5.5669e-01,  3.4003e-01,\n",
       "            4.6406e-01],\n",
       "          [-8.5616e-01,  5.4399e-01,  6.8976e-01, -3.3415e-01,  1.6785e-01,\n",
       "           -1.5899e-01],\n",
       "          [-6.7346e-01,  5.6813e-01,  9.4129e-01, -3.5634e-01,  1.9733e-01,\n",
       "           -7.4987e-02],\n",
       "          [-8.4705e-01,  5.2809e-01,  8.9791e-01, -5.1510e-01,  1.7796e-01,\n",
       "            1.3102e-02],\n",
       "          [-8.3743e-01,  6.6419e-01,  8.8420e-01, -5.3119e-01,  6.6735e-01,\n",
       "            5.1825e-01],\n",
       "          [-2.2864e-01,  6.9066e-01,  9.5077e-01, -2.4555e-01,  1.2108e-02,\n",
       "            2.1750e-01],\n",
       "          [-9.6757e-01,  3.1904e-01,  7.5930e-01, -4.6519e-01,  8.0225e-01,\n",
       "           -6.0901e-02],\n",
       "          [-6.6725e-01,  4.3062e-01,  3.9452e-01, -4.0539e-01,  5.0005e-01,\n",
       "            4.6465e-01],\n",
       "          [-5.4997e-01,  7.8505e-01,  8.4457e-01, -4.9379e-02,  1.8750e-01,\n",
       "            2.0654e-01],\n",
       "          [-5.6885e-01,  4.0509e-01,  6.0493e-01, -2.1819e-01,  4.9438e-01,\n",
       "            3.6402e-01],\n",
       "          [-8.5072e-01,  4.4557e-01,  1.0830e+00, -3.5981e-01,  5.3944e-01,\n",
       "            6.2440e-02],\n",
       "          [-5.6323e-01,  4.6183e-01,  5.1527e-01, -5.6757e-01,  5.5994e-01,\n",
       "            2.5088e-01],\n",
       "          [-8.3598e-01,  4.0714e-01,  7.6351e-01, -5.3302e-01,  5.6335e-01,\n",
       "            1.5692e-01],\n",
       "          [-5.9466e-01, -2.6923e-02,  4.9476e-01, -4.2324e-01,  2.5723e-01,\n",
       "           -4.0538e-01],\n",
       "          [-9.1780e-01, -1.6308e-02,  5.1383e-01, -6.4492e-01,  1.8666e-01,\n",
       "           -1.4426e-01],\n",
       "          [-1.0352e+00,  2.2046e-01,  8.0550e-01, -6.5297e-01,  7.4952e-01,\n",
       "            3.4698e-01],\n",
       "          [-3.5828e-01,  2.9859e-01,  9.6228e-01, -3.8818e-01,  3.2138e-01,\n",
       "            1.5050e-01],\n",
       "          [-1.1549e+00, -1.2098e-01,  7.3712e-01, -5.5806e-01,  8.7722e-01,\n",
       "           -1.9713e-01],\n",
       "          [-3.6308e-01,  2.3344e-01, -1.0865e-02, -2.6621e-01,  3.0799e-01,\n",
       "            2.5755e-01],\n",
       "          [-2.4785e-01,  6.3531e-01,  7.4288e-01, -3.6488e-02, -6.2531e-03,\n",
       "            3.2398e-01],\n",
       "          [-8.6241e-01,  1.9640e-01,  3.9567e-01, -5.8207e-01,  6.0408e-01,\n",
       "            2.6473e-02],\n",
       "          [-9.8191e-01, -1.0465e-01,  9.4340e-01, -5.0388e-01,  6.4215e-01,\n",
       "           -1.4828e-01],\n",
       "          [-9.6076e-01,  1.6196e-01,  5.3783e-01, -4.2441e-01,  6.2603e-01,\n",
       "            2.2504e-01],\n",
       "          [-1.0434e-01,  3.5013e-01,  5.0083e-01, -5.1714e-01,  4.2983e-01,\n",
       "            1.4515e-01],\n",
       "          [-9.3926e-01, -1.5956e-01,  4.4275e-01, -7.4305e-01,  5.4282e-01,\n",
       "           -3.9445e-02],\n",
       "          [-7.9146e-01,  1.6362e-02,  4.6452e-01, -3.4651e-01,  1.9946e-01,\n",
       "           -1.2997e-01],\n",
       "          [-5.5913e-01,  2.7975e-01,  5.7433e-01, -2.0767e-01,  1.1537e-01,\n",
       "           -1.6020e-01],\n",
       "          [-4.8825e-01,  1.6335e-01,  7.1906e-01, -1.8870e-01, -7.3582e-02,\n",
       "            1.5256e-01],\n",
       "          [-4.7830e-01,  2.9023e-01,  9.2628e-01, -2.8990e-01,  4.5135e-01,\n",
       "            2.5358e-01],\n",
       "          [-7.4145e-01,  2.3776e-01,  3.5537e-01, -6.0164e-02,  3.7726e-01,\n",
       "            2.2955e-01],\n",
       "          [-6.5503e-01,  7.0672e-02,  2.5946e-01, -3.1061e-01,  6.6862e-01,\n",
       "            3.9735e-01],\n",
       "          [-5.7748e-01,  5.0687e-01,  7.2086e-01,  8.2606e-02,  3.5777e-01,\n",
       "            4.4316e-01],\n",
       "          [-7.0715e-01,  2.2568e-01,  3.0591e-01, -4.4063e-01,  6.5945e-01,\n",
       "            2.2959e-01],\n",
       "          [-5.4477e-01, -1.1584e-01,  3.7552e-01, -2.2082e-01,  6.5246e-01,\n",
       "            3.5642e-01],\n",
       "          [-8.1572e-01,  1.5667e-01,  4.2331e-01, -3.1757e-01,  6.9663e-01,\n",
       "            4.3209e-01],\n",
       "          [-4.8139e-01,  2.0561e-01,  4.2145e-01, -4.3578e-01,  7.3387e-01,\n",
       "            3.0207e-01],\n",
       "          [-4.5078e-01,  1.4170e-01,  3.3538e-01,  1.8479e-02,  2.6664e-01,\n",
       "            2.1420e-01],\n",
       "          [-6.6318e-01,  4.0565e-01,  6.0526e-01, -1.3973e-01,  1.3662e-01,\n",
       "           -1.8337e-01],\n",
       "          [-4.9029e-01,  3.9679e-01,  8.5610e-01, -1.7858e-01,  1.7436e-01,\n",
       "           -1.1950e-01],\n",
       "          [-6.6976e-01,  3.4782e-01,  7.9868e-01, -3.4336e-01,  1.5532e-01,\n",
       "           -1.5650e-02],\n",
       "          [-6.5556e-01,  4.8870e-01,  7.7036e-01, -3.7583e-01,  6.5069e-01,\n",
       "            5.0307e-01],\n",
       "          [ 5.7347e-02,  6.6565e-01,  8.8160e-01, -1.3602e-02,  3.8712e-02,\n",
       "            1.0247e-01],\n",
       "          [-7.7228e-01,  1.6213e-01,  6.4480e-01, -3.1635e-01,  7.7976e-01,\n",
       "           -7.3002e-02],\n",
       "          [-4.6478e-01,  3.0722e-01,  3.1934e-01, -2.1735e-01,  4.5576e-01,\n",
       "            4.4030e-01],\n",
       "          [-3.3720e-01,  6.7335e-01,  8.0061e-01,  1.4274e-01,  1.1300e-01,\n",
       "            1.4384e-01],\n",
       "          [-3.7672e-01,  2.6432e-01,  5.2123e-01, -2.7536e-02,  4.5843e-01,\n",
       "            3.4588e-01],\n",
       "          [-6.6882e-01,  2.7023e-01,  9.7459e-01, -2.0375e-01,  5.1872e-01,\n",
       "            4.8230e-02],\n",
       "          [-3.5689e-01,  3.4180e-01,  4.4954e-01, -3.9113e-01,  4.9424e-01,\n",
       "            2.0384e-01],\n",
       "          [-6.4951e-01,  2.3430e-01,  6.5556e-01, -3.7488e-01,  5.3463e-01,\n",
       "            1.3718e-01],\n",
       "          [-4.1843e-01,  6.3742e-01,  6.8527e-01,  1.8178e-03,  2.3901e-02,\n",
       "           -1.7448e-01],\n",
       "          [-3.5556e-01,  5.3495e-01,  9.7193e-01, -6.2379e-02,  2.1618e-01,\n",
       "           -2.2336e-01],\n",
       "          [-7.1626e-01,  5.5878e-01,  9.1604e-01, -3.0006e-01,  1.2185e-01,\n",
       "            2.4838e-02],\n",
       "          [-7.8589e-01,  6.7659e-01,  9.2352e-01, -3.6637e-01,  6.1791e-01,\n",
       "            5.6074e-01],\n",
       "          [-2.1029e-01,  5.9792e-01,  8.0299e-01, -2.4341e-01,  4.6603e-02,\n",
       "            3.2679e-01],\n",
       "          [-9.1960e-01,  3.1619e-01,  8.0905e-01, -3.1790e-01,  7.2605e-01,\n",
       "            3.3529e-02],\n",
       "          [-6.8486e-01,  4.6783e-01,  4.5335e-01, -3.9138e-01,  4.3834e-01,\n",
       "            1.9568e-01],\n",
       "          [ 6.7595e-02,  5.6482e-01,  6.4818e-01,  2.2931e-02,  1.9853e-01,\n",
       "            4.2749e-01],\n",
       "          [-7.4659e-01,  4.3150e-01,  1.1367e+00, -1.7877e-01,  4.9226e-01,\n",
       "            7.9712e-02],\n",
       "          [-7.6900e-01,  4.5413e-01,  6.1347e-01, -2.4794e-01,  4.3054e-01,\n",
       "            3.6535e-01],\n",
       "          [-7.3794e-01,  4.2585e-01,  8.2972e-01, -3.8410e-01,  4.6886e-01,\n",
       "            1.6036e-01],\n",
       "          [-7.1245e-01,  1.6936e-01,  3.8279e-01, -4.5014e-01, -3.9184e-02,\n",
       "            5.3912e-02],\n",
       "          [-5.3092e-01,  3.9600e-01,  3.5322e-01, -2.2147e-01, -1.9596e-01,\n",
       "            8.0547e-02],\n",
       "          [-6.1574e-01,  6.0059e-01,  7.7394e-01, -1.9361e-01,  4.2967e-01,\n",
       "            9.3487e-01],\n",
       "          [-4.3851e-01,  5.0055e-01,  1.0252e+00, -3.3028e-01,  3.0212e-01,\n",
       "            4.4887e-01],\n",
       "          [-7.3577e-01,  2.7686e-01,  7.0814e-01, -1.8362e-01,  6.0538e-01,\n",
       "            4.0711e-01],\n",
       "          [-6.4033e-01,  3.2040e-01,  3.3689e-01, -4.0964e-01,  4.7850e-01,\n",
       "            5.3588e-01],\n",
       "          [-5.4109e-01,  7.3701e-01,  8.2642e-01,  1.6024e-02,  1.7582e-01,\n",
       "            6.2327e-01],\n",
       "          [-6.7173e-01,  4.1641e-01,  4.0640e-01, -4.8291e-01,  5.3906e-01,\n",
       "            4.1089e-01],\n",
       "          [-4.8144e-01,  5.8394e-02,  2.9490e-01, -3.2905e-01,  4.0884e-01,\n",
       "            5.5208e-01],\n",
       "          [-4.3010e-01,  3.7085e-01,  7.5541e-01,  1.4941e-01,  1.2235e-01,\n",
       "            3.0102e-01],\n",
       "          [-7.7574e-01,  3.4990e-01,  5.2430e-01, -3.5775e-01,  5.6825e-01,\n",
       "            6.0786e-01],\n",
       "          [-4.5206e-01,  4.2912e-01,  5.3452e-01, -5.1017e-01,  5.6618e-01,\n",
       "            4.6954e-01]], grad_fn=<CopyBackwards>),\n",
       "  'pred': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'span': [(12, 15, 34, 40, 2)]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del inputs[\"relation_mask\"]\n",
    "del inputs[\"relation_label\"]\n",
    "outputs = model(**inputs, is_training=False)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "continuing-style",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  3680,  1012,  3123,  5086, 14863,  4783,  2147,  2063,  1010,\n",
       "           2028,  1997, 11154,  1005,  1055,  2087,  5182,  9559,  1998,  1037,\n",
       "           2280,  2266,  1997,  1996,  2406,  1005,  1055,  2152,  2457,  1010,\n",
       "           2000,  8556,  1012,   102]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'entity_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'entity_label': tensor([2, 2, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'relation_mask': tensor([[0, 0, 0, 0, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 2, 2, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 1, 1, 1, 1, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 3, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 1, 1, 0, 0, 0, 0],\n",
       "         [2, 2, 2, 3, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 2, 2, 0, 0, 0, 0],\n",
       "         [2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 1, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 1, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "          3, 3, 2, 2, 0, 0, 0, 0]]),\n",
       " 'relation_label': tensor([5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inputs = next(generator)\n",
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polyphonic-action",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': tensor(4.6240, grad_fn=<DivBackward0>),\n",
       " 'entity': {'logit': tensor([[ 0.5819,  0.0741, -0.0334,  0.3927,  0.7033],\n",
       "          [ 0.2299,  0.1545,  0.0303,  0.1535,  0.7243],\n",
       "          [ 0.2250,  0.0322, -0.2512,  0.0519,  0.5079],\n",
       "          [ 0.4887, -0.3116, -0.2378,  0.3460,  0.3862],\n",
       "          [ 0.4297, -0.1324, -0.1910,  0.2687,  0.8742],\n",
       "          [ 0.3672,  0.3250,  0.0173,  0.1853,  0.7590],\n",
       "          [ 0.5862,  0.2360, -0.3061,  0.3153,  0.9475],\n",
       "          [ 0.5526, -0.0358, -0.2751,  0.3291,  0.5196],\n",
       "          [ 0.1845, -0.1523, -0.2466,  0.3240,  0.5028],\n",
       "          [ 0.4602, -0.0172, -0.1951,  0.2573,  0.5737],\n",
       "          [ 0.3466, -0.2835, -0.1531,  0.2155,  0.7509],\n",
       "          [ 0.3280, -0.2454, -0.0652,  0.1407,  0.8799],\n",
       "          [ 0.4998, -0.1880, -0.2143,  0.2748,  0.9990],\n",
       "          [ 0.4512, -0.1586, -0.2129,  0.2846,  0.8072],\n",
       "          [ 0.5344, -0.0406, -0.3060,  0.3799,  0.9146],\n",
       "          [ 0.4092, -0.5269, -0.3275,  0.2977,  0.5559],\n",
       "          [ 0.5125,  0.0968, -0.2877,  0.2664,  1.0201],\n",
       "          [ 0.5548,  0.0690, -0.2010,  0.2591,  0.9985],\n",
       "          [ 0.4035,  0.0391,  0.0505,  0.1816,  1.0554],\n",
       "          [ 0.3666,  0.2015,  0.1074,  0.1271,  0.9629],\n",
       "          [ 0.5285,  0.0654, -0.1450,  0.2074,  1.0140],\n",
       "          [ 0.5986,  0.0977, -0.3637,  0.2680,  0.7914],\n",
       "          [ 0.2825,  0.0716,  0.0908, -0.0208,  0.9570],\n",
       "          [ 0.5988,  0.1748, -0.3520,  0.3142,  0.8990],\n",
       "          [ 0.4911,  0.1313, -0.1477,  0.3921,  0.6616],\n",
       "          [ 0.2063,  0.1184,  0.0020,  0.0306,  0.7232],\n",
       "          [ 0.5601,  0.0559, -0.2234,  0.3880,  0.9575],\n",
       "          [ 0.1411,  0.2720, -0.0939,  0.0569,  0.7033],\n",
       "          [ 0.5391, -0.0564, -0.2097,  0.3377,  0.6895],\n",
       "          [ 0.4513,  0.0783, -0.1873,  0.2561,  0.9846],\n",
       "          [ 0.5200, -0.1362, -0.2031,  0.3124,  0.5453],\n",
       "          [ 0.3873, -0.0059,  0.0992,  0.0077,  0.7808],\n",
       "          [ 0.5647,  0.1815, -0.1741,  0.4096,  0.5855],\n",
       "          [ 0.5636,  0.0955, -0.0193,  0.2534,  1.0506],\n",
       "          [ 0.2879,  0.0836,  0.0582, -0.0241,  0.8783],\n",
       "          [ 0.6325,  0.0644, -0.2514,  0.3632,  0.7056],\n",
       "          [ 0.3128, -0.0837, -0.3760,  0.0762,  0.1835],\n",
       "          [ 0.4573,  0.1643, -0.2461,  0.1455,  0.6770],\n",
       "          [ 0.6057,  0.1264, -0.1544,  0.4392,  0.5939],\n",
       "          [ 0.5555,  0.1394, -0.2872,  0.3004,  0.8650],\n",
       "          [ 0.4928,  0.1752, -0.2911,  0.2341,  0.5651],\n",
       "          [ 0.6479,  0.1167, -0.3595,  0.2511,  0.6801],\n",
       "          [ 0.2992,  0.3734, -0.0433,  0.1318,  0.7623],\n",
       "          [ 0.5049,  0.0824, -0.0484,  0.1926,  1.0302],\n",
       "          [ 0.5042,  0.1076, -0.2737,  0.4316,  0.9781],\n",
       "          [ 0.4254,  0.1562, -0.0068,  0.2029,  1.0797],\n",
       "          [ 0.5226,  0.0561, -0.3366,  0.2613,  0.4916],\n",
       "          [ 0.1806, -0.3647, -0.3306,  0.3544,  0.3709],\n",
       "          [ 0.4269, -0.0844, -0.1842,  0.2476,  1.0191],\n",
       "          [ 0.4090, -0.0436, -0.1460,  0.2714,  0.4847],\n",
       "          [ 0.3872, -0.4138, -0.2032,  0.4966,  0.2436],\n",
       "          [ 0.4028, -0.0367,  0.0475,  0.1218,  0.8394],\n",
       "          [ 0.6487,  0.0457, -0.3541,  0.2725,  0.6484],\n",
       "          [ 0.4998,  0.0828, -0.0549,  0.4914,  0.6036],\n",
       "          [ 0.5157,  0.0585, -0.0777,  0.4267,  0.6997],\n",
       "          [ 0.4851, -0.0313, -0.2539,  0.3204,  1.0415],\n",
       "          [ 0.2659,  0.1897, -0.0319,  0.0881,  0.7114],\n",
       "          [ 0.2474,  0.1979, -0.0030,  0.0572,  0.7310],\n",
       "          [ 0.4815, -0.1034, -0.2671,  0.2510,  0.6655],\n",
       "          [ 0.2151, -0.0883, -0.2408,  0.3799,  0.4262],\n",
       "          [ 0.1179,  0.2810, -0.0084,  0.0522,  0.5879],\n",
       "          [ 0.3913,  0.0997, -0.1097,  0.2478,  0.6279],\n",
       "          [ 0.4016, -0.0304,  0.0180,  0.2454,  0.9440],\n",
       "          [ 0.3419,  0.1591,  0.0630,  0.1033,  1.0819],\n",
       "          [ 0.3594,  0.2705,  0.0161,  0.1155,  0.8051],\n",
       "          [ 0.4812,  0.2284,  0.0795,  0.2521,  0.8152],\n",
       "          [ 0.3063, -0.4941, -0.3443,  0.3297,  0.4796],\n",
       "          [ 0.6042,  0.0524, -0.2286,  0.3108,  0.5573],\n",
       "          [ 0.6157,  0.0943, -0.3543,  0.3021,  0.7777],\n",
       "          [ 0.5917, -0.0016, -0.2940,  0.3387,  0.6498],\n",
       "          [ 0.5257,  0.0529, -0.2899,  0.2785,  0.9875],\n",
       "          [ 0.4796, -0.1516, -0.2223,  0.3700,  0.6731],\n",
       "          [ 0.4531,  0.0969, -0.2766,  0.1526,  0.7127],\n",
       "          [ 0.5122,  0.0725, -0.0558,  0.1901,  1.0524],\n",
       "          [ 0.4119, -0.1457, -0.1649,  0.2605,  0.9734],\n",
       "          [ 0.6153,  0.1003, -0.3731,  0.2212,  0.7220],\n",
       "          [ 0.4414,  0.1672,  0.1652,  0.1907,  0.9092],\n",
       "          [ 0.5632,  0.0640, -0.2706,  0.3378,  1.0701],\n",
       "          [ 0.4505, -0.0144, -0.2830,  0.3881,  0.8425],\n",
       "          [ 0.5922,  0.0820, -0.1140,  0.2568,  1.0254],\n",
       "          [ 0.4888,  0.0707, -0.3671,  0.2058,  0.5175],\n",
       "          [ 0.5394, -0.0415, -0.2595,  0.3447,  0.9199],\n",
       "          [ 0.2142, -0.2733, -0.2074,  0.2267,  0.7117],\n",
       "          [ 0.3146, -0.0681, -0.1320,  0.2127,  0.4019],\n",
       "          [ 0.5076, -0.0554, -0.1816,  0.2499,  1.0102],\n",
       "          [ 0.2189, -0.3536, -0.1939,  0.1880,  0.7000],\n",
       "          [ 0.4393, -0.0160, -0.2600,  0.2217,  0.9196],\n",
       "          [ 0.4147, -0.1619, -0.1793,  0.2574,  1.0040],\n",
       "          [ 0.5246, -0.0557, -0.2801,  0.2595,  0.9196],\n",
       "          [ 0.4047, -0.0289,  0.0336,  0.1046,  0.7500],\n",
       "          [ 0.5272,  0.0292, -0.1775,  0.2501,  0.9497],\n",
       "          [ 0.4735, -0.4175, -0.2419,  0.3438,  0.5997],\n",
       "          [ 0.5037,  0.1005, -0.0372,  0.2020,  1.0535],\n",
       "          [ 0.2842,  0.1737,  0.0982,  0.0019,  0.9215],\n",
       "          [ 0.3737, -0.0911, -0.2841,  0.2956,  0.9291],\n",
       "          [ 0.5205,  0.0415, -0.2759,  0.3044,  0.9104],\n",
       "          [ 0.4280, -0.1757, -0.1643,  0.2704,  0.9148],\n",
       "          [ 0.4340,  0.1322, -0.0248,  0.1873,  1.0756],\n",
       "          [ 0.2372,  0.1391,  0.0175,  0.1744,  0.7074],\n",
       "          [ 0.4260, -0.1961, -0.2193,  0.2691,  0.8342],\n",
       "          [ 0.4994,  0.1083, -0.0263,  0.2137,  1.0328],\n",
       "          [ 0.1925, -0.0978, -0.0284,  0.1059,  0.4238],\n",
       "          [ 0.2933, -0.2952, -0.2338,  0.3511,  0.7991],\n",
       "          [ 0.4698,  0.1690, -0.4175,  0.0939,  0.4649]],\n",
       "         grad_fn=<AddmmBackward>),\n",
       "  'pred': tensor([4, 4, 4, 0, 4, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 0, 4, 4, 4, 4, 4, 4, 4, 0, 4,\n",
       "          4, 4, 3, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "          4, 4, 4, 4, 4, 4, 4, 0]),\n",
       "  'span': [(0, 3, 4),\n",
       "   (4, 8, 4),\n",
       "   (11, 12, 4),\n",
       "   (17, 21, 4),\n",
       "   (29, 31, 4),\n",
       "   (12, 15, 4),\n",
       "   (8, 10, 4),\n",
       "   (25, 28, 4),\n",
       "   (21, 25, 4),\n",
       "   (28, 29, 4),\n",
       "   (15, 16, 4)],\n",
       "  'embedding': tensor([4., 4., 4., 0., 4., 4., 4., 4., 4., 4., 0., 4., 4., 4., 4., 4., 0., 4.,\n",
       "          4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0.])},\n",
       " 'relation': {'logit': tensor([[-0.2996,  0.1895,  0.5213, -0.3448,  0.9180, -0.0300],\n",
       "          [-0.2550,  0.2271,  0.2525, -0.4614,  0.4496, -0.5348],\n",
       "          [-0.5312,  0.1963,  0.3590, -0.1638,  0.0966, -0.6613],\n",
       "          [ 0.0418,  0.0437,  0.2424,  0.1019,  0.7991,  0.1843],\n",
       "          [-0.2778,  0.1570,  0.6869, -0.3490, -0.0591, -0.6286],\n",
       "          [-0.5569, -0.2356,  0.2219, -0.1995,  0.6625, -0.6580],\n",
       "          [-0.3107, -0.0666,  0.4536, -0.4024,  1.0961, -0.1816],\n",
       "          [-0.2422,  0.1048,  0.7353, -0.3102,  0.3161, -0.6792],\n",
       "          [-0.1937,  0.6068,  0.4381, -0.3870,  0.2589, -0.5887],\n",
       "          [ 0.2179, -0.2344,  0.6424, -0.1191,  0.6530, -0.1630],\n",
       "          [-0.0245,  0.0610,  0.3822,  0.1238,  0.6306, -0.0440],\n",
       "          [-0.1721,  0.5519,  0.5671, -0.2922,  0.8958,  0.1444]],\n",
       "         grad_fn=<CopyBackwards>),\n",
       "  'pred': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "  'span': []}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del inputs[\"entity_label\"]\n",
    "# outputs = model(**inputs, is_training=True)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-uganda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
