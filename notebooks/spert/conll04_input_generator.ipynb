{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suitable-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "english-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../parser\")\n",
    "import conll04_parser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "missing-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.entity_encode = {'O': 0, 'B-Loc': 1, 'I-Loc': 1, 'B-Peop': 2, 'I-Peop': 2, \n",
    "                 'B-Org': 3, 'I-Org': 3, 'B-Other': 4, 'I-Other': 4}\n",
    "parser.TRAIN_PATH = \"../\" + parser.TRAIN_PATH\n",
    "parser.DEV_PATH = \"../\" + parser.DEV_PATH\n",
    "parser.TEST_PATH = \"../\" + parser.TEST_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exterior-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "frequent-sphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_mask(doc, is_training, neg_entity_count, max_span_size):\n",
    "    sentence_length = doc[\"data_frame\"].shape[0]\n",
    "    entity_pool = set([(l, r) for l in range(sentence_length) \\\n",
    "                       for r in range(l + 1, min(sentence_length, l + max_span_size) + 1)])\n",
    "    # print(sorted(entity_pool))\n",
    "    entity_mask = []\n",
    "    entity_label = []\n",
    "    \n",
    "    for key in doc[\"entity_position\"]:\n",
    "        l, r = doc[\"entity_position\"][key]\n",
    "        if r - l <= max_span_size: entity_pool.remove((l, r))\n",
    "        entity_mask.append([0] * l + [1] * (r - l) + [0] * (sentence_length - r))\n",
    "        entity_label.append(doc[\"data_frame\"].at[l, \"entity_embedding\"])\n",
    "        \n",
    "    if is_training:\n",
    "        # If training then add a limited number of negative spans\n",
    "        for l, r in random.sample(entity_pool, min(len(entity_pool), neg_entity_count)):\n",
    "            entity_mask.append([0] * l + [1] * (r - l) + [0] * (sentence_length - r))\n",
    "            entity_label.append(0)\n",
    "    else:\n",
    "        # Else add all possible negative spans\n",
    "        for l, r in entity_pool:\n",
    "            entity_mask.append([0] * l + [1] * (r - l) + [0] * (sentence_length - r))\n",
    "            entity_label.append(0)\n",
    "            \n",
    "    return torch.tensor(entity_mask, dtype=torch.long), torch.tensor(entity_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "helpful-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test generate_entity_mask()\n",
    "# rawdata = parser.extract_data(\"train\")\n",
    "# doc0 = rawdata[0]\n",
    "# entity_mask0, entity_label0 = generate_entity_mask(doc0, True, 5, 10)\n",
    "# assert entity_mask0.shape == (8, 44)\n",
    "# assert torch.equal(entity_label0, torch.tensor([1, 1, 1, 0, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compact-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relation_mask(doc, is_training, neg_relation_count):\n",
    "    sentence_length = doc[\"data_frame\"].shape[0]\n",
    "    relation_pool = set([(e1, e2) for e1 in doc[\"entity_position\"].keys() \\\n",
    "                       for e2 in doc[\"entity_position\"].keys() if e1 != e2])\n",
    "    # print(relation_pool)\n",
    "    relation_mask = []\n",
    "    relation_label = []\n",
    "    \n",
    "    for key in doc[\"relations\"]:\n",
    "        relation_pool.remove((doc[\"relations\"][key][\"source\"], doc[\"relations\"][key][\"target\"]))\n",
    "        e1 = doc[\"entity_position\"][doc[\"relations\"][key][\"source\"]]\n",
    "        e2 = doc[\"entity_position\"][doc[\"relations\"][key][\"target\"]]\n",
    "        c = (min(e1[1], e2[1]), max(e1[0], e2[0]))\n",
    "        template = [0] * sentence_length\n",
    "        template[e1[0]: e1[1]] = [1] * (e1[1] - e1[0])\n",
    "        template[e2[0]: e2[1]] = [2] * (e2[1] - e2[0])\n",
    "        template[c[0]: c[1]] = [3] * (c[1] - c[0])\n",
    "        relation_mask.append(template)        \n",
    "        relation_label.append(doc[\"relations\"][key][\"type\"])\n",
    "        \n",
    "    # Only use real entities to generate false relations (refer to the paper)\n",
    "    if is_training:\n",
    "        # Only add negative relations when training\n",
    "        for first, second in random.sample(relation_pool, min(len(relation_pool), neg_relation_count)):\n",
    "            e1 = doc[\"entity_position\"][first]\n",
    "            e2 = doc[\"entity_position\"][second]\n",
    "            c = (min(e1[1], e2[1]), max(e1[0], e2[0]))\n",
    "            template = [0] * sentence_length\n",
    "            template[e1[0]: e1[1]] = [1] * (e1[1] - e1[0])\n",
    "            template[e2[0]: e2[1]] = [2] * (e2[1] - e2[0])\n",
    "            template[c[0]: c[1]] = [3] * (c[1] - c[0])\n",
    "            relation_mask.append(template)        \n",
    "            relation_label.append(0)\n",
    "    \n",
    "    return torch.tensor(relation_mask, dtype=torch.long), torch.tensor(relation_label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "steady-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test generate_relation_mask()\n",
    "# rawdata = parser.extract_data(\"train\")\n",
    "# doc0 = rawdata[0]\n",
    "# relation_mask0, relation_label0 = generate_relation_mask(doc0, True, 5)\n",
    "# assert relation_mask0.shape == (6, 44)\n",
    "# assert torch.equal(relation_label0, torch.tensor([2, 2, 0, 0, 0, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flush-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(group, device, \n",
    "                   is_training=True,\n",
    "                   neg_entity_count=100, \n",
    "                   neg_relation_count=100, \n",
    "                   max_span_size=10):\n",
    "    \"\"\"Generate input for the spert model\n",
    "    'group' is the dataset (\"train\", \"dev\", or \"test\")\n",
    "    'device' is the device where pytorch runs on (e.g. device = torch.device(\"cuda\"))\n",
    "    \"\"\"\n",
    "    data = parser.extract_data(group)\n",
    "    for doc in data:\n",
    "        # Add CLS and SEP to the sentence\n",
    "        input_ids = [parser.CLS_TOKEN] + doc[\"data_frame\"][\"token_ids\"].tolist() + [parser.SEP_TOKEN]\n",
    "        \n",
    "        entity_mask, entity_label = generate_entity_mask(doc, is_training, neg_entity_count, max_span_size)\n",
    "        assert entity_mask.shape[1] == len(input_ids) - 2\n",
    "        \n",
    "        relation_mask, relation_label = generate_relation_mask(doc, is_training, neg_relation_count)\n",
    "        assert relation_mask.shape[1] == len(input_ids) - 2\n",
    "        \n",
    "        yield {\n",
    "            \"input_ids\": torch.tensor([input_ids]).long().to(device), \n",
    "            \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "            \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "            \"entity_mask\": entity_mask.to(device),\n",
    "            \"entity_label\": entity_label.to(device),\n",
    "            \"relation_mask\": relation_mask.to(device),\n",
    "            \"relation_label\": relation_label.to(device)\n",
    "        }\n",
    "        del input_ids\n",
    "        del entity_mask\n",
    "        del entity_label\n",
    "        del relation_mask\n",
    "        del relation_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "structural-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "import conll04_input_generator as input_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "defined-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "generator = input_generator.data_generator(\"train\", device)\n",
    "for data in generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-lightning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
