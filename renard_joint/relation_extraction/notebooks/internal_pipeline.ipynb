{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "discrete-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "genetic-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import internal_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "polar-reader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_doc(\n",
    "    document, \n",
    "    pretrain_model, \n",
    "    ignore_index=CrossEntropyLoss().ignore_index,\n",
    "    max_token_count=512,\n",
    "    cls_token=internal_parser.CLS_TOKEN,\n",
    "    sep_token=internal_parser.SEP_TOKEN\n",
    "):\n",
    "    \"\"\"Transform a parsed document with a pre-trained model (BERT)\n",
    "    Only the first token of each word is labeled, the others are masked as 'ignore_index'\n",
    "    The label of O is 0\n",
    "    The label of I is the negation of the corresponding label of B\n",
    "    \"\"\"\n",
    "    transformed_tokens = []\n",
    "    padding_token_count = 2\n",
    "    \n",
    "    tokens = document[\"data_frame\"][\"token_ids\"].tolist()\n",
    "    begins = document[\"data_frame\"][\"begins\"].tolist()\n",
    "    ends = document[\"data_frame\"][\"ends\"].tolist()\n",
    "    labels = document[\"data_frame\"][\"entity_embedding\"].tolist()\n",
    "    words = document[\"data_frame\"][\"words\"].tolist()\n",
    "    sentence_embedding = document[\"data_frame\"][\"sentence_embedding\"].tolist()\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if i > 0 and begins[i] == begins[i-1] and ends[i] == ends[i-1]:\n",
    "            # Extra tokens from the same word are ignored\n",
    "            labels[i] = ignore_index\n",
    "\n",
    "    for entity in document[\"entity_position\"]:\n",
    "        begin, end = document[\"entity_position\"][entity]\n",
    "        for i in range(begin + 1, end):\n",
    "            # Every subsequence word of an entity is label as I instead of B\n",
    "            if labels[i] != ignore_index:\n",
    "                labels[i] = -labels[i]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        j = i\n",
    "        while j < len(tokens) and sentence_embedding[i] == sentence_embedding[j] and j - i < max_token_count-padding_token_count:\n",
    "            j += 1\n",
    "        # Segment the document and encode with the pre-trained model\n",
    "        inputs = tokens[i:j]\n",
    "        # Add CLS and SEP tokens\n",
    "        inputs = [cls_token] + inputs\n",
    "        inputs.append(sep_token)\n",
    "        # RUn pretrained model\n",
    "        outputs = pretrain_model(\n",
    "            input_ids=torch.tensor([inputs]), \n",
    "            token_type_ids=torch.tensor([[0] * len(inputs)]),\n",
    "            attention_mask=torch.tensor([[1] * len(inputs)])\n",
    "        )\n",
    "        transformed_tokens += outputs.last_hidden_state[0, 1:-1].tolist()   \n",
    "        i = j\n",
    "            \n",
    "    assert len(transformed_tokens) == len(labels) == len(words)\n",
    "    return pd.DataFrame(transformed_tokens), pd.DataFrame(list(zip(labels, words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intermediate-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acceptable-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test transform docs\n",
    "# rawdata = internal_parser.extract_data(internal_parser.get_docs(\"Training\"))\n",
    "# doc0 = rawdata[0]\n",
    "# token_df0, label_df0 = transform_doc(doc0, bert_model, max_token_count=10)\n",
    "# assert token_df0.shape[0] == label_df0.shape[0]\n",
    "# assert token_df0.shape[1] == 768\n",
    "# assert label_df0.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading entity recognition model...\")\n",
    "ner_clf = pickle.load(open(\"../model/ner/internal_nn_1024.model\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entity_recognition(document, mode=\"evaluate/predict\"):\n",
    "    \"\"\"Given a document, run entity recognition and add the prediction to document[\"data_frame\"]\n",
    "    Mode: \"evaluate\" - run evaluation to compare prediction with true values\n",
    "          \"predict\" - return the prediction\n",
    "    \"\"\"\n",
    "    pass\n",
    "def evaluate_entity_recognition(true_labels, predicted_labels):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-timber",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Predicting...\")\n",
    "y_pred = clf.predict(test_tokens)\n",
    "\n",
    "print(\"Results:\")\n",
    "precision, recall, fbeta_score, support = precision_recall_fscore_support(test_labels[\"0\"], y_pred, average=None, labels=clf.classes_)\n",
    "result = pd.DataFrame(index=[label_map_bio[label] for label in clf.classes_])\n",
    "result[\"precision\"] = precision\n",
    "result[\"recall\"] = recall\n",
    "result[\"fbeta_score\"] = fbeta_score\n",
    "result[\"support\"] = support\n",
    "result.loc[\"macro\"] = list(precision_recall_fscore_support(test_labels[\"0\"], y_pred, average=\"macro\"))\n",
    "result.loc[\"micro\"] = list(precision_recall_fscore_support(test_labels[\"0\"], y_pred, average=\"micro\"))                \n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
