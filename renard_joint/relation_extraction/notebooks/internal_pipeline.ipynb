{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "silver-snapshot",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comic-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../parser\")\n",
    "sys.path.append(\"../spert\")\n",
    "import internal_parser as parser\n",
    "import evaluator\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rubber-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_bio = {}\n",
    "for key in parser.entity_encode:\n",
    "    if parser.entity_encode[key] == 0:\n",
    "        label_map_bio[0] = \"O\"\n",
    "    else:\n",
    "        label_map_bio[parser.entity_encode[key]] = \"B-\" + key\n",
    "        label_map_bio[-parser.entity_encode[key]] = \"I-\" + key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "special-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_map = {v: k for k, v in parser.entity_encode.items()}\n",
    "entity_classes = list(entity_label_map.keys())\n",
    "entity_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "binding-reply",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_label_map = {v: k for k, v in parser.relation_encode.items()}\n",
    "relation_classes = list(relation_label_map.keys())\n",
    "relation_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaningful-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inside-trunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sensitive-orange",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "provincial-combining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading entity recognition model...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading entity recognition model...\")\n",
    "ner_model = pickle.load(open(\"../../model/ner/internal_nn_1024.model\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "focal-numbers",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relation extraction model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMre(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=1536, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading relation extraction model...\")\n",
    "re_model = model.BertForMre(len(relation_classes)+1)\n",
    "re_model.load_state_dict(torch.load(\"../../model/re/internal_100.model\", map_location=device))\n",
    "re_model.eval() # Set model for evaluation only\n",
    "re_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unauthorized-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_doc(\n",
    "    document, \n",
    "    pretrain_model, \n",
    "    ignore_index=CrossEntropyLoss().ignore_index,\n",
    "    max_token_count=512,\n",
    "    cls_token=parser.CLS_TOKEN,\n",
    "    sep_token=parser.SEP_TOKEN\n",
    "):\n",
    "    \"\"\"Transform a parsed document with a pre-trained model (BERT)\n",
    "    Only the first token of each word is labeled, the others are masked as 'ignore_index'\n",
    "    The label of O is 0\n",
    "    The label of I is the negation of the corresponding label of B\n",
    "    \"\"\"\n",
    "    transformed_tokens = []\n",
    "    padding_token_count = 2\n",
    "    \n",
    "    tokens = document[\"data_frame\"][\"token_ids\"].tolist()\n",
    "    begins = document[\"data_frame\"][\"begins\"].tolist()\n",
    "    ends = document[\"data_frame\"][\"ends\"].tolist()\n",
    "    labels = document[\"data_frame\"][\"entity_embedding\"].tolist()\n",
    "    words = document[\"data_frame\"][\"words\"].tolist()\n",
    "    sentence_embedding = document[\"data_frame\"][\"sentence_embedding\"].tolist()\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        if i > 0 and begins[i] == begins[i-1] and ends[i] == ends[i-1]:\n",
    "            # Extra tokens from the same word are ignored\n",
    "            labels[i] = ignore_index\n",
    "\n",
    "    for entity in document[\"entity_position\"]:\n",
    "        begin, end = document[\"entity_position\"][entity]\n",
    "        for i in range(begin + 1, end):\n",
    "            # Every subsequence word of an entity is label as I instead of B\n",
    "            if labels[i] != ignore_index:\n",
    "                labels[i] = -labels[i]\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        j = i\n",
    "        while j < len(tokens) and sentence_embedding[i] == sentence_embedding[j] and j - i < max_token_count-padding_token_count:\n",
    "            j += 1\n",
    "        # Segment the document and encode with the pre-trained model\n",
    "        inputs = tokens[i:j]\n",
    "        # Add CLS and SEP tokens\n",
    "        inputs = [cls_token] + inputs\n",
    "        inputs.append(sep_token)\n",
    "        # RUn pretrained model\n",
    "        outputs = pretrain_model(\n",
    "            input_ids=torch.tensor([inputs]), \n",
    "            token_type_ids=torch.tensor([[0] * len(inputs)]),\n",
    "            attention_mask=torch.tensor([[1] * len(inputs)])\n",
    "        )\n",
    "        transformed_tokens += outputs.last_hidden_state[0, 1:-1].tolist()   \n",
    "        i = j\n",
    "            \n",
    "    assert len(transformed_tokens) == len(labels) == len(words)\n",
    "    return pd.DataFrame(transformed_tokens), pd.DataFrame(list(zip(labels, words)), columns=[\"labels\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "romance-peninsula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test transform docs\n",
    "rawdata = parser.extract_data(parser.get_docs(\"Test\"))\n",
    "doc0 = rawdata[0]\n",
    "token_df0, label_df0 = transform_doc(doc0, bert_model)\n",
    "assert token_df0.shape[0] == label_df0.shape[0]\n",
    "assert token_df0.shape[1] == 768\n",
    "assert label_df0.shape[1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "behavioral-innocent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_entity(ner_model, tokens, labels, \n",
    "                   ignore_index=CrossEntropyLoss().ignore_index):\n",
    "    \"\"\"Given a document, runs entity recognition, returns the predicted entity embedding and spans\"\"\"\n",
    "    true_entity_embedding = np.zeros(tokens.shape[0])\n",
    "    pred_entity_embedding = np.zeros(tokens.shape[0])\n",
    "    true_entity_span = []\n",
    "    pred_entity_span = []\n",
    "    true_entity_span_lock = True\n",
    "    pred_entity_span_lock = True\n",
    "    \n",
    "    test_tokens = tokens[labels[\"labels\"] != ignore_index]\n",
    "    pred_labels = ner_model.predict(test_tokens)\n",
    "    \n",
    "    j = -1\n",
    "    true_label = None\n",
    "    pred_label = None\n",
    "    for i in range(tokens.shape[0]):\n",
    "        if labels.at[i, \"labels\"] != ignore_index:\n",
    "            j += 1\n",
    "            true_label = labels.at[i, \"labels\"]\n",
    "            pred_label = pred_labels[j]\n",
    "            \n",
    "            true_entity_type = label_map_bio[true_label]\n",
    "            if true_entity_type.startswith(\"B\") or (true_entity_type.startswith(\"I\") and true_entity_span_lock):\n",
    "                true_entity_span.append((i, i + 1, abs(true_label)))\n",
    "                true_entity_span_lock = False\n",
    "            elif true_entity_type == \"O\":\n",
    "                true_entity_span_lock = True\n",
    "                \n",
    "            pred_entity_type = label_map_bio[pred_label]\n",
    "            if pred_entity_type.startswith(\"B\") or (pred_entity_type.startswith(\"I\") and pred_entity_span_lock):\n",
    "                pred_entity_span.append((i, i + 1, abs(pred_label)))\n",
    "                pred_entity_span_lock = False\n",
    "            elif pred_entity_type == \"O\":\n",
    "                pred_entity_span_lock = True\n",
    "            \n",
    "            true_entity_embedding[i] = true_label\n",
    "            pred_entity_embedding[i] = pred_label\n",
    "                \n",
    "        if not true_entity_span_lock:\n",
    "            if abs(true_label) != abs(true_entity_span[-1][2]):\n",
    "                true_entity_span.append((i, i + 1, abs(true_label)))\n",
    "            else:\n",
    "                true_entity_span[-1] = (true_entity_span[-1][0], i + 1, abs(true_label))\n",
    "                \n",
    "        if not pred_entity_span_lock:\n",
    "            if abs(pred_label) != abs(pred_entity_span[-1][2]):\n",
    "                pred_entity_span.append((i, i + 1, abs(pred_label)))\n",
    "            else:\n",
    "                pred_entity_span[-1] = (pred_entity_span[-1][0], i + 1, abs(pred_label))\n",
    "    \n",
    "    return np.abs(true_entity_embedding), np.abs(pred_entity_embedding), true_entity_span, pred_entity_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "historic-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 7. 7. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 7. 0. 3. 0. 0. 0. 0. 0. 7. 7. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 7. 0. 7. 0. 7. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 7. 7. 7. 0.\n",
      " 7. 0. 0. 0. 0. 3. 0. 0. 4. 4. 0. 0. 0. 0. 0. 0. 7. 7. 0. 0. 0. 0. 0. 0.\n",
      " 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 7. 0. 3. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 3.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 7. 4. 0. 0. 0. 0. 7. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 7. 7. 4. 4. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 7. 7. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 7. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 3. 3. 3. 0. 0. 0. 2. 5. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.\n",
      " 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 7. 0. 7. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 7. 7. 0.\n",
      " 0. 0. 0. 0. 0. 3. 0. 0. 4. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0.\n",
      " 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 7. 0. 3. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 7. 7. 0. 3.\n",
      " 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 7. 7. 0. 0. 0. 0. 0. 7. 7. 0.\n",
      " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 4. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 3. 0. 0.\n",
      " 0. 0. 0. 0. 7. 0. 0. 0. 0. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 7. 0. 0. 0. 7. 7. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 5. 0. 2. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 2. 2. 2. 0. 5. 4. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [(4, 6, 7), (14, 16, 3), (27, 28, 7), (29, 31, 3), (35, 37, 7), (38, 40, 3), (53, 56, 7), (57, 59, 7), (75, 76, 3), (87, 88, 7), (108, 110, 3), (116, 119, 7), (120, 121, 7), (125, 127, 3), (128, 132, 4), (136, 138, 7), (145, 146, 3), (162, 164, 3), (174, 175, 7), (176, 178, 3), (178, 179, 7), (200, 202, 7), (203, 205, 7), (206, 208, 7), (209, 211, 7), (212, 214, 7), (215, 217, 3), (230, 232, 7), (232, 233, 4), (237, 238, 7), (245, 246, 2), (284, 285, 2), (318, 319, 2), (337, 339, 7), (339, 341, 4), (342, 343, 7), (351, 352, 7), (353, 356, 7), (364, 365, 7), (369, 370, 7), (391, 392, 7), (395, 396, 7), (413, 416, 3), (419, 420, 2), (420, 421, 5), (429, 430, 2), (432, 433, 5), (450, 451, 2), (493, 494, 2), (537, 538, 2), (542, 544, 1), (562, 563, 2), (581, 582, 2), (602, 605, 2)] [(5, 6, 7), (27, 28, 7), (36, 37, 7), (38, 40, 3), (53, 56, 7), (58, 59, 7), (87, 88, 7), (116, 119, 7), (125, 127, 3), (128, 129, 4), (137, 138, 7), (145, 146, 3), (174, 175, 7), (176, 178, 3), (178, 179, 7), (200, 202, 7), (203, 205, 7), (206, 208, 7), (209, 211, 7), (212, 214, 7), (215, 217, 3), (220, 221, 3), (230, 232, 7), (237, 241, 7), (245, 246, 2), (272, 273, 7), (284, 285, 2), (298, 300, 3), (317, 318, 2), (318, 319, 2), (339, 340, 4), (342, 343, 7), (351, 352, 7), (357, 358, 3), (364, 365, 7), (369, 370, 7), (391, 392, 7), (395, 397, 7), (415, 416, 3), (419, 420, 2), (429, 430, 2), (450, 451, 2), (493, 494, 2), (495, 497, 5), (537, 538, 2), (542, 544, 1), (562, 563, 2), (579, 580, 5), (581, 582, 2), (584, 585, 5), (602, 603, 2), (603, 605, 2), (606, 607, 5), (607, 608, 4), (653, 654, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Test entity recognition\n",
    "true_entity_embedding, pred_entity_embedding, true_entity_span, pred_entity_span \\\n",
    "    = predict_entity(ner_model, token_df0, label_df0)\n",
    "print(true_entity_embedding, pred_entity_embedding, true_entity_span, pred_entity_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "square-cradle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_relation_span(doc):\n",
    "    true_relation_span = []\n",
    "    for relation in doc[\"relations\"]:\n",
    "        source = doc[\"relations\"][relation][\"source\"]\n",
    "        target = doc[\"relations\"][relation][\"target\"]\n",
    "        relation_type = doc[\"relations\"][relation][\"type\"]\n",
    "        \n",
    "        e1_begin = doc[\"entity_position\"][source][0]\n",
    "        e1_end = doc[\"entity_position\"][source][1]\n",
    "        e1_type = doc[\"data_frame\"].at[e1_begin, \"entity_embedding\"]\n",
    "        \n",
    "        e2_begin = doc[\"entity_position\"][target][0]\n",
    "        e2_end = doc[\"entity_position\"][target][1]\n",
    "        e2_type = doc[\"data_frame\"].at[e2_begin, \"entity_embedding\"]\n",
    "        \n",
    "        true_relation_span.append(((e1_begin, e1_end, e1_type),\n",
    "                                   (e2_begin, e2_end, e2_type), \n",
    "                                   relation_type))\n",
    "    return true_relation_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "relevant-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((125, 127, 3), (128, 132, 4), 1), ((128, 132, 4), (136, 138, 7), 2), ((215, 217, 3), (232, 233, 4), 1), ((232, 233, 4), (230, 232, 7), 2), ((339, 341, 4), (337, 339, 7), 2), ((413, 416, 3), (432, 433, 5), 3), ((413, 416, 3), (420, 421, 5), 3)]\n"
     ]
    }
   ],
   "source": [
    "# Test get_true_relation_span()\n",
    "true_relation_span = get_true_relation_span(doc0)\n",
    "assert len(doc0[\"relations\"]) == len(true_relation_span)\n",
    "print(true_relation_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "vital-means",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entity_mask(sentence_length, entity_span, offset):\n",
    "    e1_mask = []\n",
    "    e2_mask = []\n",
    "    candidate_relation_span = []\n",
    "    for e1 in entity_span:\n",
    "        for e2 in entity_span:\n",
    "            if e1 != e2:\n",
    "                template = [0] * sentence_length\n",
    "                template[e1[0]: e1[1]] = [1] * (e1[1] - e1[0])\n",
    "                e1_mask.append(template)\n",
    "                \n",
    "                template = [0] * sentence_length\n",
    "                template[e2[0]: e2[1]] = [1] * (e2[1] - e2[0])\n",
    "                e2_mask.append(template)\n",
    "                \n",
    "                candidate_relation_span.append(((e1[0] + offset, e1[1] + offset, e1[2]), \n",
    "                                                (e2[0] + offset, e2[1] + offset, e2[2])))\n",
    "                \n",
    "    return torch.tensor(e1_mask, dtype=torch.long), torch.tensor(e2_mask, dtype=torch.long), candidate_relation_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "addressed-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_doc(doc, pred_entity_span, max_token_count=512, max_entity_pair=1000):\n",
    "    \"\"\"Prepare inputs for the relation extraction\"\"\"\n",
    "    sentence_id = 0\n",
    "    starting_index = 0\n",
    "    input_ids = []\n",
    "    # ddd a final row with dummy sentence embedding\n",
    "    doc[\"data_frame\"].loc[doc[\"data_frame\"].index.max() + 1, \"sentence_embedding\"] \\\n",
    "        = doc[\"data_frame\"][\"sentence_embedding\"].max() + 1\n",
    "    for index, row in doc[\"data_frame\"].iterrows():\n",
    "        if row[\"sentence_embedding\"] != sentence_id or index - starting_index >= max_token_count - 2:\n",
    "            offset = starting_index - 1  # -1: space for CLS token\n",
    "            new_entity_span = []\n",
    "            for entity in pred_entity_span:\n",
    "                if starting_index <= entity[0] < entity[1] <= index:\n",
    "                    new_entity_span.append((entity[0] - offset, \n",
    "                                            entity[1] - offset, \n",
    "                                            entity[2]))\n",
    "\n",
    "            # If this sentence has at least two entities for a possible relation\n",
    "            if len(new_entity_span) >= 2:\n",
    "                # Add CLS and SEP to the sentence\n",
    "                input_ids = [parser.CLS_TOKEN] + input_ids + [parser.SEP_TOKEN]\n",
    "                e1_mask, e2_mask, candidate_relation_span = generate_entity_mask(len(input_ids), new_entity_span, offset)\n",
    "                # print(candidate_relation_span)\n",
    "                assert e1_mask.shape[0] == e2_mask.shape[0] == len(candidate_relation_span)\n",
    "                assert len(input_ids) == e1_mask.shape[1] == e2_mask.shape[1]\n",
    "                for i in range(0, e1_mask.shape[0], max_entity_pair):\n",
    "                    yield {\n",
    "                        \"input_ids\": torch.tensor([input_ids]).long().to(device), \n",
    "                        \"attention_mask\": torch.ones((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                        \"token_type_ids\": torch.zeros((1, len(input_ids)), dtype=torch.long).to(device),\n",
    "                        \"e1_mask\": e1_mask[i: min(i+max_entity_pair, e1_mask.shape[0])].to(device),\n",
    "                        \"e2_mask\": e2_mask[i: min(i+max_entity_pair, e1_mask.shape[0])].to(device)\n",
    "                    }, {\n",
    "                        \"offset\": offset,\n",
    "                        \"candidate_relation_span\": candidate_relation_span[i: min(i+max_entity_pair, e1_mask.shape[0])]\n",
    "                    }\n",
    "                \n",
    "            sentence_id = row[\"sentence_embedding\"]\n",
    "            input_ids = []\n",
    "            starting_index = index\n",
    "\n",
    "        input_ids.append(row[\"token_ids\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "precious-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_relation(re_model, doc, pred_entity_span, max_token_count=512, max_entity_pair=1000):\n",
    "    \"\"\"Predict the relation type in a document given the predicted entity spans\"\"\"\n",
    "    pred_relation_span = []\n",
    "    data_generator = prepare_doc(doc, pred_entity_span, max_token_count, max_entity_pair)\n",
    "    for inputs, infos in data_generator:\n",
    "        outputs = re_model(**inputs)\n",
    "        pred_label = F.softmax(outputs.logits, dim=-1).argmax(dim=1)\n",
    "        # print(pred_label)\n",
    "        for i in range(pred_label.shape[0]):\n",
    "            if pred_label[i] != 0:\n",
    "                candidate_relation = infos[\"candidate_relation_span\"][i]\n",
    "                pred_relation_span.append((candidate_relation[0], candidate_relation[1], pred_label[i].item()))\n",
    "    return pred_relation_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "supreme-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Test predict_relation()\n",
    "print(predict_relation(re_model, doc0, pred_entity_span))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bound-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(group, bert_model, ner_model, re_model, \n",
    "             entity_label_map, entity_classes,\n",
    "             relation_label_map, relation_classes,\n",
    "             max_token_count=512, max_entity_pair=1000):\n",
    "    true_entity_embeddings = []\n",
    "    pred_entity_embeddings = []\n",
    "    true_entity_spans = []\n",
    "    pred_entity_spans = []\n",
    "    true_relation_spans = []\n",
    "    pred_relation_spans = []\n",
    "    \n",
    "    docs = parser.get_docs(group)\n",
    "    data = parser.extract_data(docs)\n",
    "    for doc in tqdm(data, total=len(data), desc=\"Evaluation \" + group):\n",
    "        token_df, label_df = transform_doc(doc, bert_model, max_token_count=max_token_count)\n",
    "        \n",
    "        # entity recognition\n",
    "        true_entity_embedding, pred_entity_embedding, true_entity_span, pred_entity_span \\\n",
    "            = predict_entity(ner_model, token_df, label_df)\n",
    "        true_entity_embeddings += true_entity_embedding.tolist()\n",
    "        pred_entity_embeddings += pred_entity_embedding.tolist()\n",
    "        true_entity_spans.append(true_entity_span)\n",
    "        pred_entity_spans.append(pred_entity_span)\n",
    "        \n",
    "        true_relation_span = get_true_relation_span(doc)\n",
    "        true_relation_spans.append(true_relation_span)\n",
    "        \n",
    "        # relation extraction\n",
    "        pred_relation_span = predict_relation(re_model, doc, pred_entity_span, \n",
    "                                              max_token_count=max_token_count, \n",
    "                                              max_entity_pair=max_entity_pair)\n",
    "        pred_relation_spans.append(pred_relation_span)\n",
    "        \n",
    "    results = pd.concat([\n",
    "        evaluator.evaluate_span(true_entity_spans, pred_entity_spans, entity_label_map, entity_classes),\n",
    "        evaluator.evaluate_results(true_entity_embeddings, pred_entity_embeddings, entity_label_map, entity_classes),\n",
    "        evaluator.evaluate_loose_relation_span(true_relation_spans, pred_relation_spans, relation_label_map, relation_classes),\n",
    "        evaluator.evaluate_span(true_relation_spans, pred_relation_spans, relation_label_map, relation_classes),\n",
    "    ], keys=[\"Entity span\", \"Entity embedding\", \"Loose relation\", \"Strict relation\"])\n",
    "    results.to_csv(\"../../model/re/internal_evaluate_\" + group + \".csv\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "phantom-census",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Test: 100%|█████████████████████████████████████████████████████████████████| 92/92 [09:02<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  fbeta_score  \\\n",
      "Entity span      EnvironmentalIssues   0.658765  0.722298     0.689070   \n",
      "                 Date                  0.776596  0.876877     0.823695   \n",
      "                 Organisation          0.765343  0.699093     0.730720   \n",
      "                 CommitmentLevel       0.380573  0.326057     0.351212   \n",
      "                 Location              0.784232  0.675000     0.725528   \n",
      "                 CoalActivity          0.708333  0.772727     0.739130   \n",
      "                 SocialIssues          0.716578  0.778423     0.746221   \n",
      "                 SocialOfficialTexts   0.268293  0.511628     0.352000   \n",
      "                 macro                 0.632339  0.670263     0.644697   \n",
      "                 micro                 0.670452  0.680590     0.675483   \n",
      "Entity embedding EnvironmentalIssues   0.841395  0.837112     0.839248   \n",
      "                 Date                  0.954654  0.968523     0.961538   \n",
      "                 Organisation          0.835218  0.721060     0.773952   \n",
      "                 CommitmentLevel       0.540166  0.383858     0.448792   \n",
      "                 Location              0.863971  0.705706     0.776860   \n",
      "                 CoalActivity          0.846154  0.846154     0.846154   \n",
      "                 SocialIssues          0.866442  0.849285     0.857778   \n",
      "                 SocialOfficialTexts   0.845361  0.766355     0.803922   \n",
      "                 macro                 0.824170  0.759757     0.788530   \n",
      "                 micro                 0.822872  0.754956     0.787453   \n",
      "Loose relation   Makes                 0.321809  0.263617     0.289820   \n",
      "                 Of                    0.248408  0.174888     0.205263   \n",
      "                 IsRelatedTo           0.658228  0.400000     0.497608   \n",
      "                 HasActivity           0.555556  0.714286     0.625000   \n",
      "                 Recognizes            0.327869  0.625000     0.430108   \n",
      "                 In                    1.000000  0.250000     0.400000   \n",
      "                 IsInvolvedIn          0.000000  0.000000     0.000000   \n",
      "                 macro                 0.444553  0.346827     0.349686   \n",
      "                 micro                 0.316951  0.242890     0.275022   \n",
      "Strict relation  Makes                 0.247368  0.204793     0.224076   \n",
      "                 Of                    0.175105  0.124066     0.145232   \n",
      "                 IsRelatedTo           0.544304  0.330769     0.411483   \n",
      "                 HasActivity           0.400000  0.571429     0.470588   \n",
      "                 Recognizes            0.151515  0.312500     0.204082   \n",
      "                 In                    0.000000  0.000000     0.000000   \n",
      "                 IsInvolvedIn          0.000000  0.000000     0.000000   \n",
      "                 macro                 0.216899  0.220508     0.207923   \n",
      "                 micro                 0.231683  0.179862     0.202510   \n",
      "\n",
      "                                      support  \n",
      "Entity span      EnvironmentalIssues   1462.0  \n",
      "                 Date                   333.0  \n",
      "                 Organisation          1213.0  \n",
      "                 CommitmentLevel        733.0  \n",
      "                 Location               280.0  \n",
      "                 CoalActivity            22.0  \n",
      "                 SocialIssues          1205.0  \n",
      "                 SocialOfficialTexts     43.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Entity embedding EnvironmentalIssues   2161.0  \n",
      "                 Date                   413.0  \n",
      "                 Organisation          1434.0  \n",
      "                 CommitmentLevel       1016.0  \n",
      "                 Location               333.0  \n",
      "                 CoalActivity            26.0  \n",
      "                 SocialIssues          1818.0  \n",
      "                 SocialOfficialTexts    214.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Loose relation   Makes                  459.0  \n",
      "                 Of                     669.0  \n",
      "                 IsRelatedTo            130.0  \n",
      "                 HasActivity              7.0  \n",
      "                 Recognizes              32.0  \n",
      "                 In                       4.0  \n",
      "                 IsInvolvedIn             0.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Strict relation  Makes                  459.0  \n",
      "                 Of                     669.0  \n",
      "                 IsRelatedTo            130.0  \n",
      "                 HasActivity              7.0  \n",
      "                 Recognizes              32.0  \n",
      "                 In                       4.0  \n",
      "                 IsInvolvedIn             0.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n"
     ]
    }
   ],
   "source": [
    "evaluate(\"Test\", bert_model, ner_model, re_model, \n",
    "         entity_label_map, entity_classes, relation_label_map, relation_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ambient-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences, bert_model, ner_model, re_model, \n",
    "            entity_label_map, entity_classes,\n",
    "            relation_label_map, relation_classes,\n",
    "            max_token_count=512, max_entity_pair=1000):\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word_list = sentence.split()\n",
    "        words = []\n",
    "        token_ids = []\n",
    "        begins = []\n",
    "        ends = []\n",
    "        # transform a sentence to a document for prediction\n",
    "        for i, word in enumerate(word_list):\n",
    "            token_id = parser.tokenizer(word)[\"input_ids\"][1:-1]\n",
    "            for tid in token_id:\n",
    "                words.append(word)\n",
    "                token_ids.append(tid)\n",
    "                begins.append(i)\n",
    "                ends.append(i + 1)\n",
    "        data_frame = pd.DataFrame()\n",
    "        data_frame[\"words\"] = words\n",
    "        data_frame[\"token_ids\"] = token_ids\n",
    "        data_frame[\"begins\"] = begins\n",
    "        data_frame[\"ends\"] = ends\n",
    "        data_frame[\"entity_embedding\"] = 0\n",
    "        data_frame[\"sentence_embedding\"] = 0\n",
    "        doc = {\"data_frame\": data_frame,\n",
    "            \"entity_position\": {}, # Suppose to appear in non-overlapping dataset\n",
    "            \"relations\": {}}\n",
    "        # predict\n",
    "        token_df, label_df = transform_doc(doc, bert_model, max_token_count=max_token_count)\n",
    "        \n",
    "        # entity recognition\n",
    "        true_entity_embedding, pred_entity_embedding, true_entity_span, pred_entity_span \\\n",
    "            = predict_entity(ner_model, token_df, label_df)\n",
    "        \n",
    "        # relation extraction\n",
    "        pred_relation_span = predict_relation(re_model, doc, pred_entity_span, \n",
    "                                              max_token_count=max_token_count, \n",
    "                                              max_entity_pair=max_entity_pair)\n",
    "        # print result\n",
    "        tokens = parser.tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Entities: (\", len(pred_entity_span), \")\")\n",
    "        for begin, end, entity_type in pred_entity_span:\n",
    "            print(entity_label_map[entity_type], \"|\", \" \".join(tokens[begin:end]))\n",
    "        print(\"Relations: (\", len(pred_relation_span), \")\")\n",
    "        for e1, e2, relation_type in pred_relation_span:\n",
    "            print(relation_label_map[relation_type], \"|\", \n",
    "                  \" \".join(tokens[e1[0]:e1[1]]), \"|\", \n",
    "                  \" \".join(tokens[e2[0]:e2[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recreational-title",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Key Opportunities\n",
      "We have significant opportunities to further \n",
      "advance sustainability within our operations and \n",
      "those of our business partners.\n",
      "Entities: ( 2 )\n",
      "Organisation | we\n",
      "CommitmentLevel | opportunities\n",
      "Relations: ( 1 )\n",
      "Makes | we | opportunities\n"
     ]
    }
   ],
   "source": [
    "predict([\"Key Opportunities\\nWe have significant opportunities to further \\nadvance sustainability within our operations and \\nthose of our business partners.\"], \n",
    "        bert_model, ner_model, re_model, \n",
    "        entity_label_map, entity_classes, relation_label_map, relation_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-gnome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
