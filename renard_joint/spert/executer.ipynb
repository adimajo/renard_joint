{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "historical-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ignored-concern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('expand_frame_repr', False)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, BertConfig, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "egyptian-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dutch-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this part for different dataset\n",
    "import internal_constants as constants\n",
    "import internal_input_generator as input_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "piano-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fresh-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "convinced-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run reference model\n",
    "# input_generator.parser.entity_encode = {'O': 0, 'B-Loc': 1, 'I-Loc': 1, 'B-Peop': 3, 'I-Peop': 3, \n",
    "#                                          'B-Org': 2, 'I-Org': 2, 'B-Other': 4, 'I-Other': 4}\n",
    "# input_generator.parser.relation_encode = {'N': 0, 'Kill': 2, 'Located_In': 5, 'OrgBased_In': 3,\n",
    "#                                            'Live_In': 4, 'Work_For': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "timely-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_map = {v: k for k, v in input_generator.parser.entity_encode.items()}\n",
    "entity_classes = list(entity_label_map.keys())\n",
    "entity_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "global-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_label_map = {v: k for k, v in input_generator.parser.relation_encode.items()}\n",
    "relation_classes = list(relation_label_map.keys())\n",
    "relation_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "magnetic-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for internal dataset\n",
    "relation_possibility = {\n",
    "    (3, 5): [0, 0, 0, 1, 0, 0, 0, 0], # Organization + Location -> IsRelatedTo\n",
    "    (3, 6): [0, 0, 0, 0, 1, 0, 0, 0], # Organization + CoalActivity -> HasActivity\n",
    "    (3, 8): [0, 0, 0, 0, 0, 1, 0, 0], # Organization + SocialOfficialText -> Recognizes\n",
    "    (3, 4): [0, 1, 0, 0, 0, 0, 0, 0], # Organization + CommitmentLevel -> Makes\n",
    "    (4, 1): [0, 0, 1, 0, 0, 0, 0, 0], # CommitmentLevel + EnvironmentalIssues -> Of\n",
    "    (4, 7): [0, 0, 1, 0, 0, 0, 0, 0]  # CommitmentLevel + SocialIssues -> Of\n",
    "}\n",
    "\n",
    "relation_classes.remove(6) # remove In\n",
    "relation_classes.remove(7) # remove IsInvolvedIn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "residential-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for conll04\n",
    "# relation_possibility = {\n",
    "#     (2, 2): [0, 1, 0, 0, 0, 0], # people kill people\n",
    "#     (1, 1): [0, 0, 1, 0, 0, 0], # location located in location\n",
    "#     (3, 1): [0, 0, 0, 1, 0, 0], # organization based in location\n",
    "#     (2, 1): [0, 0, 0, 0, 1, 0], # people live in location\n",
    "#     (2, 3): [0, 0, 0, 0, 0, 1]  # people work for organization\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "delayed-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(constants.model_path)\n",
    "input_generator.parser.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "amended-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_params = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': constants.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.0}]\n",
    "    return optimizer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "constitutional-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_first_tokens(embedding, words):\n",
    "    \"\"\"Take the embedding of the first token of each word\"\"\"\n",
    "    reduced_embedding = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0 or word != words[i-1]:\n",
    "            reduced_embedding.append(embedding[i])\n",
    "    return reduced_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compact-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(spert_model, group, suffix=\"\", compute_loss=False):\n",
    "    \"\"\"\n",
    "    If 'compute_loss' is True, this function performed similarly to train, without backwarding losses,\n",
    "    useful for printing losses on the test dataset.\n",
    "    \"\"\"\n",
    "    spert_model.eval()\n",
    "    eval_generator = input_generator.data_generator(group, device, \n",
    "                                                    is_training=compute_loss,\n",
    "                                                    neg_entity_count=constants.neg_entity_count, \n",
    "                                                    neg_relation_count=constants.neg_relation_count, \n",
    "                                                    max_span_size=constants.max_span_size)\n",
    "    eval_dataset = list(eval_generator)\n",
    "    eval_size = len(eval_dataset)\n",
    "    \n",
    "    if compute_loss:\n",
    "        entity_losses = []\n",
    "        relation_losses = []\n",
    "        losses = []\n",
    "    else:\n",
    "        eval_entity_span_pred = []\n",
    "        eval_entity_span_true = []\n",
    "        eval_entity_embedding_pred = []\n",
    "        eval_entity_embedding_true = []\n",
    "        eval_relation_span_pred = []\n",
    "        eval_relation_span_true = []\n",
    "        \n",
    "    for inputs, infos in tqdm(eval_dataset, total=eval_size, desc=\"Evaluation \"+group):\n",
    "        # forward\n",
    "        outputs = spert_model(**inputs, is_training=compute_loss)\n",
    "        \n",
    "        if compute_loss:\n",
    "            losses.append(outputs[\"loss\"].item())\n",
    "            entity_losses.append(outputs[\"entity\"][\"loss\"])\n",
    "            if outputs[\"relation\"] is not None: relation_losses.append(outputs[\"relation\"][\"loss\"])\n",
    "        else:\n",
    "            # retrieve results for evaluation\n",
    "            eval_entity_span_pred.append(outputs[\"entity\"][\"span\"])\n",
    "            eval_entity_span_true.append(infos[\"entity_span\"])\n",
    "\n",
    "            if not constants.is_overlapping:\n",
    "                eval_entity_embedding_pred += take_first_tokens(outputs[\"entity\"][\"embedding\"].tolist(), infos[\"words\"])\n",
    "                eval_entity_embedding_true += take_first_tokens(infos[\"entity_embedding\"].tolist(), infos[\"words\"])\n",
    "                assert len(eval_entity_embedding_pred) == len(eval_entity_embedding_true)\n",
    "\n",
    "            eval_relation_span_pred.append([] if outputs[\"relation\"] == None else outputs[\"relation\"][\"span\"])\n",
    "            eval_relation_span_true.append(infos[\"relation_span\"])\n",
    "    \n",
    "    if compute_loss:\n",
    "        print(\"average evaluation loss:\", sum(losses) / len(losses))\n",
    "        print(\"average evaluation entity loss:\", sum(entity_losses) / len(entity_losses))\n",
    "        print(\"average evaluation relation loss:\", sum(relation_losses) / len(relation_losses))\n",
    "        return  sum(losses) / len(losses), sum(entity_losses) / len(entity_losses), sum(relation_losses) / len(relation_losses)\n",
    "    else:\n",
    "        # evaluate & save\n",
    "        results = pd.concat([\n",
    "            evaluator.evaluate_span(eval_entity_span_true, eval_entity_span_pred, entity_label_map, entity_classes),\n",
    "            evaluator.evaluate_results(eval_entity_embedding_true, eval_entity_embedding_pred, entity_label_map, entity_classes),\n",
    "            evaluator.evaluate_loose_relation_span(eval_relation_span_true, eval_relation_span_pred, relation_label_map, relation_classes),\n",
    "            evaluator.evaluate_span(eval_relation_span_true, eval_relation_span_pred, relation_label_map, relation_classes),\n",
    "        ], keys=[\"Entity span\", \"Entity embedding\", \"Loose relation\", \"Strict relation\"])\n",
    "        results.to_csv(constants.model_save_path + \"evaluate_\" + group + \"_\" + suffix + \".csv\")\n",
    "        print(\"Evaluation result on \" + group + \" dataset\")\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "olympic-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train the model and evaluate on the dev dataset\n",
    "    \"\"\"\n",
    "    min_loss = math.inf\n",
    "    wait_time = 0\n",
    "    train_losses = []\n",
    "    train_entity_losses = []\n",
    "    train_relation_losses = []\n",
    "    eval_losses = []\n",
    "    eval_entity_losses = []\n",
    "    eval_relation_losses = []\n",
    "    \n",
    "    # Training\n",
    "    train_generator = input_generator.data_generator(constants.train_dataset, device, \n",
    "                                                     is_training=True,\n",
    "                                                     neg_entity_count=constants.neg_entity_count, \n",
    "                                                     neg_relation_count=constants.neg_relation_count, \n",
    "                                                     max_span_size=constants.max_span_size)\n",
    "    train_dataset = list(train_generator)\n",
    "    random.shuffle(train_dataset)\n",
    "    train_size = len(train_dataset)\n",
    "    config = BertConfig.from_pretrained(constants.model_path)\n",
    "    spert_model = model.SpERT.from_pretrained(constants.model_path,\n",
    "                                              config=config,\n",
    "                                              # SpERT model parameters\n",
    "                                              relation_types=constants.relation_types, \n",
    "                                              entity_types=constants.entity_types, \n",
    "                                              width_embedding_size=constants.width_embedding_size, \n",
    "                                              prop_drop=constants.prop_drop, \n",
    "                                              freeze_transformer=False, \n",
    "                                              max_pairs=constants.max_pairs, \n",
    "                                              is_overlapping=constants.is_overlapping, \n",
    "                                              relation_filter_threshold=constants.relation_filter_threshold,\n",
    "                                              relation_possibility=relation_possibility)\n",
    "    spert_model.to(device)\n",
    "    optimizer_params = get_optimizer_params(spert_model)\n",
    "    optimizer = AdamW(optimizer_params, lr=constants.lr, weight_decay=constants.weight_decay, correct_bias=False)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                             num_warmup_steps=constants.lr_warmup*train_size*constants.epochs, \n",
    "                             num_training_steps=train_size*constants.epochs)\n",
    "    \n",
    "    for epoch in range(constants.epochs):\n",
    "        losses = []\n",
    "        entity_losses = []\n",
    "        relation_losses = []\n",
    "        train_entity_pred = []\n",
    "        train_entity_true = []\n",
    "        train_relation_pred = []\n",
    "        train_relation_true = []\n",
    "        spert_model.zero_grad()\n",
    "        for inputs, infos in tqdm(train_dataset, total=train_size, desc='Train epoch %s' % epoch):\n",
    "            spert_model.train()\n",
    "            \n",
    "            # forward\n",
    "            outputs = spert_model(**inputs, is_training=True)\n",
    "            \n",
    "            # backward\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(spert_model.parameters(), constants.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            spert_model.zero_grad()\n",
    "            \n",
    "            # retrieve results for evaluation\n",
    "            losses.append(loss.item())\n",
    "            entity_losses.append(outputs[\"entity\"][\"loss\"])\n",
    "            if outputs[\"relation\"] is not None: relation_losses.append(outputs[\"relation\"][\"loss\"])\n",
    "\n",
    "            train_entity_pred += outputs[\"entity\"][\"pred\"].tolist()\n",
    "            train_entity_true += inputs[\"entity_label\"].tolist()\n",
    "            train_relation_pred += [] if outputs[\"relation\"] == None else outputs[\"relation\"][\"pred\"].tolist()\n",
    "            train_relation_true += inputs[\"relation_label\"].tolist()\n",
    "            assert len(train_entity_pred) == len(train_entity_true)\n",
    "            assert len(train_relation_pred) == len(train_relation_true)\n",
    "            \n",
    "        # evaluate & save checkpoint\n",
    "        print(\"epoch:\", epoch,\"average training loss:\", sum(losses) / len(losses))\n",
    "        print(\"epoch:\", epoch,\"average training entity loss:\", sum(entity_losses) / len(entity_losses))\n",
    "        print(\"epoch:\", epoch,\"average training relation loss:\", sum(relation_losses) / len(relation_losses))\n",
    "        results = pd.concat([\n",
    "            evaluator.evaluate_results(train_entity_true, train_entity_pred, entity_label_map, entity_classes),\n",
    "            evaluator.evaluate_results(train_relation_true, train_relation_pred, relation_label_map, relation_classes)\n",
    "        ], keys=[\"Entity\", \"Relation\"])\n",
    "        results.to_csv(constants.model_save_path + \"epoch_\" + str(epoch) + \".csv\")\n",
    "        \n",
    "        # evalutation on dev dataset\n",
    "        eval_loss, eval_entity_loss, eval_relation_loss = evaluate(spert_model, constants.dev_dataset, compute_loss=True)\n",
    "        evaluate(spert_model, constants.dev_dataset, suffix=str(epoch))\n",
    "        \n",
    "        # record losses\n",
    "        train_losses.append(sum(losses) / len(losses))\n",
    "        train_entity_losses.append(sum(entity_losses) / len(entity_losses))\n",
    "        train_relation_losses.append(sum(relation_losses) / len(relation_losses))\n",
    "        eval_losses.append(eval_loss)\n",
    "        eval_entity_losses.append(eval_entity_loss)\n",
    "        eval_relation_losses.append(eval_relation_loss)\n",
    "        \n",
    "        torch.save(spert_model.state_dict(), constants.model_save_path + \"epoch_\" + str(epoch) + \".model\")\n",
    "        \n",
    "        # early stopping\n",
    "        if eval_loss < min_loss:\n",
    "            min_loss = eval_loss\n",
    "            wait_time = 0\n",
    "        else:\n",
    "            wait_time += 1\n",
    "        if wait_time > constants.patience:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "    plt.plot(list(range(len(train_losses))), train_losses, 'r')\n",
    "    plt.plot(list(range(len(eval_losses))), eval_losses, 'b')\n",
    "    plt.show()\n",
    "    plt.plot(list(range(len(train_entity_losses))), train_entity_losses, 'r')\n",
    "    plt.plot(list(range(len(eval_entity_losses))), eval_entity_losses, 'b')\n",
    "    plt.show()\n",
    "    plt.plot(list(range(len(train_relation_losses))), train_relation_losses, 'r')\n",
    "    plt.plot(list(range(len(eval_relation_losses))), eval_relation_losses, 'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "previous-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(spert_model, sentences):\n",
    "    for sentence in sentences:\n",
    "        word_list = sentence.split()\n",
    "        words = []\n",
    "        token_ids = []\n",
    "        # transform a sentence to a document for prediction\n",
    "        for word in word_list:\n",
    "            token_id = tokenizer(word)[\"input_ids\"][1:-1]\n",
    "            for tid in token_id:\n",
    "                words.append(word)\n",
    "                token_ids.append(tid)\n",
    "        data_frame = pd.DataFrame()\n",
    "        data_frame[\"words\"] = words\n",
    "        data_frame[\"token_ids\"] = token_ids\n",
    "        data_frame[\"entity_embedding\"] = 0\n",
    "        data_frame[\"sentence_embedding\"] = 0 # for internal datasets\n",
    "        doc = {\"data_frame\": data_frame,\n",
    "            \"entity_position\": {}, # Suppose to appear in non-overlapping dataset\n",
    "            \"entities\": {}, # Suppose to appear in overlapping dataset\n",
    "            \"relations\": {}}\n",
    "        # predict\n",
    "        inputs, infos = input_generator.doc_to_input(doc, device, \n",
    "                                                     is_training=False, \n",
    "                                                     max_span_size=constants.max_span_size)\n",
    "        outputs = spert_model(**inputs, is_training=False)\n",
    "        pred_entity_span = outputs[\"entity\"][\"span\"]\n",
    "        pred_relation_span = [] if outputs[\"relation\"] is None else outputs[\"relation\"][\"span\"]\n",
    "        # print result\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Entities: (\", len(pred_entity_span), \")\")\n",
    "        for begin, end, entity_type in pred_entity_span:\n",
    "            print(entity_label_map[entity_type], \"|\", \" \".join(tokens[begin:end]))\n",
    "        print(\"Relations: (\", len(pred_relation_span), \")\")\n",
    "        for e1, e2, relation_type in pred_relation_span:\n",
    "            print(relation_label_map[relation_type], \"|\", \n",
    "                  \" \".join(tokens[e1[0]:e1[1]]), \"|\", \n",
    "                  \" \".join(tokens[e2[0]:e2[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-instrumentation",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing SpERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SpERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpERT were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['relation_classifier.weight', 'relation_classifier.bias', 'entity_classifier.weight', 'entity_classifier.bias', 'width_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Train epoch 0: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [18:43<00:00,  4.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 average training loss: 0.4843374995312501\n",
      "epoch: 0 average training entity loss: 0.3458811406296279\n",
      "epoch: 0 average training relation loss: 0.20337072730089734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Test: 100%|█████████████████████████████████████████████████████████████| 1738/1738 [03:46<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average evaluation loss: 0.07585260456281107\n",
      "average evaluation entity loss: 0.05058491181381801\n",
      "average evaluation relation loss: 0.03869185011304652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Test:  94%|█████████████████████████████████████████████████████████▏   | 1628/1738 [06:22<00:34,  3.19it/s]"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-western",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(constants.model_path)\n",
    "spert_model = model.SpERT.from_pretrained(constants.model_path,\n",
    "                                          config=config,\n",
    "                                          # SpERT model parameters\n",
    "                                          relation_types=constants.relation_types, \n",
    "                                          entity_types=constants.entity_types, \n",
    "                                          width_embedding_size=constants.width_embedding_size, \n",
    "                                          prop_drop=constants.prop_drop, \n",
    "                                          freeze_transformer=True, \n",
    "                                          max_pairs=constants.max_pairs, \n",
    "                                          is_overlapping=constants.is_overlapping, \n",
    "                                          relation_filter_threshold=constants.relation_filter_threshold,\n",
    "                                          relation_possibility=relation_possibility)\n",
    "spert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(constants.model_save_path + \"epoch_\" + str(29) + \".model\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run reference model\n",
    "# state_dict = torch.load(\"../../model/spert/reference/reference_model.bin\")\n",
    "# state_dict[\"relation_classifier.weight\"] = state_dict[\"rel_classifier.weight\"]\n",
    "# state_dict[\"relation_classifier.bias\"] = state_dict[\"rel_classifier.bias\"]\n",
    "# state_dict[\"width_embedding.weight\"] = state_dict[\"size_embeddings.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-glossary",
   "metadata": {},
   "outputs": [],
   "source": [
    "spert_model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-clothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(spert_model, constants.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-sullivan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(spert_model, sentences=[\"The companys main initiative is to develop the Program for the Restitution of Livelihoods in Mozambique and Malawi to assist 15,500 families affected by involuntary displacement due to the Nacala Corridor installation .\", \n",
    "#                                 \"The measures taken by BPCL intended to support rights to exercise freedom of association and bargaining are as follows : BPCL does not discriminate between its permanent and contract employees and they are treated alike by the Company in terms of various aspects especially for protecting their rights and skilling .\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-president",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
