{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "changing-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fitting-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AdamW, BertConfig, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bacterial-essex",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "promotional-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify this part for different dataset\n",
    "import internal_constants as constants\n",
    "import internal_input_generator as input_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cultural-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "british-provider",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run reference model\n",
    "# input_generator.parser.entity_encode = {'O': 0, 'B-Loc': 1, 'I-Loc': 1, 'B-Peop': 3, 'I-Peop': 3, \n",
    "#                                          'B-Org': 2, 'I-Org': 2, 'B-Other': 4, 'I-Other': 4}\n",
    "# input_generator.parser.relation_encode = {'N': 0, 'Kill': 2, 'Located_In': 5, 'OrgBased_In': 3,\n",
    "#                                            'Live_In': 4, 'Work_For': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adequate-better",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_label_map = {v: k for k, v in input_generator.parser.entity_encode.items()}\n",
    "entity_classes = list(entity_label_map.keys())\n",
    "entity_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "supported-occurrence",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_label_map = {v: k for k, v in input_generator.parser.relation_encode.items()}\n",
    "relation_classes = list(relation_label_map.keys())\n",
    "relation_classes.remove(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "major-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(constants.model_path)\n",
    "input_generator.parser.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "female-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_params(model):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_params = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': constants.weight_decay},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n",
    "         'weight_decay': 0.0}]\n",
    "    return optimizer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "statutory-privacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_first_tokens(embedding, words):\n",
    "    \"\"\"Take the embedding of the first token of each word\"\"\"\n",
    "    reduced_embedding = []\n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0 or word != words[i-1]:\n",
    "            reduced_embedding.append(embedding[i])\n",
    "    return reduced_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "suspended-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(spert_model, group):\n",
    "    spert_model.eval()\n",
    "    eval_entity_span_pred = []\n",
    "    eval_entity_span_true = []\n",
    "    eval_entity_embedding_pred = []\n",
    "    eval_entity_embedding_true = []\n",
    "    eval_relation_span_pred = []\n",
    "    eval_relation_span_true = []\n",
    "    eval_generator = input_generator.data_generator(group, device, \n",
    "                                                    is_training=False,\n",
    "                                                    neg_entity_count=constants.neg_entity_count, \n",
    "                                                    neg_relation_count=constants.neg_relation_count, \n",
    "                                                    max_span_size=constants.max_span_size)\n",
    "    eval_dataset = list(eval_generator)\n",
    "    eval_size = len(eval_dataset)\n",
    "    for inputs, infos in tqdm(eval_dataset, total=eval_size, desc=\"Evaluation \"+group):\n",
    "        # forward\n",
    "        outputs = spert_model(**inputs, is_training=False)\n",
    "        # retrieve results for evaluation\n",
    "        eval_entity_span_pred.append(outputs[\"entity\"][\"span\"])\n",
    "        eval_entity_span_true.append(infos[\"entity_span\"])\n",
    "        if not constants.is_overlapping:\n",
    "            eval_entity_embedding_pred += take_first_tokens(outputs[\"entity\"][\"embedding\"].tolist(), infos[\"words\"])\n",
    "            eval_entity_embedding_true += take_first_tokens(infos[\"entity_embedding\"].tolist(), infos[\"words\"])\n",
    "            assert len(eval_entity_embedding_pred) == len(eval_entity_embedding_true)\n",
    "        eval_relation_span_pred.append([] if outputs[\"relation\"] == None else outputs[\"relation\"][\"span\"])\n",
    "        eval_relation_span_true.append(infos[\"relation_span\"])\n",
    "    # evaluate & save\n",
    "    results = pd.concat([\n",
    "        evaluator.evaluate_span(eval_entity_span_true, eval_entity_span_pred, entity_label_map, entity_classes),\n",
    "        evaluator.evaluate_results(eval_entity_embedding_true, eval_entity_embedding_pred, entity_label_map, entity_classes),\n",
    "        evaluator.evaluate_loose_relation_span(eval_relation_span_true, eval_relation_span_pred, relation_label_map, relation_classes),\n",
    "        evaluator.evaluate_span(eval_relation_span_true, eval_relation_span_pred, relation_label_map, relation_classes),\n",
    "    ], keys=[\"Entity span\", \"Entity embedding\", \"Loose relation\", \"Strict relation\"])\n",
    "    results.to_csv(constants.model_save_path + \"evaluate_\" + group + \".csv\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "photographic-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train the model and evaluate on the dev dataset\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    train_generator = input_generator.data_generator(constants.train_dataset, device, \n",
    "                                                     is_training=True,\n",
    "                                                     neg_entity_count=constants.neg_entity_count, \n",
    "                                                     neg_relation_count=constants.neg_relation_count, \n",
    "                                                     max_span_size=constants.max_span_size)\n",
    "    train_dataset = list(train_generator)\n",
    "    random.shuffle(train_dataset)\n",
    "    train_size = len(train_dataset)\n",
    "    config = BertConfig.from_pretrained(constants.model_path)\n",
    "    spert_model = model.SpERT.from_pretrained(constants.model_path,\n",
    "                                              config=config,\n",
    "                                              # SpERT model parameters\n",
    "                                              relation_types=constants.relation_types, \n",
    "                                              entity_types=constants.entity_types, \n",
    "                                              width_embedding_size=constants.width_embedding_size, \n",
    "                                              prop_drop=constants.prop_drop, \n",
    "                                              freeze_transformer=False, \n",
    "                                              max_pairs=constants.max_pairs, \n",
    "                                              is_overlapping=constants.is_overlapping, \n",
    "                                              relation_filter_threshold=constants.relation_filter_threshold)\n",
    "    spert_model.to(device)\n",
    "    optimizer_params = get_optimizer_params(spert_model)\n",
    "    optimizer = AdamW(optimizer_params, lr=constants.lr, weight_decay=constants.weight_decay, correct_bias=False)\n",
    "    scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "                             num_warmup_steps=constants.lr_warmup*train_size*constants.epochs, \n",
    "                             num_training_steps=train_size*constants.epochs)\n",
    "    for epoch in range(constants.epochs):\n",
    "        losses = []\n",
    "        entity_losses = []\n",
    "        relation_losses = []\n",
    "        train_entity_pred = []\n",
    "        train_entity_true = []\n",
    "        train_relation_pred = []\n",
    "        train_relation_true = []\n",
    "        spert_model.zero_grad()\n",
    "        for inputs, infos in tqdm(train_dataset, total=train_size, desc='Train epoch %s' % epoch):\n",
    "            spert_model.train()\n",
    "            # forward\n",
    "            outputs = spert_model(**inputs, is_training=True)\n",
    "            # backward\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(spert_model.parameters(), constants.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            spert_model.zero_grad()\n",
    "            # retrieve results for evaluation\n",
    "            losses.append(loss.item())\n",
    "            entity_losses.append(outputs[\"entity\"][\"loss\"])\n",
    "            if outputs[\"relation\"] is not None: relation_losses.append(outputs[\"relation\"][\"loss\"])\n",
    "                \n",
    "            train_entity_pred += outputs[\"entity\"][\"pred\"].tolist()\n",
    "            train_entity_true += inputs[\"entity_label\"].tolist()\n",
    "            train_relation_pred += [] if outputs[\"relation\"] == None else outputs[\"relation\"][\"pred\"].tolist()\n",
    "            train_relation_true += inputs[\"relation_label\"].tolist()\n",
    "            assert len(train_entity_pred) == len(train_entity_true)\n",
    "            assert len(train_relation_pred) == len(train_relation_true)\n",
    "            \n",
    "        # evaluate & save checkpoint\n",
    "        print(\"epoch:\", epoch,\"average loss:\", sum(losses) / len(losses))\n",
    "        print(\"epoch:\", epoch,\"average entity loss:\", sum(entity_losses) / len(entity_losses))\n",
    "        print(\"epoch:\", epoch,\"average relation loss:\", sum(relation_losses) / len(relation_losses))\n",
    "        results = pd.concat([\n",
    "            evaluator.evaluate_results(train_entity_true, train_entity_pred, entity_label_map, entity_classes),\n",
    "            evaluator.evaluate_results(train_relation_true, train_relation_pred, relation_label_map, relation_classes)\n",
    "        ], keys=[\"Entity\", \"Relation\"])\n",
    "        results.to_csv(constants.model_save_path + \"epoch_\" + str(epoch) + \".csv\")\n",
    "#         evaluate(spert_model, constants.dev_dataset)\n",
    "        \n",
    "    torch.save(spert_model.state_dict(), constants.model_save_path + \"epoch_\" + str(constants.epochs-1) + \".model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "human-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(spert_model, sentences):\n",
    "    for sentence in sentences:\n",
    "        word_list = sentence.split()\n",
    "        words = []\n",
    "        token_ids = []\n",
    "        # transform a sentence to a document for prediction\n",
    "        for word in word_list:\n",
    "            token_id = tokenizer(word)[\"input_ids\"][1:-1]\n",
    "            for tid in token_id:\n",
    "                words.append(word)\n",
    "                token_ids.append(tid)\n",
    "        data_frame = pd.DataFrame()\n",
    "        data_frame[\"words\"] = words\n",
    "        data_frame[\"token_ids\"] = token_ids\n",
    "        data_frame[\"entity_embedding\"] = 0\n",
    "        data_frame[\"sentence_embedding\"] = 0 # for internal datasets\n",
    "        doc = {\"data_frame\": data_frame,\n",
    "            \"entity_position\": {}, # Suppose to appear in non-overlapping dataset\n",
    "            \"entities\": {}, # Suppose to appear in overlapping dataset\n",
    "            \"relations\": {}}\n",
    "        # predict\n",
    "        inputs, infos = input_generator.doc_to_input(doc, device, \n",
    "                                                     is_training=False, \n",
    "                                                     max_span_size=constants.max_span_size)\n",
    "        outputs = spert_model(**inputs, is_training=False)\n",
    "        pred_entity_span = outputs[\"entity\"][\"span\"]\n",
    "        pred_relation_span = [] if outputs[\"relation\"] is None else outputs[\"relation\"][\"span\"]\n",
    "        # print result\n",
    "        tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "        print(\"Sentence:\", sentence)\n",
    "        print(\"Entities: (\", len(pred_entity_span), \")\")\n",
    "        for begin, end, entity_type in pred_entity_span:\n",
    "            print(entity_label_map[entity_type], \"|\", \" \".join(tokens[begin:end]))\n",
    "        print(\"Relations: (\", len(pred_relation_span), \")\")\n",
    "        for e1, e2, relation_type in pred_relation_span:\n",
    "            print(relation_label_map[relation_type], \"|\", \n",
    "                  \" \".join(tokens[e1[0]:e1[1]]), \"|\", \n",
    "                  \" \".join(tokens[e2[0]:e2[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "illegal-relative",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing SpERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SpERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpERT were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['relation_classifier.weight', 'relation_classifier.bias', 'entity_classifier.weight', 'entity_classifier.bias', 'width_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Train epoch 0: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [16:07<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 average loss: 0.15172028699474469\n",
      "epoch: 0 average entity loss: 0.11030006967447699\n",
      "epoch: 0 average relation loss: 0.06083981858865757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 1: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [15:29<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 average loss: 0.04332021641778717\n",
      "epoch: 1 average entity loss: 0.03438879502500655\n",
      "epoch: 1 average relation loss: 0.0131188606501313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 2: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [16:06<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 average loss: 0.034399874909490376\n",
      "epoch: 2 average entity loss: 0.027926940694261618\n",
      "epoch: 2 average relation loss: 0.00950772760387502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 3: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [16:05<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 average loss: 0.025558522234761058\n",
      "epoch: 3 average entity loss: 0.020832276425999167\n",
      "epoch: 3 average relation loss: 0.006942115637433896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 4: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [15:50<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 average loss: 0.02212322045844141\n",
      "epoch: 4 average entity loss: 0.018446291719866816\n",
      "epoch: 4 average relation loss: 0.005400832960524757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 5: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [15:52<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 average loss: 0.018231513675384777\n",
      "epoch: 5 average entity loss: 0.01526657045908748\n",
      "epoch: 5 average relation loss: 0.004355037675175397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 6: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [15:49<00:00,  5.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 average loss: 0.01475106143597573\n",
      "epoch: 6 average entity loss: 0.01245381156459537\n",
      "epoch: 6 average relation loss: 0.0033743006243261995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 7: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [16:10<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 average loss: 0.012405536330103611\n",
      "epoch: 7 average entity loss: 0.010450324473934548\n",
      "epoch: 7 average relation loss: 0.002871900289029747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 8: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [15:32<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 average loss: 0.01064019513968736\n",
      "epoch: 8 average entity loss: 0.009081379445582256\n",
      "epoch: 8 average relation loss: 0.002289656366415395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 9: 100%|███████████████████████████████████████████████████████████████| 5398/5398 [16:17<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 average loss: 0.009359292747585762\n",
      "epoch: 9 average entity loss: 0.007928729949430722\n",
      "epoch: 9 average relation loss: 0.0021012729079924026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 10: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [16:07<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 average loss: 0.007696885762210025\n",
      "epoch: 10 average entity loss: 0.006639368876114192\n",
      "epoch: 10 average relation loss: 0.0015533268587273692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 11: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:05<00:00,  5.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 average loss: 0.0069388687949410305\n",
      "epoch: 11 average entity loss: 0.006087739462728491\n",
      "epoch: 11 average relation loss: 0.0012501758072963225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 12: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:15<00:00,  5.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 average loss: 0.006004213424350307\n",
      "epoch: 12 average entity loss: 0.00539060518995347\n",
      "epoch: 12 average relation loss: 0.0009012945136238629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 13: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [16:00<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 average loss: 0.004885807245881634\n",
      "epoch: 13 average entity loss: 0.004372331818240096\n",
      "epoch: 13 average relation loss: 0.0007542150425917376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 14: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [16:08<00:00,  5.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 average loss: 0.004280994186264318\n",
      "epoch: 14 average entity loss: 0.0038699190061368266\n",
      "epoch: 14 average relation loss: 0.000603805116150665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 15: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [14:55<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 average loss: 0.0036033403589032778\n",
      "epoch: 15 average entity loss: 0.003332561989033275\n",
      "epoch: 15 average relation loss: 0.0003977310672201115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 16: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:30<00:00,  5.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 average loss: 0.003086374234258742\n",
      "epoch: 16 average entity loss: 0.0028721957900093927\n",
      "epoch: 16 average relation loss: 0.00031459461433918506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 17: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:58<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 average loss: 0.0026420038069134505\n",
      "epoch: 17 average entity loss: 0.0025029194141867905\n",
      "epoch: 17 average relation loss: 0.00020429322236625635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 18: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:56<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 average loss: 0.002256052722454975\n",
      "epoch: 18 average entity loss: 0.00215244394547915\n",
      "epoch: 18 average relation loss: 0.00015218510244380417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch 19: 100%|██████████████████████████████████████████████████████████████| 5398/5398 [15:52<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 average loss: 0.0019492959071023047\n",
      "epoch: 19 average entity loss: 0.0018653828046246066\n",
      "epoch: 19 average relation loss: 0.0001232552027671597\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "noticed-creativity",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing SpERT: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing SpERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpERT were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['relation_classifier.weight', 'relation_classifier.bias', 'entity_classifier.weight', 'entity_classifier.bias', 'width_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SpERT(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (relation_classifier): Linear(in_features=2354, out_features=7, bias=True)\n",
       "  (entity_classifier): Linear(in_features=1561, out_features=9, bias=True)\n",
       "  (width_embedding): Embedding(100, 25)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(constants.model_path)\n",
    "spert_model = model.SpERT.from_pretrained(constants.model_path,\n",
    "                                          config=config,\n",
    "                                          # SpERT model parameters\n",
    "                                          relation_types=constants.relation_types, \n",
    "                                          entity_types=constants.entity_types, \n",
    "                                          width_embedding_size=constants.width_embedding_size, \n",
    "                                          prop_drop=constants.prop_drop, \n",
    "                                          freeze_transformer=True, \n",
    "                                          max_pairs=constants.max_pairs, \n",
    "                                          is_overlapping=constants.is_overlapping, \n",
    "                                          relation_filter_threshold=constants.relation_filter_threshold)\n",
    "spert_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "responsible-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(constants.model_save_path + \"epoch_\" + str(constants.epochs-1) + \".model\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "close-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To run reference model\n",
    "# state_dict = torch.load(\"../../model/spert/reference/reference_model.bin\")\n",
    "# state_dict[\"relation_classifier.weight\"] = state_dict[\"rel_classifier.weight\"]\n",
    "# state_dict[\"relation_classifier.bias\"] = state_dict[\"rel_classifier.bias\"]\n",
    "# state_dict[\"width_embedding.weight\"] = state_dict[\"size_embeddings.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "piano-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spert_model.load_state_dict(state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "earlier-burst",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation Test: 100%|█████████████████████████████████████████████████████████████| 1738/1738 [07:05<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  fbeta_score  \\\n",
      "Entity span      EnvironmentalIssues   0.717712  0.798222     0.755829   \n",
      "                 Date                  0.921212  0.912913     0.917044   \n",
      "                 Organisation          0.872390  0.619951     0.724819   \n",
      "                 CommitmentLevel       0.401416  0.541610     0.461092   \n",
      "                 Location              0.618644  0.782143     0.690852   \n",
      "                 CoalActivity          0.695652  0.727273     0.711111   \n",
      "                 SocialIssues          0.793369  0.853942     0.822542   \n",
      "                 SocialOfficialTexts   0.592593  0.744186     0.659794   \n",
      "                 macro                 0.701623  0.747530     0.717885   \n",
      "                 micro                 0.707498  0.740125     0.723444   \n",
      "Entity embedding EnvironmentalIssues   0.796259  0.906481     0.847803   \n",
      "                 Date                  0.952830  0.978208     0.965352   \n",
      "                 Organisation          0.895425  0.573222     0.698980   \n",
      "                 CommitmentLevel       0.472422  0.581693     0.521394   \n",
      "                 Location              0.665829  0.795796     0.725034   \n",
      "                 CoalActivity          0.785714  0.846154     0.814815   \n",
      "                 SocialIssues          0.858115  0.903030     0.880000   \n",
      "                 SocialOfficialTexts   0.758454  0.733645     0.745843   \n",
      "                 macro                 0.773131  0.789779     0.774903   \n",
      "                 micro                 0.771297  0.790447     0.780754   \n",
      "Loose relation   Makes                 0.308411  0.359477     0.331992   \n",
      "                 Of                    0.313942  0.467862     0.375750   \n",
      "                 IsRelatedTo           0.455696  0.276923     0.344498   \n",
      "                 HasActivity           0.437500  1.000000     0.608696   \n",
      "                 Recognizes            0.678571  0.593750     0.633333   \n",
      "                 In                    0.333333  0.250000     0.285714   \n",
      "                 IsInvolvedIn          0.000000  0.000000     0.000000   \n",
      "                 macro                 0.361065  0.421145     0.368569   \n",
      "                 micro                 0.326297  0.415834     0.365664   \n",
      "Strict relation  Makes                 0.266791  0.311547     0.287437   \n",
      "                 Of                    0.258517  0.385650     0.309538   \n",
      "                 IsRelatedTo           0.430380  0.261538     0.325359   \n",
      "                 HasActivity           0.375000  0.857143     0.521739   \n",
      "                 Recognizes            0.535714  0.468750     0.500000   \n",
      "                 In                    0.000000  0.000000     0.000000   \n",
      "                 IsInvolvedIn          0.000000  0.000000     0.000000   \n",
      "                 macro                 0.266629  0.326375     0.277725   \n",
      "                 micro                 0.274699  0.350500     0.308004   \n",
      "\n",
      "                                      support  \n",
      "Entity span      EnvironmentalIssues   1462.0  \n",
      "                 Date                   333.0  \n",
      "                 Organisation          1213.0  \n",
      "                 CommitmentLevel        733.0  \n",
      "                 Location               280.0  \n",
      "                 CoalActivity            22.0  \n",
      "                 SocialIssues          1205.0  \n",
      "                 SocialOfficialTexts     43.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Entity embedding EnvironmentalIssues   2160.0  \n",
      "                 Date                   413.0  \n",
      "                 Organisation          1434.0  \n",
      "                 CommitmentLevel       1016.0  \n",
      "                 Location               333.0  \n",
      "                 CoalActivity            26.0  \n",
      "                 SocialIssues          1815.0  \n",
      "                 SocialOfficialTexts    214.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Loose relation   Makes                  459.0  \n",
      "                 Of                     669.0  \n",
      "                 IsRelatedTo            130.0  \n",
      "                 HasActivity              7.0  \n",
      "                 Recognizes              32.0  \n",
      "                 In                       4.0  \n",
      "                 IsInvolvedIn             0.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n",
      "Strict relation  Makes                  459.0  \n",
      "                 Of                     669.0  \n",
      "                 IsRelatedTo            130.0  \n",
      "                 HasActivity              7.0  \n",
      "                 Recognizes              32.0  \n",
      "                 In                       4.0  \n",
      "                 IsInvolvedIn             0.0  \n",
      "                 macro                    NaN  \n",
      "                 micro                    NaN  \n"
     ]
    }
   ],
   "source": [
    "evaluate(spert_model, constants.test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "timely-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = torch.load(constants.model_save_path + \"epoch_\" + str(constants.epochs-1) + \".model\")\n",
    "# state_dict[\"rel_classifier.weight\"] = state_dict[\"relation_classifier.weight\"]\n",
    "# state_dict[\"rel_classifier.bias\"] = state_dict[\"relation_classifier.bias\"]\n",
    "# state_dict[\"size_embeddings.weight\"] = state_dict[\"width_embedding.weight\"]\n",
    "# del state_dict[\"relation_classifier.weight\"]\n",
    "# del state_dict[\"relation_classifier.bias\"]\n",
    "# del state_dict[\"width_embedding.weight\"]\n",
    "# del state_dict[\"bert.embeddings.position_ids\"]\n",
    "# torch.save(state_dict, \"../../model/spert/pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "better-vehicle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: However, the Rev. Jesse Jackson, a native of South Carolina, joined critics of FEMA's effort.\n",
      "Entities: ( 0 )\n",
      "Relations: ( 0 )\n",
      "Sentence: International Paper spokeswoman Ann Silvernail said that under French law the company was barred from releasing details pending government approval.\n",
      "Entities: ( 1 )\n",
      "Organisation | company\n",
      "Relations: ( 0 )\n"
     ]
    }
   ],
   "source": [
    "predict(spert_model, sentences=[\"However, the Rev. Jesse Jackson, a native of South Carolina, joined critics of FEMA's effort.\", \n",
    "                                \"International Paper spokeswoman Ann Silvernail said that under French law the company was barred from releasing details pending government approval.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "nutritional-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence = \" \".join([\"An\", \"art\", \"exhibit\", \"at\", \"the\", \"Hakawati\", \"Theatre\", \"in\", \"Arab\", \"east\", \"Jerusalem\", \"was\", \"a\", \"series\", \"of\", \"portraits\", \"of\", \"Palestinians\", \"killed\", \"in\", \"the\", \"rebellion\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "correct-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"entities\": [{\"type\": \"Loc\", \"start\": 5, \"end\": 7}, {\"type\": \"Loc\", \"start\": 10, \"end\": 11}, {\"type\": \"Other\", \"start\": 17, \"end\": 18}], \"relations\": [{\"type\": \"Located_In\", \"head\": 0, \"tail\": 1}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "drawn-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(spert_model, sentences=[test_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ancient-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence = \" \".join([\"PERUGIA\", \",\", \"Italy\", \"(\", \"AP\", \")\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "desperate-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict(spert_model, sentences=[test_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-drain",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
