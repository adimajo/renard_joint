{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import conll04_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-quilt",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_raw = conll04_parser.extract_data(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_raw = conll04_parser.extract_data(\"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_raw = conll04_parser.extract_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(\n",
    "    raw_data, \n",
    "    pretrain_model, \n",
    "    ignore_index=CrossEntropyLoss().ignore_index,\n",
    "    max_token_count=512,\n",
    "    cls_token=conll04_parser.CLS_TOKEN,\n",
    "    sep_token=conll04_parser.SEP_TOKEN\n",
    "):\n",
    "    \"\"\"Transform the parsed dataset with a pre-trained model\n",
    "    Only the first token of each word is labeled, the others are masked as 'ignore_index'\n",
    "    The label of O is 0\n",
    "    The label of I is the negation of the corresponding label of B\n",
    "    \"\"\"\n",
    "    progress = IntProgress(min=0, max=len(raw_data)) # instantiate the bar\n",
    "    display(progress) # display the bar\n",
    "    \n",
    "    padding_token_count = (1 if cls_token else 0) + (1 if sep_token else 0)\n",
    "    \n",
    "    transformed_tokens = []\n",
    "    true_labels = []\n",
    "    true_words = []\n",
    "    \n",
    "    for document in raw_data:\n",
    "        progress.value += 1\n",
    "        ids = document[\"data_frame\"][\"ids\"].tolist()\n",
    "        tokens = document[\"data_frame\"][\"token_ids\"].tolist()\n",
    "        labels = document[\"data_frame\"][\"entity_embedding\"].tolist()\n",
    "        words = document[\"data_frame\"][\"words\"].tolist()\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if i > 0 and ids[i] == ids[i-1]:\n",
    "                # Extra tokens from the same word are ignored\n",
    "                labels[i] = ignore_index\n",
    "                    \n",
    "        # print(list(zip(document[\"data_frame\"][\"words\"].tolist(), labels)))\n",
    "        if cls_token: \n",
    "            tokens = [cls_token] + tokens\n",
    "            labels = [ignore_index] + labels\n",
    "            words = [\"[CLS]\"] + words\n",
    "        if sep_token:\n",
    "            tokens.append(sep_token)\n",
    "            labels.append(ignore_index)\n",
    "            words.append(\"[SEP]\")\n",
    "        outputs = pretrain_model(\n",
    "            input_ids=torch.tensor([tokens]), \n",
    "            token_type_ids=torch.tensor([[0] * len(tokens)]),\n",
    "            attention_mask=torch.tensor([[1] * len(tokens)])\n",
    "        )\n",
    "        transformed_tokens += outputs.last_hidden_state[0].tolist()\n",
    "        true_labels += labels\n",
    "        true_words += words\n",
    "            \n",
    "    assert len(transformed_tokens) == len(true_labels) == len(true_words)\n",
    "    return pd.DataFrame(transformed_tokens), pd.DataFrame(list(zip(true_labels, true_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokens, train_labels = transform_data(train_raw, bert_model)\n",
    "# print(\"Saving train tokens of shape\", train_tokens.shape)\n",
    "# train_tokens.to_csv(\"train_tokens.csv\", index=False)\n",
    "# print(\"Saving train labels of shape\", train_labels.shape)\n",
    "# train_labels.to_csv(\"train_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_tokens, dev_labels = transform_data(dev_raw, bert_model)\n",
    "# print(\"Saving dev tokens of shape\", dev_tokens.shape)\n",
    "# dev_tokens.to_csv(\"dev_tokens.csv\", index=False)\n",
    "# print(\"Saving dev labels of shape\", dev_labels.shape)\n",
    "# dev_labels.to_csv(\"dev_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_tokens, test_labels = transform_data(test_raw, bert_model)\n",
    "# print(\"Saving test tokens of shape\", test_tokens.shape)\n",
    "# test_tokens.to_csv(\"test_tokens.csv\", index=False)\n",
    "# print(\"Saving test labels of shape\", test_labels.shape)\n",
    "# test_labels.to_csv(\"test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "waiting-syndicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = pd.read_csv(\"conll04\\\\train_tokens.csv\")\n",
    "train_labels = pd.read_csv(\"conll04\\\\train_labels.csv\")\n",
    "dev_tokens = pd.read_csv(\"conll04\\\\dev_tokens.csv\")\n",
    "dev_labels = pd.read_csv(\"conll04\\\\dev_labels.csv\")\n",
    "test_tokens = pd.read_csv(\"conll04\\\\test_tokens.csv\")\n",
    "test_labels = pd.read_csv(\"conll04\\\\test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-victoria",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tokens.shape, train_labels.shape)\n",
    "print(dev_tokens.shape, dev_labels.shape)\n",
    "print(test_tokens.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = train_tokens[train_labels[\"0\"] != -100]\n",
    "train_labels = train_labels[train_labels[\"0\"] != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_tokens = dev_tokens[dev_labels[\"0\"] != -100]\n",
    "dev_labels = dev_labels[dev_labels[\"0\"] != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = test_tokens[test_labels[\"0\"] != -100]\n",
    "test_labels = test_labels[test_labels[\"0\"] != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = pd.concat([train_tokens, dev_tokens], ignore_index=True)\n",
    "training_labels = pd.concat([train_labels, dev_labels], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_tokens.shape, training_labels.shape, test_tokens.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map_bio = {v: k for k, v in conll04_parser.entity_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-shock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier_bio(clf, x_train, y_train, x_test, y_test):\n",
    "    print(\"Fitting...\")\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Predicting...\")\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(y_test, y_pred, average=None, labels=clf.classes_)\n",
    "    result = pd.DataFrame(index=[label_map_bio[label] for label in clf.classes_])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    print(result)\n",
    "    \n",
    "    return clf, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "kneighbor_clf, kneighbor_result = run_classifier_bio(\n",
    "    KNeighborsClassifier(), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.956005  0.983709     0.969660     6384\n",
    "B-Loc     0.787686  0.868852     0.826281      427\n",
    "I-Loc     0.837838  0.604878     0.702550      205\n",
    "B-Peop    0.874172  0.822430     0.847512      321\n",
    "I-Peop    0.850515  0.894309     0.871863      369\n",
    "B-Org     0.741722  0.565657     0.641834      198\n",
    "I-Org     0.674157  0.497925     0.572792      241\n",
    "B-Other   0.790476  0.624060     0.697479      133\n",
    "I-Other   0.927083  0.684615     0.787611      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf, random_forest_result = run_classifier_bio(\n",
    "    RandomForestClassifier(n_estimators=20, verbose=1), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.844645  0.998120     0.914991     6384\n",
    "B-Loc     0.748148  0.473068     0.579627      427\n",
    "I-Loc     0.955224  0.312195     0.470588      205\n",
    "B-Peop    0.843243  0.485981     0.616601      321\n",
    "I-Peop    0.919149  0.585366     0.715232      369\n",
    "B-Org     0.807692  0.106061     0.187500      198\n",
    "I-Org     0.894737  0.070539     0.130769      241\n",
    "B-Other   0.950000  0.142857     0.248366      133\n",
    "I-Other   0.976190  0.315385     0.476744      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf, random_forest_result = run_classifier_bio(\n",
    "    RandomForestClassifier(n_estimators=20, class_weight=\"balanced\", verbose=1), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.815591  0.999687     0.898304     6384\n",
    "B-Loc     0.893443  0.255269     0.397086      427\n",
    "I-Loc     1.000000  0.278049     0.435115      205\n",
    "B-Peop    0.943662  0.417445     0.578834      321\n",
    "I-Peop    0.960674  0.463415     0.625229      369\n",
    "B-Org     0.944444  0.085859     0.157407      198\n",
    "I-Org     1.000000  0.053942     0.102362      241\n",
    "B-Other   0.950000  0.142857     0.248366      133\n",
    "I-Other   1.000000  0.253846     0.404908      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_clf, random_forest_result = run_classifier_bio(\n",
    "    RandomForestClassifier(n_estimators=20, class_weight=\"balanced_subsample\", verbose=1), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.817425  0.999373     0.899288     6384\n",
    "B-Loc     0.868217  0.262295     0.402878      427\n",
    "I-Loc     0.982456  0.273171     0.427481      205\n",
    "B-Peop    0.964789  0.426791     0.591793      321\n",
    "I-Peop    0.953125  0.495935     0.652406      369\n",
    "B-Org     1.000000  0.095960     0.175115      198\n",
    "I-Org     1.000000  0.037344     0.072000      241\n",
    "B-Other   0.947368  0.135338     0.236842      133\n",
    "I-Other   1.000000  0.276923     0.433735      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-namibia",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf, nn_result = run_classifier_bio(\n",
    "    MLPClassifier((512,), verbose=True), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.981159  0.978853     0.980005     6384\n",
    "B-Loc     0.913753  0.918033     0.915888      427\n",
    "I-Loc     0.893855  0.780488     0.833333      205\n",
    "B-Peop    0.944625  0.903427     0.923567      321\n",
    "I-Peop    0.937008  0.967480     0.952000      369\n",
    "B-Org     0.737557  0.823232     0.778043      198\n",
    "I-Org     0.693069  0.871369     0.772059      241\n",
    "B-Other   0.838095  0.661654     0.739496      133\n",
    "I-Other   0.877193  0.769231     0.819672      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_clf, nn_result = run_classifier_bio(\n",
    "    MLPClassifier((1024,), verbose=True), \n",
    "    training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"]\n",
    ")\n",
    "\n",
    "r = \"\"\"\n",
    "Results:\n",
    "         precision    recall  fbeta_score  support\n",
    "O         0.976516  0.983553     0.980022     6384\n",
    "B-Loc     0.919908  0.941452     0.930556      427\n",
    "I-Loc     0.875706  0.756098     0.811518      205\n",
    "B-Peop    0.958466  0.934579     0.946372      321\n",
    "I-Peop    0.954787  0.972900     0.963758      369\n",
    "B-Org     0.814433  0.797980     0.806122      198\n",
    "I-Org     0.759843  0.800830     0.779798      241\n",
    "B-Other   0.818966  0.714286     0.763052      133\n",
    "I-Other   0.855856  0.730769     0.788382      130\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-classification",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
