{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import internal_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "obvious-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "regular-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_raw = internal_parser.extract_data(internal_parser.get_docs(\"Training\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excess-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = internal_parser.extract_data(internal_parser.get_docs(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "valuable-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complete-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(\n",
    "    raw_data, \n",
    "    pretrain_model, \n",
    "    ignore_index=CrossEntropyLoss().ignore_index,\n",
    "    max_token_count=128,\n",
    "    cls_token=internal_parser.CLS_TOKEN,\n",
    "    sep_token=internal_parser.SEP_TOKEN\n",
    "):\n",
    "    \"\"\"Transform the parsed dataset with a pre-trained model\n",
    "    Only the first token of each word is labeled, the others are masked as 'ignore_index'\n",
    "    The label of O is 0\n",
    "    The label of I is the negation of the corresponding label of B\n",
    "    \"\"\"\n",
    "    progress = IntProgress(min=0, max=len(raw_data)) # instantiate the bar\n",
    "    display(progress) # display the bar\n",
    "    \n",
    "    padding_token_count = (1 if cls_token else 0) + (1 if sep_token else 0)\n",
    "    \n",
    "    transformed_tokens = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for document in raw_data:\n",
    "        progress.value += 1\n",
    "        tokens = document[\"data_frame\"][\"token_ids\"].tolist()\n",
    "        begins = document[\"data_frame\"][\"begins\"].tolist()\n",
    "        ends = document[\"data_frame\"][\"ends\"].tolist()\n",
    "        labels = document[\"data_frame\"][\"entity_embedding\"].tolist()\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if i > 0 and begins[i] == begins[i-1] and ends[i] == ends[i-1]:\n",
    "                # Extra tokens from the same word are ignored\n",
    "                labels[i] = ignore_index\n",
    "                \n",
    "        for entity in document[\"entity_position\"]:\n",
    "            begin, end = document[\"entity_position\"][entity]\n",
    "            for i in range(begin + 1, end):\n",
    "                # Every subsequence word of an entity is label as I instead of B\n",
    "                if labels[i] != ignore_index:\n",
    "                    labels[i] = -labels[i]\n",
    "                    \n",
    "        # print(list(zip(document[\"data_frame\"][\"words\"].tolist(), labels)))\n",
    "        \n",
    "        for i in range(0, len(tokens), max_token_count-padding_token_count):\n",
    "            # Segment the document and encode with the pre-trained model\n",
    "            inputs = tokens[i:min(len(tokens), i+max_token_count-padding_token_count)]\n",
    "            tmp_labels = labels[i:min(len(tokens), i+max_token_count-padding_token_count)]\n",
    "            if cls_token: \n",
    "                inputs = [cls_token] + inputs\n",
    "                tmp_labels = [ignore_index] + tmp_labels\n",
    "            if sep_token:\n",
    "                inputs.append(sep_token)\n",
    "                tmp_labels.append(ignore_index)\n",
    "            outputs = pretrain_model(\n",
    "                input_ids=torch.tensor([inputs]), \n",
    "                token_type_ids=torch.tensor([[0] * len(inputs)]),\n",
    "                attention_mask=torch.tensor([[1] * len(inputs)])\n",
    "            )\n",
    "            transformed_tokens += outputs.last_hidden_state[0].tolist()\n",
    "            true_labels += tmp_labels\n",
    "            \n",
    "    assert len(transformed_tokens) == len(true_labels)\n",
    "    return np.array(transformed_tokens), np.array(true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smooth-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a16018354e340b7a9995c10a6e3ba9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=288)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_tokens, training_labels = transform_data(training_raw, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "historical-trace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196471, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(training_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "exact-radar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196471,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-confidence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
