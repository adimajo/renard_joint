{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "descending-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertModel\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outstanding-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\\\\parser\")\n",
    "import internal_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intellectual-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "regular-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_raw = internal_parser.extract_data(internal_parser.get_docs(\"Training\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "premier-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = internal_parser.extract_data(internal_parser.get_docs(\"Test\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "serial-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(\n",
    "    raw_data, \n",
    "    pretrain_model, \n",
    "    ignore_index=CrossEntropyLoss().ignore_index,\n",
    "    max_token_count=512,\n",
    "    cls_token=internal_parser.CLS_TOKEN,\n",
    "    sep_token=internal_parser.SEP_TOKEN\n",
    "):\n",
    "    \"\"\"Transform the parsed dataset with a pre-trained model\n",
    "    Only the first token of each word is labeled, the others are masked as 'ignore_index'\n",
    "    The label of O is 0\n",
    "    The label of I is the negation of the corresponding label of B\n",
    "    \"\"\"\n",
    "    progress = IntProgress(min=0, max=len(raw_data)) # instantiate the bar\n",
    "    display(progress) # display the bar\n",
    "    \n",
    "    padding_token_count = (1 if cls_token else 0) + (1 if sep_token else 0)\n",
    "    \n",
    "    transformed_tokens = []\n",
    "    true_labels = []\n",
    "    true_words = []\n",
    "    \n",
    "    for document in raw_data:\n",
    "        progress.value += 1\n",
    "        tokens = document[\"data_frame\"][\"token_ids\"].tolist()\n",
    "        begins = document[\"data_frame\"][\"begins\"].tolist()\n",
    "        ends = document[\"data_frame\"][\"ends\"].tolist()\n",
    "        labels = document[\"data_frame\"][\"entity_embedding\"].tolist()\n",
    "        words = document[\"data_frame\"][\"words\"].tolist()\n",
    "        sentence_embedding = document[\"data_frame\"][\"sentence_embedding\"].tolist()\n",
    "        \n",
    "        for i in range(len(tokens)):\n",
    "            if i > 0 and begins[i] == begins[i-1] and ends[i] == ends[i-1]:\n",
    "                # Extra tokens from the same word are ignored\n",
    "                labels[i] = ignore_index\n",
    "                \n",
    "        for entity in document[\"entity_position\"]:\n",
    "            begin, end = document[\"entity_position\"][entity]\n",
    "            for i in range(begin + 1, end):\n",
    "                # Every subsequence word of an entity is label as I instead of B\n",
    "                if labels[i] != ignore_index:\n",
    "                    labels[i] = -labels[i]\n",
    "                    \n",
    "        # print(list(zip(document[\"data_frame\"][\"words\"].tolist(), labels)))\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            j = i\n",
    "            while j < len(tokens) and sentence_embedding[i] == sentence_embedding[j] and j - i < max_token_count-padding_token_count:\n",
    "                j += 1\n",
    "            # Segment the document and encode with the pre-trained model\n",
    "            inputs = tokens[i:j]\n",
    "            tmp_labels = labels[i:j]\n",
    "            tmp_words = words[i:j]\n",
    "            if cls_token: \n",
    "                inputs = [cls_token] + inputs\n",
    "                tmp_labels = [ignore_index] + tmp_labels\n",
    "                tmp_words = [\"[CLS]\"] + tmp_words\n",
    "            if sep_token:\n",
    "                inputs.append(sep_token)\n",
    "                tmp_labels.append(ignore_index)\n",
    "                tmp_words.append(\"[SEP]\")\n",
    "            outputs = pretrain_model(\n",
    "                input_ids=torch.tensor([inputs]), \n",
    "                token_type_ids=torch.tensor([[0] * len(inputs)]),\n",
    "                attention_mask=torch.tensor([[1] * len(inputs)])\n",
    "            )\n",
    "            transformed_tokens += outputs.last_hidden_state[0].tolist()\n",
    "            true_labels += tmp_labels\n",
    "            true_words += tmp_words\n",
    "            i = j\n",
    "            \n",
    "    assert len(transformed_tokens) == len(true_labels) == len(true_words)\n",
    "    return pd.DataFrame(transformed_tokens), pd.DataFrame(list(zip(true_labels, true_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79dd6426bdd2473a8fe798fd290878c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=288)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_tokens, training_labels = transform_data(training_raw, bert_model)\n",
    "print(\"Saving training tokens of shape\", training_tokens.shape)\n",
    "training_tokens.to_csv(\"training_tokens.csv\", index=False)\n",
    "print(\"Saving training labels of shape\", training_labels.shape)\n",
    "training_labels.to_csv(\"training_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens, test_labels = transform_data(test_raw, bert_model)\n",
    "print(\"Saving test tokens of shape\", test_tokens.shape)\n",
    "test_tokens.to_csv(\"test_tokens.csv\", index=False)\n",
    "print(\"Saving test labels of shape\", test_labels.shape)\n",
    "test_labels.to_csv(\"test_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = pd.read_csv(\"128_by_128\\\\training_tokens.csv\")\n",
    "training_labels = pd.read_csv(\"128_by_128\\\\training_labels.csv\")\n",
    "test_tokens = pd.read_csv(\"128_by_128\\\\test_tokens.csv\")\n",
    "test_labels = pd.read_csv(\"128_by_128\\\\test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-produce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_tokens.shape, training_labels.shape, test_tokens.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tokens = training_tokens[training_labels[\"0\"] != -100]\n",
    "training_labels = training_labels[training_labels[\"0\"] != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = test_tokens[test_labels[\"0\"] != -100]\n",
    "test_labels = test_labels[test_labels[\"0\"] != -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_tokens.shape, training_labels.shape, test_tokens.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "commercial-wallpaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {v: k for k, v in internal_parser.entity_encode.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(clf, x_train, y_train, x_test, y_test):\n",
    "    y_train = y_train.abs()\n",
    "    y_test = y_test.abs()\n",
    "    \n",
    "    print(\"Fitting...\")\n",
    "    clf.fit(x_train, y_train)\n",
    "    print(\"Predicting...\")\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    print(\"Results:\")\n",
    "    precision, recall, fbeta_score, support = precision_recall_fscore_support(y_test, y_pred, average=None, labels=clf.classes_)\n",
    "    result = pd.DataFrame(index=[label_map[label] for label in clf.classes_])\n",
    "    result[\"precision\"] = precision\n",
    "    result[\"recall\"] = recall\n",
    "    result[\"fbeta_score\"] = fbeta_score\n",
    "    result[\"support\"] = support\n",
    "    print(result)\n",
    "    \n",
    "    return clf, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kneighbor_clf, kneighbor_result = run_classifier(KNeighborsClassifier(), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])\n",
    "# Results:\n",
    "#                      precision    recall  fbeta_score  support\n",
    "# None                  0.971389  0.971116     0.971253    46289\n",
    "# EnvironmentalIssues   0.822701  0.865340     0.843482     2161\n",
    "# Date                  0.970588  0.958838     0.964677      413\n",
    "# Organisation          0.723154  0.901674     0.802607     1434\n",
    "# CommitmentLevel       0.586402  0.407480     0.480836     1016\n",
    "# Location              0.797665  0.615616     0.694915      333\n",
    "# CoalActivity          0.916667  0.846154     0.880000       26\n",
    "# SocialIssues          0.877734  0.860836     0.869203     1818\n",
    "# SocialOfficialTexts   0.788360  0.696262     0.739454      214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree_clf, decision_tree_result = run_classifier(DecisionTreeClassifier(), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])\n",
    "# Results:\n",
    "#                      precision    recall  fbeta_score  support\n",
    "# None                  0.929607  0.914645     0.922065    46289\n",
    "# EnvironmentalIssues   0.528506  0.587691     0.556529     2161\n",
    "# Date                  0.609572  0.585956     0.597531      413\n",
    "# Organisation          0.362570  0.495816     0.418851     1434\n",
    "# CommitmentLevel       0.140426  0.162402     0.150616     1016\n",
    "# Location              0.237458  0.213213     0.224684      333\n",
    "# CoalActivity          0.129032  0.153846     0.140351       26\n",
    "# SocialIssues          0.553613  0.522552     0.537634     1818\n",
    "# SocialOfficialTexts   0.252809  0.210280     0.229592      214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-silly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest_clf, random_forest_result = run_classifier(RandomForestClassifier(n_estimators=10, max_depth=10, verbose=1), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-south",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_clf, nn_result = run_classifier(MLPClassifier((512,), verbose=True), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BIO embedding is used for further relation extraction\n",
    "#\n",
    "# label_map_bio = {}\n",
    "# for key in internal_parser.entity_encode:\n",
    "#     if internal_parser.entity_encode[key] == 0:\n",
    "#         label_map_bio[0] = \"O\"\n",
    "#     else:\n",
    "#         label_map_bio[internal_parser.entity_encode[key]] = \"B-\" + key\n",
    "#         label_map_bio[-internal_parser.entity_encode[key]] = \"I-\" + key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_classifier_bio(clf, x_train, y_train, x_test, y_test):\n",
    "#     print(\"Fitting...\")\n",
    "#     clf.fit(x_train, y_train)\n",
    "#     print(\"Predicting...\")\n",
    "#     y_pred = clf.predict(x_test)\n",
    "    \n",
    "#     print(\"Results:\")\n",
    "#     precision, recall, fbeta_score, support = precision_recall_fscore_support(y_test, y_pred, average=None, labels=clf.classes_)\n",
    "#     result = pd.DataFrame(index=[label_map_bio[label] for label in clf.classes_])\n",
    "#     result[\"precision\"] = precision\n",
    "#     result[\"recall\"] = recall\n",
    "#     result[\"fbeta_score\"] = fbeta_score\n",
    "#     result[\"support\"] = support\n",
    "#     print(result)\n",
    "    \n",
    "#     return clf, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kneighbor_clf, kneighbor_result = run_classifier_bio(KNeighborsClassifier(), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree_clf, decision_tree_result = run_classifier_bio(DecisionTreeClassifier(), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_forest_clf, random_forest_result = run_classifier_bio(RandomForestClassifier(verbose=1), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_clf, nn_result = run_classifier_bio(MLPClassifier((512,), verbose=True), training_tokens, training_labels[\"0\"], test_tokens, test_labels[\"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-contest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
